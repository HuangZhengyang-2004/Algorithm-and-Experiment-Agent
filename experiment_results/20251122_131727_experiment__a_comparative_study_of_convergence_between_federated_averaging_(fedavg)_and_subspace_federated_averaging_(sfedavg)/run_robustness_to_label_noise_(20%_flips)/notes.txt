# Title: Experiment: A Comparative Study of Convergence between Federated Averaging (FedAvg) and Subspace Federated Averaging (SFedAvg)
# Experiment description: Compare the convergence behavior of Federated Averaging (FedAvg) and Subspace Federated Averaging (SFedAvg) on a standard dataset (e.g., MNIST or CIFAR-10) under identical settings. Measure metrics such as training loss, test accuracy, and communication efficiency over multiple rounds of training.
# Timestamp: 20251122_131727

## Experiment Log
LOGIC_VALIDATION_PASSED

Non-IID partition via Dirichlet α=0.1 over labels (strong class dominance per client; empty-client handling enabled).
Label noise: none (0%).
Special processing/mods: standardized features (StandardScaler); Dirichlet allocation with rounding; skip empty clients in training/aggregation/communication accounting.
Key differences: heterogeneity_alpha flag activates non-IID split; SFedAvg applies per-round projector Π_t with one-sided projected momentum to damp inter-client gradient variance.

## Tuning Analysis: non-iid_label_skew_(dirichlet α=0.1)
1) Best Configuration: stepsize=0.30, momentum=0.95, subspace_rank_fraction=0.35 (config_7). It yields the lowest SFedAvg train loss and matches the highest SFedAvg test accuracy peak observed among configs.
2) Performance Improvement: SFedAvg final train loss improved by ≈20.1% vs baseline (0.2658 → 0.2125); early-round loss (round 10) improved by ≈23.9% (0.7433 → 0.5653). Peak SFedAvg test accuracy increased by ≈0.3% relative (0.9361 → 0.9389), while FedAvg peak accuracy improved by ≈0.9% relative (0.95 → 0.9583) under some configs.
3) Parameter Insights: Momentum had the strongest impact; higher μ=0.95 consistently accelerated SFedAvg convergence under label skew. Subspace rank was second-most influential: larger rank (0.30–0.35) improved loss/accuracy but increased communication; lower rank (0.15–0.20) reduced comms with notable accuracy degradation. Stepsize adjustments within 0.20–0.35 mainly traded stability vs speed with smaller effects than momentum/rank.
4) Recommendation: For accuracy/convergence priority, use stepsize=0.30, momentum=0.95, subspace_rank_fraction=0.35. If communication budget is critical, consider stepsize=0.30, momentum=0.90, subspace_rank_fraction=0.15 to reduce SFedAvg comms (~19% lower than baseline) at the cost of accuracy; a balanced alternative is stepsize=0.35, momentum=0.80, subspace_rank_fraction=0.30 for solid accuracy with moderate comm overhead.

