{
  "scenarios": [
    {
      "name": "Non-IID Label Skew (Dirichlet α=0.1)",
      "description": "Assess convergence and stability of SFedAvg vs FedAvg under heterogeneous client label distributions created by Dirichlet partitioning.",
      "parameters": {
        "--n_clients": 50,
        "--client_fraction": 0.5,
        "--rounds": 50,
        "--local_steps": 2,
        "--stepsize": 0.3,
        "--momentum": 0.9,
        "--batch_size": 32,
        "--subspace_rank_fraction": 0.25,
        "--seed": 42,
        "--heterogeneity_alpha": 0.1
      },
      "dataset": "Digits dataset partitioned across clients via Dirichlet(α=0.1) over labels to induce strong non-IID skew; each client receives a dominant class and minority classes.",
      "expected_insight": "Projected momentum in SFedAvg should reduce inter-client gradient variance and improve early-round convergence vs FedAvg; potential accuracy gains under skewed data.",
      "metrics": [
        "train_loss_fedavg",
        "train_loss_sfedavg",
        "test_acc_fedavg",
        "test_acc_sfedavg",
        "comm_cost_fedavg",
        "comm_cost_sfedavg",
        "effective_update_dims_fedavg",
        "effective_update_dims_sfedavg"
      ]
    },
    {
      "name": "Robustness to Label Noise (20% flips)",
      "description": "Evaluate resilience of SFedAvg vs FedAvg when labels are corrupted with random flips.",
      "parameters": {
        "--n_clients": 50,
        "--client_fraction": 0.5,
        "--rounds": 50,
        "--local_steps": 2,
        "--stepsize": 0.3,
        "--momentum": 0.9,
        "--batch_size": 32,
        "--subspace_rank_fraction": 0.25,
        "--seed": 7,
        "--label_noise_fraction": 0.2
      },
      "dataset": "Digits dataset with per-client label corruption: uniformly flip 20% of labels at random to simulate noisy or adversarial clients.",
      "expected_insight": "SFedAvg’s one-sided projection should dampen high-variance noisy gradients, yielding lower train loss and higher test accuracy than FedAvg under label noise.",
      "metrics": [
        "train_loss_fedavg",
        "train_loss_sfedavg",
        "test_acc_fedavg",
        "test_acc_sfedavg",
        "comm_cost_fedavg",
        "comm_cost_sfedavg",
        "effective_update_dims_fedavg",
        "effective_update_dims_sfedavg"
      ]
    },
    {
      "name": "Scalability: Large N and High-D Features",
      "description": "Test communication-efficiency and convergence as the number of clients and feature dimension increase.",
      "parameters": {
        "--n_clients": 200,
        "--client_fraction": 0.2,
        "--rounds": 30,
        "--local_steps": 1,
        "--stepsize": 0.2,
        "--momentum": 0.9,
        "--batch_size": 64,
        "--subspace_rank_fraction": 0.1,
        "--seed": 11,
        "--feature_expansion_dim": 1024
      },
      "dataset": "Digits features expanded to 1024 dimensions using random Fourier features or Gaussian projections to emulate a higher-capacity model and increased communication payload.",
      "expected_insight": "SFedAvg should achieve comparable accuracy with substantially reduced cumulative communication cost vs FedAvg as N and d grow, demonstrating scalability benefits.",
      "metrics": [
        "train_loss_fedavg",
        "train_loss_sfedavg",
        "test_acc_fedavg",
        "test_acc_sfedavg",
        "comm_cost_fedavg",
        "comm_cost_sfedavg",
        "effective_update_dims_fedavg",
        "effective_update_dims_sfedavg"
      ]
    },
    {
      "name": "Hyperparameter Sensitivity: No Momentum",
      "description": "Isolate the role of momentum by disabling it and comparing SFedAvg’s projection-only benefits with FedAvg.",
      "parameters": {
        "--n_clients": 50,
        "--client_fraction": 0.5,
        "--rounds": 30,
        "--local_steps": 2,
        "--stepsize": 0.3,
        "--momentum": 0.0,
        "--batch_size": 32,
        "--subspace_rank_fraction": 0.25,
        "--seed": 5
      },
      "dataset": "IID split of the digits dataset across clients to focus on algorithmic differences arising from momentum and projection.",
      "expected_insight": "Without momentum, both methods converge more slowly; SFedAvg’s projection still offers stability and may retain a small accuracy advantage over FedAvg.",
      "metrics": [
        "train_loss_fedavg",
        "train_loss_sfedavg",
        "test_acc_fedavg",
        "test_acc_sfedavg",
        "comm_cost_fedavg",
        "comm_cost_sfedavg",
        "effective_update_dims_fedavg",
        "effective_update_dims_sfedavg"
      ]
    },
    {
      "name": "Edge Case: Full-Rank SFedAvg with Aggressive Local Training",
      "description": "Probe failure modes when SFedAvg reduces to full-rank projection (Π=I) under large stepsize and many local steps.",
      "parameters": {
        "--n_clients": 20,
        "--client_fraction": 0.1,
        "--rounds": 25,
        "--local_steps": 10,
        "--stepsize": 0.5,
        "--momentum": 0.9,
        "--batch_size": 32,
        "--subspace_rank_fraction": 1.0,
        "--seed": 3
      },
      "dataset": "IID split of the digits dataset; focus on optimization dynamics given aggressive local updates and full-rank projection (SFedAvg ≈ FedAvg with momentum).",
      "expected_insight": "Both algorithms may exhibit instability or divergence under aggressive settings; since Π=I, SFedAvg should behave like momentum FedAvg, revealing limits of local training and stepsize.",
      "metrics": [
        "train_loss_fedavg",
        "train_loss_sfedavg",
        "test_acc_fedavg",
        "test_acc_sfedavg",
        "comm_cost_fedavg",
        "comm_cost_sfedavg",
        "effective_update_dims_fedavg",
        "effective_update_dims_sfedavg"
      ]
    }
  ],
  "rationale": "The scenarios are anchored in the SFedAvg pseudocode: they vary client subsets, subspace rank (Π_t), momentum usage, and local update dynamics. We cover non-IID heterogeneity (Dirichlet skew), robustness to label noise, scalability with many clients and higher feature dimension (communication and effective update coordinates), hyperparameter sensitivity (disabling momentum), and edge-case instability (full-rank projection with aggressive local steps and stepsize). Together, these expose how one-sided projected momentum interacts with aggregation and communication, and when SFedAvg provides convergence or efficiency advantages over FedAvg."
}