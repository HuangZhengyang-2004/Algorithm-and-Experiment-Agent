{
  "scenarios": [
    {
      "name": "IID-Baseline-Convergence",
      "description": "Establish a clean baseline comparison of FedAvg vs SFedAvg under IID client data to verify round-by-round convergence and the effect of one-sided subspace projection with momentum.",
      "parameters": {
        "--rounds": 60,
        "--clients": 20,
        "--client_frac": 0.2,
        "--local_steps": 5,
        "--lr": 0.1,
        "--mu": 0.9,
        "--batch_size": 64,
        "--reg": 0.0001,
        "--subspace_frac": 0.25
      },
      "dataset": "Single global dataset split IID across 20 clients via stratified partition (balanced label proportions per client).",
      "expected_insight": "Both methods should converge; SFedAvg may achieve comparable or slightly faster early-round loss decrease due to projected momentum while requiring transmission of the subspace, offering a baseline for later scenarios.",
      "metrics": [
        "train_loss/FedAvg",
        "train_loss/SFedAvg",
        "test_accuracy/FedAvg",
        "test_accuracy/SFedAvg",
        "cumulative_comm_params/FedAvg",
        "cumulative_comm_params/SFedAvg",
        "accuracy_vs_cumulative_comm"
      ]
    },
    {
      "name": "NonIID-Label-Skew",
      "description": "Stress-test client drift and aggregation stability by imposing strong label skew so each client predominantly sees few classes.",
      "parameters": {
        "--rounds": 80,
        "--clients": 20,
        "--client_frac": 0.25,
        "--local_steps": 10,
        "--lr": 0.05,
        "--mu": 0.9,
        "--batch_size": 64,
        "--reg": 0.0001,
        "--subspace_frac": 0.25
      },
      "dataset": "Construct non-IID splits with strong label skew (e.g., Dirichlet α≈0.2 over label distribution per client), so each client sees 1–3 dominant classes.",
      "expected_insight": "FedAvg may exhibit slower or unstable convergence due to client drift; SFedAvg’s one-sided projected momentum can temper drift and stabilize training, improving loss/accuracy at a given round budget.",
      "metrics": [
        "train_loss/FedAvg",
        "train_loss/SFedAvg",
        "test_accuracy/FedAvg",
        "test_accuracy/SFedAvg",
        "cumulative_comm_params/FedAvg",
        "cumulative_comm_params/SFedAvg",
        "rounds_to_target_accuracy"
      ]
    },
    {
      "name": "Robustness-to-Label-Noise",
      "description": "Quantify robustness when a subset of clients have corrupted labels, probing whether subspace projection filters harmful directions.",
      "parameters": {
        "--rounds": 80,
        "--clients": 20,
        "--client_frac": 0.25,
        "--local_steps": 5,
        "--lr": 0.1,
        "--mu": 0.95,
        "--batch_size": 64,
        "--reg": 0.001,
        "--subspace_frac": 0.2
      },
      "dataset": "IID base split, then choose 30% of clients to flip labels uniformly at random for 20% of their local samples to simulate noisy supervision.",
      "expected_insight": "FedAvg accuracy degrades more under label noise; SFedAvg with projection and higher momentum should dampen corrupted gradient components, preserving better test accuracy at similar communication budgets.",
      "metrics": [
        "train_loss/FedAvg",
        "train_loss/SFedAvg",
        "test_accuracy/FedAvg",
        "test_accuracy/SFedAvg",
        "cumulative_comm_params/FedAvg",
        "cumulative_comm_params/SFedAvg",
        "final_accuracy_gap"
      ]
    },
    {
      "name": "Scalability-Many-Clients-Low-Participation",
      "description": "Evaluate behavior as the number of clients grows with small per-round participation, emphasizing communication efficiency and convergence speed.",
      "parameters": {
        "--rounds": 100,
        "--clients": 100,
        "--client_frac": 0.1,
        "--local_steps": 5,
        "--lr": 0.1,
        "--mu": 0.9,
        "--batch_size": 32,
        "--reg": 0.0001,
        "--subspace_frac": 0.1
      },
      "dataset": "IID stratified split across 100 clients to isolate the effect of participation sparsity and aggregation scale.",
      "expected_insight": "With many clients and low participation, FedAvg may require more rounds; SFedAvg can achieve competitive accuracy with fewer effective gradient directions per client, potentially improving accuracy-per-communication if convergence accelerates.",
      "metrics": [
        "test_accuracy/FedAvg",
        "test_accuracy/SFedAvg",
        "cumulative_comm_params/FedAvg",
        "cumulative_comm_params/SFedAvg",
        "accuracy_vs_cumulative_comm",
        "time_to_80pct_of_best_accuracy"
      ]
    },
    {
      "name": "Hyperparam-and-Edge-Case-Subspace-and-Momentum",
      "description": "Probe sensitivity to the subspace dimension and the role of momentum by using very small subspace and zero momentum.",
      "parameters": {
        "--rounds": 60,
        "--clients": 20,
        "--client_frac": 0.2,
        "--local_steps": 5,
        "--lr": 0.1,
        "--mu": 0.0,
        "--batch_size": 64,
        "--reg": 0.0001,
        "--subspace_frac": 0.05
      },
      "dataset": "IID split; hold data constant to isolate hyperparameter effects of projection and momentum.",
      "expected_insight": "With μ=0 the update reduces to projected gradients, revealing whether momentum projection is key to SFedAvg’s gains; very small subspace (r/d=0.05) may slow or stall learning, illustrating the trade-off between dimensionality reduction and convergence.",
      "metrics": [
        "train_loss/FedAvg",
        "train_loss/SFedAvg",
        "test_accuracy/FedAvg",
        "test_accuracy/SFedAvg",
        "cumulative_comm_params/FedAvg",
        "cumulative_comm_params/SFedAvg",
        "early_round_accuracy"
      ]
    }
  ],
  "rationale": "The scenarios are grounded in the algorithm’s core elements—client subsampling, local steps with momentum, and one-sided random subspace projection. We cover IID baseline behavior, non-IID heterogeneity to induce client drift, robustness to label noise to test projection’s filtering effect, scalability with many clients and sparse participation to examine communication–convergence trade-offs, and a hyperparameter/edge-case scenario to dissect the roles of subspace dimension and momentum. Together, these scenarios reveal where SFedAvg’s projected momentum helps or hurts relative to FedAvg and how convergence interacts with communication costs."
}