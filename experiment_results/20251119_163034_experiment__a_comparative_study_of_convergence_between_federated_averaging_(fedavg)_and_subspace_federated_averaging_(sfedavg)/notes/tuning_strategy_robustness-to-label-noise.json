{
  "selected_parameters": ["lr", "mu", "subspace_frac"],
  "selection_rationale": "Under label noise, learning rate controls sensitivity to mislabeled gradients, momentum averages out noisy signals across rounds, and the subspace fraction (r/D) in SFedAvg filters gradient directions—trading expressivity for robustness.",
  "configs": [
    {
      "lr": 0.07,
      "mu": 0.95,
      "subspace_frac": 0.20,
      "rationale": "Slightly smaller LR with high momentum to stabilize updates against corrupted gradients while keeping baseline projection."
    },
    {
      "lr": 0.10,
      "mu": 0.98,
      "subspace_frac": 0.20,
      "rationale": "Increase momentum for stronger temporal smoothing of noisy updates at the same subspace size."
    },
    {
      "lr": 0.08,
      "mu": 0.90,
      "subspace_frac": 0.20,
      "rationale": "Moderate LR with lower momentum to test if less accumulation reduces bias from persistent mislabeled directions."
    },
    {
      "lr": 0.10,
      "mu": 0.95,
      "subspace_frac": 0.15,
      "rationale": "Tighter subspace to more aggressively filter harmful directions, trading some expressivity for robustness."
    },
    {
      "lr": 0.10,
      "mu": 0.95,
      "subspace_frac": 0.30,
      "rationale": "Larger subspace to recover expressivity; checks if higher momentum suffices to counter noise without strong projection."
    },
    {
      "lr": 0.12,
      "mu": 0.95,
      "subspace_frac": 0.25,
      "rationale": "Slightly higher LR with a mid-sized subspace to accelerate convergence while relying on projection and momentum to stabilize."
    },
    {
      "lr": 0.08,
      "mu": 0.98,
      "subspace_frac": 0.20,
      "rationale": "Combine conservative LR with strong momentum for maximal smoothing in the baseline subspace size."
    }
  ],
  "expected_improvement": "Compared to baseline, we expect higher momentum (mu≈0.98) or a slightly reduced subspace (r/D≈0.15–0.20) to improve test accuracy under noise by filtering/shrinking corrupted directions; moderate LR (0.07–0.10) should further stabilize training, yielding better accuracy at similar communication cost."
}
