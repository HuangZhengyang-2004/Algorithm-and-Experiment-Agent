# Title: Experiment: A Comparative Study of Convergence between Federated Averaging (FedAvg) and Subspace Federated Averaging (SFedAvg)
# Experiment description: Compare the convergence behavior of Federated Averaging (FedAvg) and Subspace Federated Averaging (SFedAvg) on a standard dataset (e.g., MNIST or CIFAR-10) under identical settings. Measure metrics such as training loss, test accuracy, and communication efficiency over multiple rounds of training.
# Timestamp: 20251119_151421

## Experiment Log

Validation: LOGIC_VALIDATION_PASSED

Checks:
- Algorithm Fidelity:
  * Server initializes θ^0 from W_init.
  * For each round t, SFedAvg samples P_t ∈ St(d,r) and sets Π_t = P_t P_t^⊤, held fixed within the round.
  * Client subset S_t is sampled with size m ≈ C·N.
  * Clients receive (θ^t, Π_t) (Π_t None for FedAvg) and run ClientUpdate; server aggregates θ^{t+1} = θ^t + (1/m) ∑ Δ_i^t.
- ClientUpdate implementation:
  * Sets θ_{i,0} ← θ^t.
  * Momentum Projection (MP) at block start: v_i^0 ← Π_t v_i^{prev} if available else 0 (only for SFedAvg).
  * For s=0..τ-1: samples minibatch, computes true gradient from data, updates v_{i,s+1} = μ v_{i,s} + Π_t g_{i,s} (SFedAvg) and θ_{i,s+1} = θ_{i,s} − η v_{i,s+1}; FedAvg uses plain SGD θ_{i,s+1} = θ_{i,s} − η g_{i,s}.
  * Returns Δ_i^t = θ_{i,τ} − θ^t and stores v_i^{prev} = v_{i,τ} (zeros for FedAvg).
- Metrics:
  * Train loss and test accuracy are computed each round from actual predictions and labels.
  * Communication cost tracks cumulative MB per round, including sending θ^t and Π_t (SFedAvg) and receiving full Δ_i^t.

Conclusion: Implementation matches the provided pseudocode for both FedAvg and SFedAvg with MP.

- Data partition: Non-IID label-skew via Dirichlet α≈0.2 across 50 clients (each dominated by ~2 classes).
- Label noise: None (0%).
- Special processing/mods: Enabled '--l' alias and disallowed abbrev parsing; SFedAvg samples Π_t each round and applies momentum projection of stored v_i.
- Key distinction: Strong client drift from skew; contrasts FedAvg's plain local SGD with SFedAvg's one-sided projected momentum under C=0.2 and subspace_ratio=0.1.

Tuning Analysis - non-iid_label_skew_(client_drift_stress_test)
1) Best Configuration: subspace_ratio=0.30, lr=0.12, local_steps=8 (config_8) achieved the highest SFedAvg test accuracy (final 0.9722) and the lowest train loss (final 0.1212), outperforming all other tested settings.
2) Performance Improvement: Compared to baseline SFedAvg (final test acc 0.9389), config_8 improves accuracy by ~3.6% relative and reduces final train loss by ~67.8%, indicating substantially faster and more stable convergence under heterogeneity.
3) Parameter Insights: subspace_ratio had the strongest impact—moderate-to-high r/d (0.25–0.35) consistently improved SFedAvg; lr needed to be conservative (≈0.10–0.12) to avoid instability with projected momentum; longer local_steps (≈8) helped SFedAvg due to momentum projection mitigating client drift, while very small r (0.05) or aggressive lr (0.25) degraded performance.
4) Recommendation: Use subspace_ratio=0.30, lr=0.12, local_steps=8 with momentum=0.9 and client_frac aligned to the scenario (e.g., 0.2); this setting balances projection strength and step size, yielding superior accuracy and markedly lower training loss under strong label skew.

Robustness to Label Noise - scenario summary
- Data partition: IID across clients.
- Label noise: 20% label flips per client (random to a different class).
- Special processing/mods: Added add_label_noise() and CLI flags (--label_noise_rate); kept partition=iid; algorithms selectable via --algorithm.
- Key distinction: Per-client noisy labels; SFedAvg applies one-sided Π_t projection with momentum projection to damp corrupted gradients versus FedAvg’s plain local SGD.

Tuning Analysis - robustness_to_label_noise
1) Best Configuration: Config_7 (subspace_ratio=0.35, lr=0.10, batch_size=64) achieved the highest SFedAvg final test accuracy (0.9500) and a low final train loss (~0.2178), outperforming all other tested settings under label noise. It maintains strong accuracy consistency in later rounds while keeping losses steadily decreasing. This indicates that a larger projection dimension with conservative learning rate is beneficial in noisy conditions.

2) Performance Improvement: Compared to baseline SFedAvg (final test acc 0.9389), config_7 improves accuracy by ~1.2% relative and reduces final train loss by ~31% (from ~0.3160 to ~0.2178). Convergence is faster early and remains stable across rounds, with fewer plateaus. Overall, SFedAvg under config_7 closes the robustness gap versus FedAvg while retaining communication characteristics.

3) Parameter Insights: subspace_ratio had the strongest impact—moving from 0.1 to 0.35 consistently improved SFedAvg accuracy and lowered loss under noise by retaining more signal while MP filters spurious components. Lower lr (≈0.10–0.12) improved stability; aggressive lr (0.15) with small r degraded results despite larger batches. Batch size increases offered limited gains; very large batches (128) could not compensate for overly small r or high lr.

4) Recommendation: Use subspace_ratio=0.35, lr=0.10, batch_size=64 (with momentum=0.9, local_steps=4, client_frac=0.3) for this scenario. As a secondary option for tighter budgets, subspace_ratio=0.30, lr=0.12, batch_size=96 also performs strongly with slightly lower communication per round. These settings balance projection strength and learning rate to damp noisy gradients while preserving convergence speed.

Scalability: Many Clients and Higher Dimensional Features - scenario summary
- Data partition: IID across 200 clients.
- Label noise: None (0%).
- Special processing/mods: Feature augmentation via --feature_expand_method ('random_linear' or 'pca_pad') with --feature_multiplier/--target_dim and --augment_seed to increase dimensionality; kept partition=iid.
- Key distinction: Many small clients (C=0.1, N=200) and higher-dimensional features; SFedAvg uses one-sided Π_t with r/d=0.05 to stabilize updates while tracking communication trade-offs.

Tuning Analysis - scalability:_many_clients_and_higher_dimensional_features
1) Best Configuration: subspace_ratio=0.06, feature_multiplier=2.5, client_frac=0.20 (config_8) yielded the highest SFedAvg final test accuracy (0.9333) and the lowest final train loss (~0.1515), maintaining stable convergence across rounds despite high dimensionality and many clients.
2) Performance Improvement: Relative to baseline SFedAvg (final acc 0.8083), config_8 improves accuracy by ~15.5% and reduces final train loss by ~88.6% (1.3304 → 0.1515), indicating markedly better optimization and generalization under large-N, high-d settings.
3) Parameter Insights: Moderate subspace ratios (≈0.05–0.08) consistently improved SFedAvg; increasing client_frac to 0.20 reduced aggregation variance and accelerated convergence; very small r/d (0.03–0.04) with extreme feature expansion underperformed, while too low participation (client_frac=0.05) hurt accuracy despite lower communication.
4) Recommendation: Use subspace_ratio=0.06, feature_multiplier≈2.5, client_frac=0.20 with momentum=0.9 and lr=0.1 for best accuracy; for reduced communication, subspace_ratio=0.08 with feature_multiplier≈2.0 and client_frac=0.10 offers a strong accuracy/communication trade-off.

Hyperparameter Sensitivity: Subspace Dimension (r/d) - scenario summary
- Data partition: IID across clients.
- Label noise: None (0%).
- Special processing/mods: Batch tuning mode 'sensitivity_subspace' runs SFedAvg only; varied --subspace_ratio ∈ {0.05, 0.10, 0.25}; no feature augmentation.
- Key distinction: Isolated r/d sweep to form sensitivity curve; all other hyperparameters fixed to probe convergence and generalization trade-offs.

Tuning Analysis - hyperparameter_sensitivity:_subspace_dimension_(r/d)
1) Best Configuration: subspace_ratio=0.25, lr=0.12, client_frac=0.10 (config_8) achieved the highest SFedAvg final test accuracy (0.9583) with a low final train loss (~0.1725), indicating the strongest convergence in this sensitivity sweep.
2) Performance Improvement: Relative to baseline SFedAvg (final acc 0.9222), config_8 improves accuracy by ~3.9% and reduces final train loss by ~41% (0.2940 → 0.1725), demonstrating faster optimization and better generalization.
3) Parameter Insights: Increasing r/d from 0.05→0.10→0.25 consistently improved accuracy and reduced loss, suggesting larger subspaces helped in this dataset; lr variations around 0.12 had modest impact, while changing client_frac showed smaller effects (0.10 slightly outperforming 0.20–0.30 at the best r/d).
4) Recommendation: Use subspace_ratio=0.25, lr=0.12, client_frac=0.10 for this scenario to maximize accuracy and convergence; if communication needs to be reduced, consider subspace_ratio=0.10 with lr=0.12 and client_frac=0.20, which remains competitive with lower projection dimension.

