{
  "scenarios": [
    {
      "name": "Non-IID Label Skew (Client Drift Stress Test)",
      "description": "Evaluate convergence under strong heterogeneity where each client sees a skewed label distribution, stressing the role of Π_t and MP in SFedAvg.",
      "parameters": {
        "--rounds": 50,
        "--local_steps": 5,
        "--client_frac": 0.2,
        "--lr": 0.1,
        "--momentum": 0.9,
        "--batch_size": 64,
        "--clients": 50,
        "--subspace_ratio": 0.1,
        "--l": 0.0005,
        "--seed": 1
      },
      "dataset": "Digits/MNIST-style classification with Dirichlet label skew (alpha≈0.2) across 50 clients; each client predominantly holds samples from 2 classes.",
      "expected_insight": "SFedAvg with one-sided projection and MP should mitigate client drift, showing faster round-wise loss decrease and higher test accuracy than FedAvg under strong heterogeneity.",
      "metrics": ["train_loss_fedavg", "train_loss_sfedavg", "test_acc_fedavg", "test_acc_sfedavg", "comm_MB_fedavg", "comm_MB_sfedavg"]
    },
    {
      "name": "Robustness to Label Noise",
      "description": "Assess sensitivity to noisy labels and whether subspace momentum projection dampens the effect of corrupted gradients.",
      "parameters": {
        "--rounds": 40,
        "--local_steps": 4,
        "--client_frac": 0.3,
        "--lr": 0.15,
        "--momentum": 0.9,
        "--batch_size": 64,
        "--clients": 40,
        "--subspace_ratio": 0.15,
        "--l": 0.001,
        "--seed": 2
      },
      "dataset": "Start with IID splits; randomly flip 20% of training labels per client to simulate adversarial/noisy conditions.",
      "expected_insight": "SFedAvg’s projected momentum should be more robust to label noise, yielding lower training loss and higher test accuracy than FedAvg, with reduced variance across rounds.",
      "metrics": ["train_loss_fedavg", "train_loss_sfedavg", "test_acc_fedavg", "test_acc_sfedavg", "comm_MB_fedavg", "comm_MB_sfedavg"]
    },
    {
      "name": "Scalability: Many Clients and Higher Dimensional Features",
      "description": "Test behavior as the number of clients and feature dimensionality grow, examining convergence speed and communication efficiency.",
      "parameters": {
        "--rounds": 30,
        "--local_steps": 3,
        "--client_frac": 0.1,
        "--lr": 0.1,
        "--momentum": 0.9,
        "--batch_size": 128,
        "--clients": 200,
        "--subspace_ratio": 0.05,
        "--l": 0.0001,
        "--seed": 3
      },
      "dataset": "Augment features (e.g., random linear feature expansion or padded PCA components) to increase dimensionality; split IID across 200 clients.",
      "expected_insight": "SFedAvg should maintain stable convergence despite many small clients, while FedAvg may exhibit slower convergence; communication curves highlight the trade-off between projector transmission and reduced drift.",
      "metrics": ["train_loss_fedavg", "train_loss_sfedavg", "test_acc_fedavg", "test_acc_sfedavg", "comm_MB_fedavg", "comm_MB_sfedavg"]
    },
    {
      "name": "Hyperparameter Sensitivity: Subspace Dimension (r/d)",
      "description": "Probe how the projection dimension influences convergence, comparing SFedAvg performance as r/d varies.",
      "parameters": {
        "--rounds": 60,
        "--local_steps": 5,
        "--client_frac": 0.2,
        "--lr": 0.12,
        "--momentum": 0.9,
        "--batch_size": 64,
        "--clients": 60,
        "--subspace_ratio": 0.1,
        "--l": 0.0005,
        "--seed": 4
      },
      "dataset": "IID splits on the base dataset; repeat runs varying --subspace_ratio ∈ {0.05, 0.1, 0.25} to form a sensitivity curve.",
      "expected_insight": "Expect a sweet spot for r/d where SFedAvg converges faster and generalizes better; too small r slows learning, while too large r reduces projection benefits and approaches momentum SGD.",
      "metrics": ["train_loss_sfedavg", "test_acc_sfedavg", "comm_MB_sfedavg"]
    },
    {
      "name": "Edge Case: Sparse Participation with Long Local Blocks",
      "description": "Stress-test stability when few clients participate per round and perform many local updates, increasing client drift.",
      "parameters": {
        "--rounds": 50,
        "--local_steps": 20,
        "--client_frac": 0.05,
        "--lr": 0.08,
        "--momentum": 0.9,
        "--batch_size": 64,
        "--clients": 100,
        "--subspace_ratio": 0.1,
        "--l": 0.0005,
        "--seed": 5
      },
      "dataset": "Non-IID splits with strong label skew and unequal sample counts per client to maximize drift.",
      "expected_insight": "FedAvg should exhibit pronounced drift and oscillations, whereas SFedAvg with MP stabilizes updates, yielding lower loss and higher accuracy for the same communication budget.",
      "metrics": ["train_loss_fedavg", "train_loss_sfedavg", "test_acc_fedavg", "test_acc_sfedavg", "comm_MB_fedavg", "comm_MB_sfedavg"]
    }
  ],
  "rationale": "These scenarios directly exercise elements specified in the pseudocode—round-based aggregation, client sampling, one-sided subspace projection Π_t, and momentum projection at block start. They span heterogeneity (label skew), robustness (label noise), scalability (many clients and higher dimensional features), hyperparameter sensitivity (varying subspace ratio r/d), and edge cases (sparse participation with long local blocks). Together they reveal how SFedAvg’s projection and MP mechanisms influence convergence, accuracy, and communication compared to FedAvg."
}
