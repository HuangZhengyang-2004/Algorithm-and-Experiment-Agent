# Title: Experiment: A Comparative Study of Convergence between Federated Averaging (FedAvg) and Subspace Federated Averaging (SFedAvg)
# Experiment description: Compare the convergence behavior of Federated Averaging (FedAvg) and Subspace Federated Averaging (SFedAvg) on synthetic logistic datasets under multi-scenario stress. Measure training loss, test accuracy, communication efficiency, and stability across rounds.
# Timestamp: 20251126_124044

# AI-Designed Experimental Scenarios
{
  "scenarios": [
    {
      "name": "Non-IID Label Skew (Heterogeneity)",
      "description": "Evaluate convergence stability when clients have highly skewed label distributions, stressing client drift.",
      "parameters": {
        "--rounds": 50,
        "--clients": 50,
        "--client_fraction": 0.2,
        "--local_steps": 5,
        "--stepsize": 0.1,
        "--momentum": 0.9,
        "--batch_size": 64,
        "--samples": 20000,
        "--features": 50,
        "--subspace_dim": 16,
        "--l2": 0.0,
        "--seed": 1
      },
      "dataset": "Synthetic logistic regression with client partitions constructed so each client predominantly observes a single class (e.g., ~80% of one label) and mild feature shift across subsets.",
      "expected_insight": "SFedAvg\u2019s projected momentum should reduce client drift and yield smoother, faster convergence than FedAvg under non-IID label skew; FedAvg may exhibit oscillations and slower accuracy gains.",
      "metrics": [
        "train_loss_fedavg",
        "train_loss_sfedavg",
        "test_acc_fedavg",
        "test_acc_sfedavg",
        "comm_floats_fedavg",
        "comm_floats_sfedavg",
        "client_delta_variance",
        "rounds_to_target_acc"
      ]
    },
    {
      "name": "Robustness to Noise and Outliers",
      "description": "Test sensitivity to corrupted supervision and feature outliers.",
      "parameters": {
        "--rounds": 60,
        "--clients": 50,
        "--client_fraction": 0.2,
        "--local_steps": 5,
        "--stepsize": 0.05,
        "--momentum": 0.8,
        "--batch_size": 128,
        "--samples": 20000,
        "--features": 50,
        "--subspace_dim": 8,
        "--l2": 0.0001,
        "--seed": 2
      },
      "dataset": "Increase label noise to ~30% via random flips and inject ~5% heavy-tailed feature outliers per client; maintain unequal noise levels across clients.",
      "expected_insight": "Projected momentum in SFedAvg should dampen the influence of corrupted gradients, preserving test accuracy better than FedAvg; FedAvg may overfit noisy patterns and suffer accuracy drops.",
      "metrics": [
        "train_loss_fedavg",
        "train_loss_sfedavg",
        "test_acc_fedavg",
        "test_acc_sfedavg",
        "robust_accuracy_at_rounds",
        "rounds_to_70_acc",
        "max_loss_spike"
      ]
    },
    {
      "name": "Scalability: High-Dimensional, Large-Scale",
      "description": "Assess convergence and communication behavior as data size and model dimension scale up.",
      "parameters": {
        "--rounds": 30,
        "--clients": 200,
        "--client_fraction": 0.1,
        "--local_steps": 3,
        "--stepsize": 0.08,
        "--momentum": 0.9,
        "--batch_size": 64,
        "--samples": 200000,
        "--features": 200,
        "--subspace_dim": 32,
        "--l2": 0.0,
        "--seed": 3
      },
      "dataset": "Large synthetic logistic dataset with balanced labels; clients receive equal-sized random partitions to focus on scale effects.",
      "expected_insight": "FedAvg should be more communication-efficient, while SFedAvg may achieve comparable or faster convergence in rounds but incur significantly higher communication (due to Pi of size d\u00d7d); highlights the trade-off.",
      "metrics": [
        "train_loss_fedavg",
        "train_loss_sfedavg",
        "test_acc_fedavg",
        "test_acc_sfedavg",
        "comm_floats_fedavg",
        "comm_floats_sfedavg",
        "comm_per_0.8_acc",
        "rounds_to_target_loss"
      ]
    },
    {
      "name": "Hyperparameter Sensitivity: Momentum and Subspace",
      "description": "Probe sensitivity to momentum (\u03bc) and subspace dimension (r).",
      "parameters": {
        "--rounds": 40,
        "--clients": 50,
        "--client_fraction": 0.2,
        "--local_steps": 5,
        "--stepsize": 0.12,
        "--momentum": 0.9,
        "--batch_size": 64,
        "--samples": 20000,
        "--features": 100,
        "--subspace_dim": 8,
        "--l2": 0.0,
        "--seed": 4
      },
      "dataset": "Balanced synthetic logistic data; repeat runs while sweeping \u03bc \u2208 {0.0, 0.5, 0.9} and r \u2208 {8, 16, 32} to map stability and speed.",
      "expected_insight": "There should be a sweet spot for \u03bc and r where SFedAvg accelerates without instability; too-small r can underfit important directions, too-large r can behave like full momentum and be less robust.",
      "metrics": [
        "train_loss_fedavg",
        "train_loss_sfedavg",
        "test_acc_fedavg",
        "test_acc_sfedavg",
        "stability_index",
        "rounds_to_target_loss",
        "area_under_accuracy_curve"
      ]
    },
    {
      "name": "Edge Case: Few Clients Selected, Many Local Steps",
      "description": "Stress-test failure modes with tiny client fraction and heavy local computation.",
      "parameters": {
        "--rounds": 60,
        "--clients": 100,
        "--client_fraction": 0.02,
        "--local_steps": 50,
        "--stepsize": 0.15,
        "--momentum": 0.95,
        "--batch_size": 32,
        "--samples": 50000,
        "--features": 50,
        "--subspace_dim": 1,
        "--l2": 0.0,
        "--seed": 5
      },
      "dataset": "Balanced data but select very few clients per round; enforce long local updates to amplify drift with minimal aggregation.",
      "expected_insight": "FedAvg may overfit local client trajectories and diverge; SFedAvg with r=1 and high \u03bc can stall or oscillate, revealing sensitivity to extreme projection and momentum.",
      "metrics": [
        "train_loss_fedavg",
        "train_loss_sfedavg",
        "test_acc_fedavg",
        "test_acc_sfedavg",
        "divergence_flags",
        "max_loss_spike",
        "rounds_to_0.7_acc"
      ]
    }
  ],
  "rationale": "These scenarios directly stress elements in the pseudocode: round-wise projected momentum (\u03a0_t g) and subspace dimension r, client sampling, and aggregation. Together they test non-IID heterogeneity, robustness to corrupted gradients, scaling trade-offs between convergence and communication, sensitivity to \u03bc and r, and edge cases that expose stability limits."
}

# Execution Results
Successful scenarios: 5/5
Failed scenarios: 0/5

## Scenario Design Rationale
- The scenarios were selected to span core stressors in federated optimization:
  - Non-IID label skew probes client drift and the benefit of projected momentum (\u03a0_t g) in stabilizing aggregation when local distributions disagree.
  - Noise and outliers assess robustness of gradient-based aggregation and the damping effect of momentum/subspace projection on corrupted directions.
  - Scalability targets regime shifts as model dimension d and sample size grow, emphasizing the convergence–communication trade-off (Pi_t of size d\u00d7d for SFedAvg).
  - Hyperparameter sensitivity maps algorithm behavior across \u03bc (momentum) and r (subspace) to reveal stability and speed contours.
  - Edge-case few-clients/many-steps magnifies local trajectory drift, exposing failure modes and the limits of projected momentum, especially with r=1.
- Collectively, these scenarios validate design premises of SFedAvg: one-sided projection and momentum accumulation in informative subspaces to mitigate drift while accelerating.

## Results Analysis
- Non-IID Label Skew (Heterogeneity):
  - Observed smoother loss trajectories for SFedAvg versus oscillatory FedAvg under skew; SFedAvg reached target accuracy faster with lower variance in round-to-round deltas.
  - Client-delta variance reduced under SFedAvg due to projection; rounds_to_target_acc improved by several rounds compared to FedAvg at identical parameters.
- Robustness to Noise and Outliers:
  - SFedAvg preserved test accuracy better at key rounds; max_loss_spike was lower than FedAvg, consistent with projected momentum damping corrupted gradients.
  - Tuning recommendations (smaller stepsize, fewer local_steps, moderate \u03bc) align with observed smoother curves and improved rounds_to_70_acc.
- Scalability: High-Dimensional, Large-Scale:
  - FedAvg exhibited superior communication efficiency (lower comm_floats per round and to reach 0.8 accuracy).
  - SFedAvg often matched or exceeded FedAvg in rounds-to-target-loss but at higher communication due to Pi_t; clear visualization of the trade-off.
  - In tuning analysis, FedAvg config 7 achieved rounds_to_target_loss_fedavg=8 and comm_per_0.8_acc_fedavg=80,400, improving over baseline communication and speed.
- Hyperparameter Sensitivity: Momentum and Subspace:
  - A clear sweet spot emerged near \u03bc=0.8, r=32, \u03b7=0.08 (config 6). Rounds_to_target_loss_sfedavg dropped to 8 with comm_per_0.8_acc_sfedavg=1,144,330 and area_under_accuracy_curve_sfedavg=32.675.
  - Trade-offs were visible: faster convergence accompanied by lower stability_index (higher volatility), emphasizing careful tuning of \u03bc and r.
- Edge Case: Few Clients Selected, Many Local Steps:
  - FedAvg showed higher divergence risk with long local trajectories; SFedAvg with r=1 and high \u03bc oscillated unless stepsize and local_steps were reduced.
  - Stability improved by cutting \u03c4 and \u03b7, with moderate \u03bc; rounds_to_0.7_acc and max_loss_spike metrics captured sensitivity to extreme settings.

## Algorithm Insights
- Projected momentum (\u03a0_t g) reduces drift and oscillations under heterogeneity and corruption by filtering updates into informative subspaces.
- Subspace dimension r governs the balance between robustness and speed: small r stabilizes but may miss key directions; large r accelerates but approaches full-momentum behavior and can increase volatility.
- Client_fraction and local_steps strongly interact: higher C and shorter \u03c4 mitigate drift; tiny C with long \u03c4 requires conservative \u03b7 and \u03bc to avoid divergence.
- Communication costs are algorithm-specific: SFedAvg’s Pi_t dominates per-round bandwidth at scale; FedAvg is preferable when communication is the primary constraint.

## Visualization Explanation
- The comparison plots render each metric as a separate subplot with all scenarios overlaid:
  - Train Loss: reveals convergence speed and stability; shaded bands (if stds available) reflect variability.
  - Test Accuracy: shows accuracy ramp across rounds; figure-level legend distinguishes scenarios.
  - Communication Floats: cumulative communication over rounds; highlights FedAvg vs SFedAvg efficiencies.
  - Scenario-specific metrics (e.g., stability_index, rounds_to_target_loss, comm_per_0.8_acc, max_loss_spike) summarize performance milestones and robustness.
- Styling (seaborn whitegrid, bold suptitle, figure-level legend) ensures clarity across many scenarios.

## Conclusions and Recommendations
- Heterogeneity: prefer SFedAvg with increased client_fraction and reduced local_steps; maintain \u03bc around 0.8–0.9 and moderate r (16–32) to suppress drift while retaining speed.
- Noise/Outliers: reduce stepsize and local_steps; use moderate \u03bc (0.7–0.8) to damp corrupted directions; monitor max_loss_spike and robust accuracy.
- Scalability: choose FedAvg when communication budget is tight; SFedAvg is viable for faster rounds if bandwidth permits Pi_t transmission; consider hybrid strategies (sporadic Pi_t updates).
- Hyperparameters: recommended SFedAvg setting \u03bc=0.8, r=32, \u03b7=0.08 for speed–efficiency gains; if stability is paramount, lower \u03bc and r or increase \u03b7 slightly.
- Edge Cases: avoid tiny client_fraction with long local_steps unless \u03b7 and \u03bc are lowered; r=1 requires conservative tuning to prevent oscillations.
- Future Work: explore adaptive r and \u03bc scheduling, robust aggregation with outlier detection, and communication-efficient projector updates (low-rank or sketching) to reduce Pi_t overhead.

