{
  "scenarios": [
    {
      "name": "Non-IID Label Skew (Heterogeneity)",
      "description": "Evaluate convergence stability when clients have highly skewed label distributions, stressing client drift.",
      "parameters": {
        "--rounds": 50,
        "--clients": 50,
        "--client_fraction": 0.2,
        "--local_steps": 5,
        "--stepsize": 0.1,
        "--momentum": 0.9,
        "--batch_size": 64,
        "--samples": 20000,
        "--features": 50,
        "--subspace_dim": 16,
        "--l2": 0.0,
        "--seed": 1
      },
      "dataset": "Synthetic logistic regression with client partitions constructed so each client predominantly observes a single class (e.g., ~80% of one label) and mild feature shift across subsets.",
      "expected_insight": "SFedAvg’s projected momentum should reduce client drift and yield smoother, faster convergence than FedAvg under non-IID label skew; FedAvg may exhibit oscillations and slower accuracy gains.",
      "metrics": ["train_loss_fedavg", "train_loss_sfedavg", "test_acc_fedavg", "test_acc_sfedavg", "comm_floats_fedavg", "comm_floats_sfedavg", "client_delta_variance", "rounds_to_target_acc"]
    },
    {
      "name": "Robustness to Noise and Outliers",
      "description": "Test sensitivity to corrupted supervision and feature outliers.",
      "parameters": {
        "--rounds": 60,
        "--clients": 50,
        "--client_fraction": 0.2,
        "--local_steps": 5,
        "--stepsize": 0.05,
        "--momentum": 0.8,
        "--batch_size": 128,
        "--samples": 20000,
        "--features": 50,
        "--subspace_dim": 8,
        "--l2": 0.0001,
        "--seed": 2
      },
      "dataset": "Increase label noise to ~30% via random flips and inject ~5% heavy-tailed feature outliers per client; maintain unequal noise levels across clients.",
      "expected_insight": "Projected momentum in SFedAvg should dampen the influence of corrupted gradients, preserving test accuracy better than FedAvg; FedAvg may overfit noisy patterns and suffer accuracy drops.",
      "metrics": ["train_loss_fedavg", "train_loss_sfedavg", "test_acc_fedavg", "test_acc_sfedavg", "robust_accuracy_at_rounds", "rounds_to_70_acc", "max_loss_spike"]
    },
    {
      "name": "Scalability: High-Dimensional, Large-Scale",
      "description": "Assess convergence and communication behavior as data size and model dimension scale up.",
      "parameters": {
        "--rounds": 30,
        "--clients": 200,
        "--client_fraction": 0.1,
        "--local_steps": 3,
        "--stepsize": 0.08,
        "--momentum": 0.9,
        "--batch_size": 64,
        "--samples": 200000,
        "--features": 200,
        "--subspace_dim": 32,
        "--l2": 0.0,
        "--seed": 3
      },
      "dataset": "Large synthetic logistic dataset with balanced labels; clients receive equal-sized random partitions to focus on scale effects.",
      "expected_insight": "FedAvg should be more communication-efficient, while SFedAvg may achieve comparable or faster convergence in rounds but incur significantly higher communication (due to Pi of size d×d); highlights the trade-off.",
      "metrics": ["train_loss_fedavg", "train_loss_sfedavg", "test_acc_fedavg", "test_acc_sfedavg", "comm_floats_fedavg", "comm_floats_sfedavg", "comm_per_0.8_acc", "rounds_to_target_loss"]
    },
    {
      "name": "Hyperparameter Sensitivity: Momentum and Subspace",
      "description": "Probe sensitivity to momentum (μ) and subspace dimension (r).",
      "parameters": {
        "--rounds": 40,
        "--clients": 50,
        "--client_fraction": 0.2,
        "--local_steps": 5,
        "--stepsize": 0.12,
        "--momentum": 0.9,
        "--batch_size": 64,
        "--samples": 20000,
        "--features": 100,
        "--subspace_dim": 8,
        "--l2": 0.0,
        "--seed": 4
      },
      "dataset": "Balanced synthetic logistic data; repeat runs while sweeping μ ∈ {0.0, 0.5, 0.9} and r ∈ {8, 16, 32} to map stability and speed.",
      "expected_insight": "There should be a sweet spot for μ and r where SFedAvg accelerates without instability; too-small r can underfit important directions, too-large r can behave like full momentum and be less robust.",
      "metrics": ["train_loss_fedavg", "train_loss_sfedavg", "test_acc_fedavg", "test_acc_sfedavg", "stability_index", "rounds_to_target_loss", "area_under_accuracy_curve"]
    },
    {
      "name": "Edge Case: Few Clients Selected, Many Local Steps",
      "description": "Stress-test failure modes with tiny client fraction and heavy local computation.",
      "parameters": {
        "--rounds": 60,
        "--clients": 100,
        "--client_fraction": 0.02,
        "--local_steps": 50,
        "--stepsize": 0.15,
        "--momentum": 0.95,
        "--batch_size": 32,
        "--samples": 50000,
        "--features": 50,
        "--subspace_dim": 1,
        "--l2": 0.0,
        "--seed": 5
      },
      "dataset": "Balanced data but select very few clients per round; enforce long local updates to amplify drift with minimal aggregation.",
      "expected_insight": "FedAvg may overfit local client trajectories and diverge; SFedAvg with r=1 and high μ can stall or oscillate, revealing sensitivity to extreme projection and momentum.",
      "metrics": ["train_loss_fedavg", "train_loss_sfedavg", "test_acc_fedavg", "test_acc_sfedavg", "divergence_flags", "max_loss_spike", "rounds_to_0.7_acc"]
    }
  ],
  "rationale": "These scenarios directly stress elements in the pseudocode: round-wise projected momentum (Π_t g) and subspace dimension r, client sampling, and aggregation. Together they test non-IID heterogeneity, robustness to corrupted gradients, scaling trade-offs between convergence and communication, sensitivity to μ and r, and edge cases that expose stability limits."
}
