{
  "scenario_design": {
    "scenarios": [
      {
        "name": "Non-IID Class Skew (Dirichlet α=0.1)",
        "description": "Evaluate convergence of FedAvg vs SFedAvg under strong client heterogeneity induced by class-imbalanced partitions.",
        "parameters": {
          "--rounds": 50,
          "--local_steps": 5,
          "--client_fraction": 0.2,
          "--stepsize": 0.2,
          "--momentum": 0.9,
          "--batch_size": 64,
          "--num_clients": 50,
          "--subspace_dim": 64,
          "--seed": 1,
          "--dataset": "mnist_or_synth"
        },
        "dataset": "Create non-IID client splits by sampling client label distributions from a Dirichlet with α=0.1; each client primarily sees a few classes.",
        "expected_insight": "SFedAvg should exhibit more stable convergence and reduced drift compared to FedAvg, achieving better loss and accuracy at similar or lower communication costs due to subspace filtering.",
        "metrics": [
          "FedAvg/train_loss",
          "FedAvg/test_accuracy",
          "FedAvg/communication_bytes",
          "SFedAvg/train_loss",
          "SFedAvg/test_accuracy",
          "SFedAvg/communication_bytes"
        ]
      },
      {
        "name": "Robustness to Label Noise (20% flips)",
        "description": "Test sensitivity of both algorithms to noisy labels with momentum projection mitigating variance.",
        "parameters": {
          "--rounds": 40,
          "--local_steps": 4,
          "--client_fraction": 0.3,
          "--stepsize": 0.15,
          "--momentum": 0.9,
          "--batch_size": 128,
          "--num_clients": 30,
          "--subspace_dim": 48,
          "--seed": 2,
          "--dataset": "mnist_or_synth"
        },
        "dataset": "Inject 20% symmetric label noise uniformly across clients; maintain original feature distribution.",
        "expected_insight": "SFedAvg should be more robust due to one-sided projection that dampens high-variance gradient components, leading to lower loss and higher accuracy than FedAvg in noisy conditions.",
        "metrics": [
          "FedAvg/train_loss",
          "FedAvg/test_accuracy",
          "FedAvg/communication_bytes",
          "SFedAvg/train_loss",
          "SFedAvg/test_accuracy",
          "SFedAvg/communication_bytes"
        ]
      },
      {
        "name": "Scalability: Many Clients, Small Client Fraction",
        "description": "Assess behavior as the system scales to many clients with few selected per round, stressing aggregation and communication.",
        "parameters": {
          "--rounds": 60,
          "--local_steps": 3,
          "--client_fraction": 0.05,
          "--stepsize": 0.12,
          "--momentum": 0.9,
          "--batch_size": 64,
          "--num_clients": 200,
          "--subspace_dim": 32,
          "--seed": 3,
          "--dataset": "mnist_or_synth"
        },
        "dataset": "Uniform splits over a larger client pool; optional mild heterogeneity by varying per-client sample sizes.",
        "expected_insight": "SFedAvg should reduce per-round communication growth while maintaining or accelerating convergence compared to FedAvg when only a small subset participates.",
        "metrics": [
          "FedAvg/train_loss",
          "FedAvg/test_accuracy",
          "FedAvg/communication_bytes",
          "SFedAvg/train_loss",
          "SFedAvg/test_accuracy",
          "SFedAvg/communication_bytes"
        ]
      },
      {
        "name": "Hyperparameter Sensitivity: r and μ Ablation",
        "description": "Systematically study the effect of subspace dimension r and momentum μ on convergence speed and stability.",
        "parameters": {
          "--rounds": 45,
          "--local_steps": 4,
          "--client_fraction": 0.25,
          "--stepsize": 0.18,
          "--momentum": 0.0,
          "--batch_size": 128,
          "--num_clients": 40,
          "--subspace_dim": 16,
          "--seed": 4,
          "--dataset": "mnist_or_synth"
        },
        "dataset": "Use a standard iid split; run multiple separate executions varying --subspace_dim ∈ {16, 64, 256} and --momentum ∈ {0.0, 0.9}.",
        "expected_insight": "Larger r typically speeds convergence but increases communication; nonzero μ reduces oscillations and improves accuracy, while very small r may underfit.",
        "metrics": [
          "FedAvg/train_loss",
          "FedAvg/test_accuracy",
          "FedAvg/communication_bytes",
          "SFedAvg/train_loss",
          "SFedAvg/test_accuracy",
          "SFedAvg/communication_bytes"
        ]
      },
      {
        "name": "Edge Cases: Extreme Local Steps and Minimal Subspace",
        "description": "Reveal failure modes under high local drift and aggressive projection.",
        "parameters": {
          "--rounds": 30,
          "--local_steps": 20,
          "--client_fraction": 0.1,
          "--stepsize": 0.1,
          "--momentum": 0.9,
          "--batch_size": 64,
          "--num_clients": 25,
          "--subspace_dim": 1,
          "--seed": 5,
          "--dataset": "mnist_or_synth"
        },
        "dataset": "Strong local computation per client on iid splits; set subspace_dim=1 to maximize projection aggressiveness.",
        "expected_insight": "FedAvg may suffer from client drift with high τ, whereas SFedAvg can mitigate drift via projection but may slow learning when r is too small; communication for SFedAvg increases only with r.",
        "metrics": [
          "FedAvg/train_loss",
          "FedAvg/test_accuracy",
          "FedAvg/communication_bytes",
          "SFedAvg/train_loss",
          "SFedAvg/test_accuracy",
          "SFedAvg/communication_bytes"
        ]
      }
    ],
    "rationale": "The scenarios are anchored in the algorithm title and pseudocode: they isolate effects of subspace projection Π_t, momentum projection at block start, client sampling, and aggregation. Together they probe heterogeneity (Dirichlet skew), robustness (label noise), scalability (many clients, small fraction), hyperparameter sensitivity (r, μ ablation), and edge cases (extreme τ and minimal r), yielding a comprehensive view of convergence and communication trade-offs between FedAvg and SFedAvg."
  },
  "execution_results": [
    {
      "name": "non-iid_class_skew_(dirichlet_α=0.1)",
      "description": "Evaluate convergence of FedAvg vs SFedAvg under strong client heterogeneity induced by class-imbalanced partitions.",
      "dataset": "Create non-IID client splits by sampling client label distributions from a Dirichlet with α=0.1; each client primarily sees a few classes.",
      "parameters": {
        "--rounds": 50,
        "--local_steps": 5,
        "--client_fraction": 0.2,
        "--stepsize": 0.2,
        "--momentum": 0.9,
        "--batch_size": 64,
        "--num_clients": 50,
        "--subspace_dim": 64,
        "--seed": 1,
        "--dataset": "mnist_or_synth"
      },
      "expected_insight": "SFedAvg should exhibit more stable convergence and reduced drift compared to FedAvg, achieving better loss and accuracy at similar or lower communication costs due to subspace filtering.",
      "status": "success",
      "run_dir": "run_non-iid_class_skew_(dirichlet_α=0.1)",
      "attempts": 1,
      "implementation_note": "Uses initial generated code"
    },
    {
      "name": "robustness_to_label_noise_(20%_flips)",
      "description": "Test sensitivity of both algorithms to noisy labels with momentum projection mitigating variance.",
      "dataset": "Inject 20% symmetric label noise uniformly across clients; maintain original feature distribution.",
      "parameters": {
        "--rounds": 40,
        "--local_steps": 4,
        "--client_fraction": 0.3,
        "--stepsize": 0.15,
        "--momentum": 0.9,
        "--batch_size": 128,
        "--num_clients": 30,
        "--subspace_dim": 48,
        "--seed": 2,
        "--dataset": "mnist_or_synth"
      },
      "expected_insight": "SFedAvg should be more robust due to one-sided projection that dampens high-variance gradient components, leading to lower loss and higher accuracy than FedAvg in noisy conditions.",
      "status": "success",
      "run_dir": "run_robustness_to_label_noise_(20%_flips)",
      "attempts": 1,
      "implementation_note": "Code was customized for this scenario's specific requirements"
    },
    {
      "name": "scalability:_many_clients,_small_client_fraction",
      "description": "Assess behavior as the system scales to many clients with few selected per round, stressing aggregation and communication.",
      "dataset": "Uniform splits over a larger client pool; optional mild heterogeneity by varying per-client sample sizes.",
      "parameters": {
        "--rounds": 60,
        "--local_steps": 3,
        "--client_fraction": 0.05,
        "--stepsize": 0.12,
        "--momentum": 0.9,
        "--batch_size": 64,
        "--num_clients": 200,
        "--subspace_dim": 32,
        "--seed": 3,
        "--dataset": "mnist_or_synth"
      },
      "expected_insight": "SFedAvg should reduce per-round communication growth while maintaining or accelerating convergence compared to FedAvg when only a small subset participates.",
      "status": "success",
      "run_dir": "run_scalability:_many_clients,_small_client_fraction",
      "attempts": 1,
      "implementation_note": "Code was customized for this scenario's specific requirements"
    },
    {
      "name": "hyperparameter_sensitivity:_r_and_μ_ablation",
      "description": "Systematically study the effect of subspace dimension r and momentum μ on convergence speed and stability.",
      "dataset": "Use a standard iid split; run multiple separate executions varying --subspace_dim ∈ {16, 64, 256} and --momentum ∈ {0.0, 0.9}.",
      "parameters": {
        "--rounds": 45,
        "--local_steps": 4,
        "--client_fraction": 0.25,
        "--stepsize": 0.18,
        "--momentum": 0.0,
        "--batch_size": 128,
        "--num_clients": 40,
        "--subspace_dim": 16,
        "--seed": 4,
        "--dataset": "mnist_or_synth"
      },
      "expected_insight": "Larger r typically speeds convergence but increases communication; nonzero μ reduces oscillations and improves accuracy, while very small r may underfit.",
      "status": "success",
      "run_dir": "run_hyperparameter_sensitivity:_r_and_μ_ablation",
      "attempts": 1,
      "implementation_note": "Code was customized for this scenario's specific requirements"
    },
    {
      "name": "edge_cases:_extreme_local_steps_and_minimal_subspace",
      "description": "Reveal failure modes under high local drift and aggressive projection.",
      "dataset": "Strong local computation per client on iid splits; set subspace_dim=1 to maximize projection aggressiveness.",
      "parameters": {
        "--rounds": 30,
        "--local_steps": 20,
        "--client_fraction": 0.1,
        "--stepsize": 0.1,
        "--momentum": 0.9,
        "--batch_size": 64,
        "--num_clients": 25,
        "--subspace_dim": 1,
        "--seed": 5,
        "--dataset": "mnist_or_synth"
      },
      "expected_insight": "FedAvg may suffer from client drift with high τ, whereas SFedAvg can mitigate drift via projection but may slow learning when r is too small; communication for SFedAvg increases only with r.",
      "status": "success",
      "run_dir": "run_edge_cases:_extreme_local_steps_and_minimal_subspace",
      "attempts": 1,
      "implementation_note": "Code was customized for this scenario's specific requirements"
    }
  ]
}