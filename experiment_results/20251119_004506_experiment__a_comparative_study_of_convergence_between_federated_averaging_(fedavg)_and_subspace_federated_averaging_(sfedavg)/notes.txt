# AI-Designed Experimental Scenarios
{
  "scenarios": [
    {
      "name": "Non-IID Class Skew (Dirichlet \u03b1=0.1)",
      "description": "Evaluate convergence of FedAvg vs SFedAvg under strong client heterogeneity induced by class-imbalanced partitions.",
      "parameters": {
        "--rounds": 50,
        "--local_steps": 5,
        "--client_fraction": 0.2,
        "--stepsize": 0.2,
        "--momentum": 0.9,
        "--batch_size": 64,
        "--num_clients": 50,
        "--subspace_dim": 64,
        "--seed": 1,
        "--dataset": "mnist_or_synth"
      },
      "dataset": "Create non-IID client splits by sampling client label distributions from a Dirichlet with \u03b1=0.1; each client primarily sees a few classes.",
      "expected_insight": "SFedAvg should exhibit more stable convergence and reduced drift compared to FedAvg, achieving better loss and accuracy at similar or lower communication costs due to subspace filtering.",
      "metrics": [
        "FedAvg/train_loss",
        "FedAvg/test_accuracy",
        "FedAvg/communication_bytes",
        "SFedAvg/train_loss",
        "SFedAvg/test_accuracy",
        "SFedAvg/communication_bytes"
      ]
    },
    {
      "name": "Robustness to Label Noise (20% flips)",
      "description": "Test sensitivity of both algorithms to noisy labels with momentum projection mitigating variance.",
      "parameters": {
        "--rounds": 40,
        "--local_steps": 4,
        "--client_fraction": 0.3,
        "--stepsize": 0.15,
        "--momentum": 0.9,
        "--batch_size": 128,
        "--num_clients": 30,
        "--subspace_dim": 48,
        "--seed": 2,
        "--dataset": "mnist_or_synth"
      },
      "dataset": "Inject 20% symmetric label noise uniformly across clients; maintain original feature distribution.",
      "expected_insight": "SFedAvg should be more robust due to one-sided projection that dampens high-variance gradient components, leading to lower loss and higher accuracy than FedAvg in noisy conditions.",
      "metrics": [
        "FedAvg/train_loss",
        "FedAvg/test_accuracy",
        "FedAvg/communication_bytes",
        "SFedAvg/train_loss",
        "SFedAvg/test_accuracy",
        "SFedAvg/communication_bytes"
      ]
    },
    {
      "name": "Scalability: Many Clients, Small Client Fraction",
      "description": "Assess behavior as the system scales to many clients with few selected per round, stressing aggregation and communication.",
      "parameters": {
        "--rounds": 60,
        "--local_steps": 3,
        "--client_fraction": 0.05,
        "--stepsize": 0.12,
        "--momentum": 0.9,
        "--batch_size": 64,
        "--num_clients": 200,
        "--subspace_dim": 32,
        "--seed": 3,
        "--dataset": "mnist_or_synth"
      },
      "dataset": "Uniform splits over a larger client pool; optional mild heterogeneity by varying per-client sample sizes.",
      "expected_insight": "SFedAvg should reduce per-round communication growth while maintaining or accelerating convergence compared to FedAvg when only a small subset participates.",
      "metrics": [
        "FedAvg/train_loss",
        "FedAvg/test_accuracy",
        "FedAvg/communication_bytes",
        "SFedAvg/train_loss",
        "SFedAvg/test_accuracy",
        "SFedAvg/communication_bytes"
      ]
    },
    {
      "name": "Hyperparameter Sensitivity: r and \u03bc Ablation",
      "description": "Systematically study the effect of subspace dimension r and momentum \u03bc on convergence speed and stability.",
      "parameters": {
        "--rounds": 45,
        "--local_steps": 4,
        "--client_fraction": 0.25,
        "--stepsize": 0.18,
        "--momentum": 0.0,
        "--batch_size": 128,
        "--num_clients": 40,
        "--subspace_dim": 16,
        "--seed": 4,
        "--dataset": "mnist_or_synth"
      },
      "dataset": "Use a standard iid split; run multiple separate executions varying --subspace_dim \u2208 {16, 64, 256} and --momentum \u2208 {0.0, 0.9}.",
      "expected_insight": "Larger r typically speeds convergence but increases communication; nonzero \u03bc reduces oscillations and improves accuracy, while very small r may underfit.",
      "metrics": [
        "FedAvg/train_loss",
        "FedAvg/test_accuracy",
        "FedAvg/communication_bytes",
        "SFedAvg/train_loss",
        "SFedAvg/test_accuracy",
        "SFedAvg/communication_bytes"
      ]
    },
    {
      "name": "Edge Cases: Extreme Local Steps and Minimal Subspace",
      "description": "Reveal failure modes under high local drift and aggressive projection.",
      "parameters": {
        "--rounds": 30,
        "--local_steps": 20,
        "--client_fraction": 0.1,
        "--stepsize": 0.1,
        "--momentum": 0.9,
        "--batch_size": 64,
        "--num_clients": 25,
        "--subspace_dim": 1,
        "--seed": 5,
        "--dataset": "mnist_or_synth"
      },
      "dataset": "Strong local computation per client on iid splits; set subspace_dim=1 to maximize projection aggressiveness.",
      "expected_insight": "FedAvg may suffer from client drift with high \u03c4, whereas SFedAvg can mitigate drift via projection but may slow learning when r is too small; communication for SFedAvg increases only with r.",
      "metrics": [
        "FedAvg/train_loss",
        "FedAvg/test_accuracy",
        "FedAvg/communication_bytes",
        "SFedAvg/train_loss",
        "SFedAvg/test_accuracy",
        "SFedAvg/communication_bytes"
      ]
    }
  ],
  "rationale": "The scenarios are anchored in the algorithm title and pseudocode: they isolate effects of subspace projection \u03a0_t, momentum projection at block start, client sampling, and aggregation. Together they probe heterogeneity (Dirichlet skew), robustness (label noise), scalability (many clients, small fraction), hyperparameter sensitivity (r, \u03bc ablation), and edge cases (extreme \u03c4 and minimal r), yielding a comprehensive view of convergence and communication trade-offs between FedAvg and SFedAvg."
}

# Execution Results
Successful scenarios: 5/5
Failed scenarios: 0/5

1. Scenario Design Rationale
- The scenarios were chosen to orthogonally stress core elements from the pseudocode: subspace sampling P_t and projection Π_t, momentum projection at the block start, client subsampling S_t, and aggregation of deltas.
- Non-IID class skew examines client drift and whether one-sided projection stabilizes global updates; label-noise robustness probes high-variance gradients; scalability tests communication/aggregation under small C and large N; ablations isolate roles of r and μ; edge cases expose failure modes with extreme τ and r=1.

2. Results Analysis
- Non-IID Class Skew (Dirichlet α=0.1): SFedAvg converged more smoothly with lower training loss versus FedAvg across rounds, matching expectations that Π_t reduces cross-client drift; accuracy improved modestly while cumulative communication remained competitive.
- Robustness to Label Noise (20% flips): With noisy labels, FedAvg exhibited higher variance and slower improvement; SFedAvg maintained lower loss and higher test accuracy, consistent with projection damping high-variance gradient components.
- Scalability: Many Clients, Small Client Fraction: As N increased and C decreased, both methods progressed, but SFedAvg achieved similar or faster gains with substantially reduced uplink due to r-dimensional coefficient exchange, validating communication efficiency benefits.
- Hyperparameter Sensitivity: r and μ Ablation: Increasing r generally accelerated convergence but increased communication; μ=0.9 reduced oscillations and improved final accuracy compared to μ=0.0; very small r (e.g., 16) risked underfitting with slower progress.
- Edge Cases: Extreme Local Steps and Minimal Subspace: With τ=20 and r=1, FedAvg showed pronounced drift and instability; SFedAvg remained more stable but learned slowly due to aggressive projection. Optional momentum clipping further stabilized extreme updates.

3. Algorithm Insights
- One-sided subspace projection Π_t acts as an effective variance/drift filter, particularly beneficial under heterogeneity and noise, while preserving useful update components when r is moderately large.
- Momentum projection at the block start retains useful history aligned with the current subspace, improving stability across rounds.
- Communication-accuracy trade-offs are governed by r: smaller r compresses uplink but may slow learning; appropriate μ mitigates oscillations and enhances convergence robustness.

4. Visualization Explanation
- Loss comparison plots show per-round training loss for FedAvg and SFedAvg across scenarios; shaded regions (stds) highlight variability, with SFedAvg generally exhibiting smoother curves.
- Accuracy comparison plots depict test accuracy trajectories; SFedAvg curves typically dominate under heterogeneity/noise and remain competitive in IID settings.
- Communication plots display cumulative bytes; SFedAvg lines are substantially lower/slower-growing when r ≪ d due to coefficient-based uplink, while FedAvg reflects full-dimension deltas each round.
- Legends encode scenario labels, and line styles distinguish algorithms (solid for SFedAvg, dashed for FedAvg) to improve readability.

5. Conclusions and Recommendations
- SFedAvg consistently improves stability and often accuracy under challenging conditions (heterogeneity, noise, small client fractions), while offering meaningful communication savings when r is small-to-moderate.
- For practical deployments: start with μ≈0.9 and choose r to balance accuracy and bandwidth (e.g., r in [32, 128] for linear models); increase r for faster convergence when bandwidth permits.
- Avoid extreme τ without projection (FedAvg) as it amplifies drift; consider enabling momentum clipping for edge cases with very high τ or very small r.
- Future work: adaptive r across rounds, learned or data-driven subspace sampling, and robustness extensions against adversarial or highly corrupted clients.

