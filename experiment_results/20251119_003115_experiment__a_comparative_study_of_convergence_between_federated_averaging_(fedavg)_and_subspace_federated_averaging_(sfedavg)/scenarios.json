{
  "scenarios": [
    {
      "name": "Baseline FedAvg Convergence",
      "description": "Establish baseline convergence behavior of standard Federated Averaging without subspace projection.",
      "parameters": {
        "--algo": "fedavg",
        "--rounds": 30,
        "--local_steps": 5,
        "--client_frac": 0.5,
        "--num_clients": 10,
        "--lr": 0.5,
        "--momentum": 0.9,
        "--batch_size": 32,
        "--r": 128,
        "--seed": 42
      },
      "dataset": "Standard digits classification with IID partitions; evenly split clients by samples.",
      "expected_insight": "Training and test loss should decrease steadily with accuracy increasing; provides a baseline to compare SFedAvg.",
      "metrics": ["train_loss", "test_loss", "test_accuracy", "communication_bytes"]
    },
    {
      "name": "Subspace SFedAvg Convergence (Moderate r)",
      "description": "Evaluate SFedAvg with one-sided random subspace and momentum projection to test convergence and communication efficiency.",
      "parameters": {
        "--algo": "sfedavg",
        "--rounds": 30,
        "--local_steps": 5,
        "--client_frac": 0.5,
        "--num_clients": 10,
        "--lr": 0.5,
        "--momentum": 0.9,
        "--batch_size": 32,
        "--r": 128,
        "--seed": 42
      },
      "dataset": "Same IID digits setup as baseline to isolate effect of subspace projection.",
      "expected_insight": "Convergence comparable to FedAvg with reduced communication overhead due to subspace projection; momentum projection stabilizes updates.",
      "metrics": ["train_loss", "test_loss", "test_accuracy", "communication_bytes"]
    },
    {
      "name": "Non-IID Heterogeneity Stress Test",
      "description": "Test performance under client distribution shift where each client has a skewed label distribution (heterogeneous data).",
      "parameters": {
        "--algo": "sfedavg",
        "--rounds": 40,
        "--local_steps": 10,
        "--client_frac": 0.5,
        "--num_clients": 20,
        "--lr": 0.3,
        "--momentum": 0.9,
        "--batch_size": 32,
        "--r": 96,
        "--seed": 123
      },
      "dataset": "Digits dataset partitioned non-IID via label skew (e.g., Dirichlet alphaâ‰ˆ0.3) so clients predominantly see a subset of classes.",
      "expected_insight": "SFedAvg should mitigate client drift via projected momentum; FedAvg may exhibit slower or oscillatory convergence under non-IID splits.",
      "metrics": ["train_loss", "test_loss", "test_accuracy", "communication_bytes"]
    },
    {
      "name": "Robustness to Label Noise",
      "description": "Assess sensitivity to corrupted labels and whether projection in SFedAvg improves robustness.",
      "parameters": {
        "--algo": "sfedavg",
        "--rounds": 40,
        "--local_steps": 8,
        "--client_frac": 0.5,
        "--num_clients": 20,
        "--lr": 0.4,
        "--momentum": 0.9,
        "--batch_size": 32,
        "--r": 80,
        "--seed": 7
      },
      "dataset": "Digits dataset with 20% labels randomly flipped per client; partitions remain balanced by sample count.",
      "expected_insight": "Projected gradients reduce variance from noisy labels, yielding steadier loss curves and potentially better final accuracy than FedAvg.",
      "metrics": ["train_loss", "test_loss", "test_accuracy", "communication_bytes"]
    },
    {
      "name": "Scalability and Edge Cases (Many Clients, Small r)",
      "description": "Evaluate scaling to many clients with small client fraction and extreme subspace dimension r to expose failure modes.",
      "parameters": {
        "--algo": "sfedavg",
        "--rounds": 50,
        "--local_steps": 5,
        "--client_frac": 0.1,
        "--num_clients": 100,
        "--lr": 0.6,
        "--momentum": 0.9,
        "--batch_size": 32,
        "--r": 16,
        "--seed": 99
      },
      "dataset": "Digits dataset replicated across 100 clients with IID or mildly skewed distributions; each round selects 10% of clients.",
      "expected_insight": "Communication cost grows sublinearly with clients due to low client fraction; very small r may slow convergence or underfit, revealing the trade-off.",
      "metrics": ["train_loss", "test_loss", "test_accuracy", "communication_bytes"]
    }
  ],
  "rationale": "The scenarios probe baseline behavior, effects of one-sided subspace and momentum projection, robustness under heterogeneity and label noise, and scalability with many clients and small r. This coverage aligns with the algorithm title and pseudocode emphasizing Stiefel-sampled subspaces, momentum projection at block start, client subsampling, and aggregation."
}
