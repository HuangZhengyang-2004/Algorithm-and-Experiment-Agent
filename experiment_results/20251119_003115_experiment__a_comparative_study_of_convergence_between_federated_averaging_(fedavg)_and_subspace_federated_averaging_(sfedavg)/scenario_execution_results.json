{
  "scenario_design": {
    "scenarios": [
      {
        "name": "Baseline FedAvg Convergence",
        "description": "Establish baseline convergence behavior of standard Federated Averaging without subspace projection.",
        "parameters": {
          "--algo": "fedavg",
          "--rounds": 30,
          "--local_steps": 5,
          "--client_frac": 0.5,
          "--num_clients": 10,
          "--lr": 0.5,
          "--momentum": 0.9,
          "--batch_size": 32,
          "--r": 128,
          "--seed": 42
        },
        "dataset": "Standard digits classification with IID partitions; evenly split clients by samples.",
        "expected_insight": "Training and test loss should decrease steadily with accuracy increasing; provides a baseline to compare SFedAvg.",
        "metrics": [
          "train_loss",
          "test_loss",
          "test_accuracy",
          "communication_bytes"
        ]
      },
      {
        "name": "Subspace SFedAvg Convergence (Moderate r)",
        "description": "Evaluate SFedAvg with one-sided random subspace and momentum projection to test convergence and communication efficiency.",
        "parameters": {
          "--algo": "sfedavg",
          "--rounds": 30,
          "--local_steps": 5,
          "--client_frac": 0.5,
          "--num_clients": 10,
          "--lr": 0.5,
          "--momentum": 0.9,
          "--batch_size": 32,
          "--r": 128,
          "--seed": 42
        },
        "dataset": "Same IID digits setup as baseline to isolate effect of subspace projection.",
        "expected_insight": "Convergence comparable to FedAvg with reduced communication overhead due to subspace projection; momentum projection stabilizes updates.",
        "metrics": [
          "train_loss",
          "test_loss",
          "test_accuracy",
          "communication_bytes"
        ]
      },
      {
        "name": "Non-IID Heterogeneity Stress Test",
        "description": "Test performance under client distribution shift where each client has a skewed label distribution (heterogeneous data).",
        "parameters": {
          "--algo": "sfedavg",
          "--rounds": 40,
          "--local_steps": 10,
          "--client_frac": 0.5,
          "--num_clients": 20,
          "--lr": 0.3,
          "--momentum": 0.9,
          "--batch_size": 32,
          "--r": 96,
          "--seed": 123
        },
        "dataset": "Digits dataset partitioned non-IID via label skew (e.g., Dirichlet alphaâ‰ˆ0.3) so clients predominantly see a subset of classes.",
        "expected_insight": "SFedAvg should mitigate client drift via projected momentum; FedAvg may exhibit slower or oscillatory convergence under non-IID splits.",
        "metrics": [
          "train_loss",
          "test_loss",
          "test_accuracy",
          "communication_bytes"
        ]
      },
      {
        "name": "Robustness to Label Noise",
        "description": "Assess sensitivity to corrupted labels and whether projection in SFedAvg improves robustness.",
        "parameters": {
          "--algo": "sfedavg",
          "--rounds": 40,
          "--local_steps": 8,
          "--client_frac": 0.5,
          "--num_clients": 20,
          "--lr": 0.4,
          "--momentum": 0.9,
          "--batch_size": 32,
          "--r": 80,
          "--seed": 7
        },
        "dataset": "Digits dataset with 20% labels randomly flipped per client; partitions remain balanced by sample count.",
        "expected_insight": "Projected gradients reduce variance from noisy labels, yielding steadier loss curves and potentially better final accuracy than FedAvg.",
        "metrics": [
          "train_loss",
          "test_loss",
          "test_accuracy",
          "communication_bytes"
        ]
      },
      {
        "name": "Scalability and Edge Cases (Many Clients, Small r)",
        "description": "Evaluate scaling to many clients with small client fraction and extreme subspace dimension r to expose failure modes.",
        "parameters": {
          "--algo": "sfedavg",
          "--rounds": 50,
          "--local_steps": 5,
          "--client_frac": 0.1,
          "--num_clients": 100,
          "--lr": 0.6,
          "--momentum": 0.9,
          "--batch_size": 32,
          "--r": 16,
          "--seed": 99
        },
        "dataset": "Digits dataset replicated across 100 clients with IID or mildly skewed distributions; each round selects 10% of clients.",
        "expected_insight": "Communication cost grows sublinearly with clients due to low client fraction; very small r may slow convergence or underfit, revealing the trade-off.",
        "metrics": [
          "train_loss",
          "test_loss",
          "test_accuracy",
          "communication_bytes"
        ]
      }
    ],
    "rationale": "The scenarios probe baseline behavior, effects of one-sided subspace and momentum projection, robustness under heterogeneity and label noise, and scalability with many clients and small r. This coverage aligns with the algorithm title and pseudocode emphasizing Stiefel-sampled subspaces, momentum projection at block start, client subsampling, and aggregation."
  },
  "execution_results": [
    {
      "name": "baseline_fedavg_convergence",
      "description": "Establish baseline convergence behavior of standard Federated Averaging without subspace projection.",
      "parameters": {
        "--algo": "fedavg",
        "--rounds": 30,
        "--local_steps": 5,
        "--client_frac": 0.5,
        "--num_clients": 10,
        "--lr": 0.5,
        "--momentum": 0.9,
        "--batch_size": 32,
        "--r": 128,
        "--seed": 42
      },
      "expected_insight": "Training and test loss should decrease steadily with accuracy increasing; provides a baseline to compare SFedAvg.",
      "status": "success",
      "run_dir": "run_baseline_fedavg_convergence",
      "attempts": 1
    },
    {
      "name": "subspace_sfedavg_convergence_(moderate_r)",
      "description": "Evaluate SFedAvg with one-sided random subspace and momentum projection to test convergence and communication efficiency.",
      "parameters": {
        "--algo": "sfedavg",
        "--rounds": 30,
        "--local_steps": 5,
        "--client_frac": 0.5,
        "--num_clients": 10,
        "--lr": 0.5,
        "--momentum": 0.9,
        "--batch_size": 32,
        "--r": 128,
        "--seed": 42
      },
      "expected_insight": "Convergence comparable to FedAvg with reduced communication overhead due to subspace projection; momentum projection stabilizes updates.",
      "status": "success",
      "run_dir": "run_subspace_sfedavg_convergence_(moderate_r)",
      "attempts": 1
    },
    {
      "name": "non-iid_heterogeneity_stress_test",
      "description": "Test performance under client distribution shift where each client has a skewed label distribution (heterogeneous data).",
      "parameters": {
        "--algo": "sfedavg",
        "--rounds": 40,
        "--local_steps": 10,
        "--client_frac": 0.5,
        "--num_clients": 20,
        "--lr": 0.3,
        "--momentum": 0.9,
        "--batch_size": 32,
        "--r": 96,
        "--seed": 123
      },
      "expected_insight": "SFedAvg should mitigate client drift via projected momentum; FedAvg may exhibit slower or oscillatory convergence under non-IID splits.",
      "status": "success",
      "run_dir": "run_non-iid_heterogeneity_stress_test",
      "attempts": 1
    },
    {
      "name": "robustness_to_label_noise",
      "description": "Assess sensitivity to corrupted labels and whether projection in SFedAvg improves robustness.",
      "parameters": {
        "--algo": "sfedavg",
        "--rounds": 40,
        "--local_steps": 8,
        "--client_frac": 0.5,
        "--num_clients": 20,
        "--lr": 0.4,
        "--momentum": 0.9,
        "--batch_size": 32,
        "--r": 80,
        "--seed": 7
      },
      "expected_insight": "Projected gradients reduce variance from noisy labels, yielding steadier loss curves and potentially better final accuracy than FedAvg.",
      "status": "success",
      "run_dir": "run_robustness_to_label_noise",
      "attempts": 1
    },
    {
      "name": "scalability_and_edge_cases_(many_clients,_small_r)",
      "description": "Evaluate scaling to many clients with small client fraction and extreme subspace dimension r to expose failure modes.",
      "parameters": {
        "--algo": "sfedavg",
        "--rounds": 50,
        "--local_steps": 5,
        "--client_frac": 0.1,
        "--num_clients": 100,
        "--lr": 0.6,
        "--momentum": 0.9,
        "--batch_size": 32,
        "--r": 16,
        "--seed": 99
      },
      "expected_insight": "Communication cost grows sublinearly with clients due to low client fraction; very small r may slow convergence or underfit, revealing the trade-off.",
      "status": "success",
      "run_dir": "run_scalability_and_edge_cases_(many_clients,_small_r)",
      "attempts": 1
    }
  ]
}