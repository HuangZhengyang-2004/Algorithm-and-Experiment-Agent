{
  "scenarios": [
    {
      "name": "Non-IID Label Skew Stress Test",
      "description": "Evaluate how SFedAvg's one-sided projection with momentum mitigates client drift under strong label-distribution heterogeneity compared to FedAvg.",
      "parameters": {
        "--rounds": 50,
        "--local_steps": 5,
        "--client_fraction": 0.2,
        "--stepsize": 0.1,
        "--momentum": 0.9,
        "--batch_size": 32,
        "--num_clients": 50,
        "--subspace_rank": 16
      },
      "dataset": "Per-client class skew: each client has 2 dominant classes comprising ~80% of its local data; remaining ~20% uniformly sampled from other classes (strong non-IID).",
      "expected_insight": "SFedAvg should reduce drift and stabilize convergence versus FedAvg, achieving lower training loss and higher early-round accuracy at the cost of additional communication to send Π_t.",
      "metrics": ["train_loss", "test_accuracy", "comm_floats", "rounds_to_target_accuracy(0.90)", "accuracy_auc_over_rounds"]
    },
    {
      "name": "Label Noise Robustness",
      "description": "Test resilience to corrupted labels by injecting random label noise and assess whether projected momentum in SFedAvg dampens noisy gradients.",
      "parameters": {
        "--rounds": 60,
        "--local_steps": 5,
        "--client_fraction": 0.25,
        "--stepsize": 0.08,
        "--momentum": 0.9,
        "--batch_size": 64,
        "--num_clients": 40,
        "--subspace_rank": 12
      },
      "dataset": "Uniform label noise: randomly flip 20% of labels per client independently; maintain otherwise IID feature distributions.",
      "expected_insight": "SFedAvg should be more robust to label noise, showing smoother loss curves and better final accuracy than FedAvg due to projection filtering of gradient directions.",
      "metrics": ["train_loss", "test_accuracy", "comm_floats", "noise_sensitivity_index(loss_variance)", "early_stopping_rounds(at_accuracy=0.85)"]
    },
    {
      "name": "Scalability: Many Clients and High-Dimensional Features",
      "description": "Assess convergence and communication scaling when increasing the number of clients and feature dimensionality, stressing the Π_t transmission in SFedAvg.",
      "parameters": {
        "--rounds": 80,
        "--local_steps": 1,
        "--client_fraction": 0.1,
        "--stepsize": 0.06,
        "--momentum": 0.9,
        "--batch_size": 32,
        "--num_clients": 200,
        "--subspace_rank": 32
      },
      "dataset": "Large synthetic dataset with ~200-dimensional features; increase per-client sample count to keep local estimates stable; split IID to isolate scaling effects.",
      "expected_insight": "SFedAvg should maintain stable convergence with many clients, while highlighting a trade-off between accuracy gains and higher communication (∝ d·r); FedAvg may show slower convergence.",
      "metrics": ["train_loss", "test_accuracy", "comm_floats", "accuracy_vs_comm_tradeoff_curve", "time_to_target_accuracy(0.90)"]
    },
    {
      "name": "Hyperparameter Stress: Aggressive LR and Deep Local Steps",
      "description": "Probe stability when local optimization is aggressive (large stepsize, many local steps), where momentum projection is expected to regularize updates.",
      "parameters": {
        "--rounds": 50,
        "--local_steps": 10,
        "--client_fraction": 0.2,
        "--stepsize": 0.2,
        "--momentum": 0.95,
        "--batch_size": 32,
        "--num_clients": 50,
        "--subspace_rank": 8
      },
      "dataset": "Baseline IID split to isolate optimizer effects; consistent feature distribution across clients.",
      "expected_insight": "FedAvg may oscillate or diverge under aggressive settings, whereas SFedAvg should exhibit improved stability and smoother convergence due to projected momentum.",
      "metrics": ["train_loss", "test_accuracy", "loss_oscillation_index(std(diff(loss_round_to_round)))", "max_accuracy_drop(window=5)"]
    },
    {
      "name": "Projection Rank Edge Case (r=1)",
      "description": "Examine failure modes when the subspace is extremely low rank, constraining updates to near one dimension.",
      "parameters": {
        "--rounds": 60,
        "--local_steps": 5,
        "--client_fraction": 0.1,
        "--stepsize": 0.1,
        "--momentum": 0.9,
        "--batch_size": 32,
        "--num_clients": 50,
        "--subspace_rank": 1
      },
      "dataset": "Moderate non-IID split (each client concentrates ~60% on 3 classes) to induce drift while avoiding pathological extremes.",
      "expected_insight": "SFedAvg with r=1 may slow convergence and underperform if the subspace is overly restrictive, but should still reduce instability versus FedAvg; highlights the rank–performance trade-off.",
      "metrics": ["train_loss", "test_accuracy", "comm_floats", "early_round_convergence_rate(slope_0_20)", "final_accuracy_gap(SFedAvg_minus_FedAvg)"]
    }
  ],
  "rationale": "The scenarios are designed around the SFedAvg pseudocode's core mechanisms—one-sided projection Π_t and momentum projection (MP)—and compare against FedAvg across key axes: (1) Non-IID heterogeneity stresses client drift where projected momentum should help; (2) Label-noise robustness tests whether projection filters noisy gradient directions; (3) Scalability increases client count and feature dimensionality to expose the communication–accuracy trade-off inherent in transmitting Π_t; (4) Hyperparameter stress uses aggressive local optimization to reveal stability benefits from MP; (5) Edge-case low rank (r=1) probes failure modes and the sensitivity to subspace rank, validating theoretical trade-offs between projection strength and convergence speed."
}
