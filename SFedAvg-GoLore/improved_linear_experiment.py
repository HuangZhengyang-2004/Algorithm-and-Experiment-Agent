"""
Improved Linear Regression Experiment with Hyperparameter Search
æ”¹è¿›ç‰ˆçº¿æ€§å›å½’å®éªŒï¼ŒåŒ…å«è¶…å‚æ•°æœç´¢
"""

import numpy as np
import matplotlib.pyplot as plt
import time
from itertools import product
from sfedavg_implementation import StiefelSampler


class ImprovedExperiment:
    """æ”¹è¿›çš„å®éªŒç±»ï¼ŒåŒ…å«è¶…å‚æ•°æœç´¢"""
    
    def __init__(self):
        # å®éªŒé…ç½®
        self.num_clients = 10
        self.client_fraction = 0.3
        self.d = 30
        self.samples_per_client = 80
        self.num_rounds = 60  # ç¨å¾®å‡å°‘ä»¥åŠ å¿«è¶…å‚æ•°æœç´¢
        self.local_steps = 5
        self.batch_size = 15
        
        # è¶…å‚æ•°æœç´¢ç©ºé—´
        self.lr_candidates = [0.005, 0.01, 0.02, 0.05]
        self.momentum_candidates = [0.0, 0.6, 0.9]
        
        # ç”Ÿæˆæ•°æ®
        self._generate_data()
    
    def _generate_data(self):
        """ç”Ÿæˆè”é‚¦æ•°æ®"""
        np.random.seed(42)
        
        # çœŸå®å‚æ•°
        self.true_theta = np.random.randn(self.d)
        self.true_theta = self.true_theta / np.linalg.norm(self.true_theta) * 2.5
        
        self.client_data = []
        for client_id in range(self.num_clients):
            # ç”Ÿæˆå¼‚è´¨æ•°æ®
            mean_shift = np.random.randn(self.d) * 0.1
            X = np.random.randn(self.samples_per_client, self.d) + mean_shift
            noise = np.random.normal(0, 0.1 * (1 + client_id * 0.05), self.samples_per_client)
            y = X @ self.true_theta + noise
            
            self.client_data.append({
                'X': X, 'y': y, 'client_id': client_id
            })
    
    def compute_global_loss(self, theta):
        """è®¡ç®—å…¨å±€æŸå¤±"""
        total_loss = 0
        total_samples = 0
        
        for client_data in self.client_data:
            X, y = client_data['X'], client_data['y']
            pred = X @ theta
            loss = np.mean((pred - y) ** 2)
            total_loss += loss * len(X)
            total_samples += len(X)
            
        return total_loss / total_samples
    
    def fedavg_method(self, learning_rate, momentum=0.0):
        """æ ‡å‡†FedAvgæ–¹æ³•"""
        theta = np.zeros(self.d)
        server_momentum = np.zeros(self.d) if momentum > 0 else None
        loss_history = []
        
        for round_idx in range(self.num_rounds):
            # é€‰æ‹©å®¢æˆ·ç«¯
            num_selected = max(1, int(self.client_fraction * self.num_clients))
            selected_clients = np.random.choice(self.num_clients, num_selected, replace=False)
            
            # å®¢æˆ·ç«¯æ›´æ–°
            client_updates = []
            for client_idx in selected_clients:
                client_data = self.client_data[client_idx]
                X, y = client_data['X'], client_data['y']
                
                local_theta = theta.copy()
                
                # æœ¬åœ°SGD
                for step in range(self.local_steps):
                    indices = np.random.choice(len(X), min(self.batch_size, len(X)), replace=False)
                    X_batch, y_batch = X[indices], y[indices]
                    
                    pred = X_batch @ local_theta
                    grad = X_batch.T @ (pred - y_batch) / len(X_batch)
                    local_theta -= learning_rate * grad
                
                client_updates.append(local_theta)
            
            # èšåˆ
            new_theta = np.mean(client_updates, axis=0)
            
            # æœåŠ¡å™¨ç«¯åŠ¨é‡ï¼ˆå¦‚æœä½¿ç”¨ï¼‰
            if server_momentum is not None:
                delta = new_theta - theta
                server_momentum = momentum * server_momentum + delta
                theta = theta + server_momentum
            else:
                theta = new_theta
            
            # è®°å½•æŸå¤±
            loss = self.compute_global_loss(theta)
            loss_history.append(loss)
        
        return {
            'theta': theta,
            'loss_history': loss_history,
            'final_loss': loss_history[-1],
            'param_error': np.linalg.norm(theta - self.true_theta),
            'comm_cost_per_round': self.d * 8
        }
    
    def sfedavg_method(self, delta, learning_rate, momentum):
        """SFedAvgæ–¹æ³•"""
        r = max(1, int(delta * self.d))
        theta = np.zeros(self.d)
        server_momentum = np.zeros(self.d)
        loss_history = []
        
        for round_idx in range(self.num_rounds):
            # åˆ·æ–°æŠ•å½±å™¨
            P = StiefelSampler.sample(self.d, r)
            Pi = P @ P.T
            
            # é€‰æ‹©å®¢æˆ·ç«¯
            num_selected = max(1, int(self.client_fraction * self.num_clients))
            selected_clients = np.random.choice(self.num_clients, num_selected, replace=False)
            
            # å®¢æˆ·ç«¯æ›´æ–°
            client_updates = []
            for client_idx in selected_clients:
                client_data = self.client_data[client_idx]
                X, y = client_data['X'], client_data['y']
                
                local_theta = theta.copy()
                local_momentum = np.zeros(self.d)
                
                # æœ¬åœ°SGD with momentum
                for step in range(self.local_steps):
                    indices = np.random.choice(len(X), min(self.batch_size, len(X)), replace=False)
                    X_batch, y_batch = X[indices], y[indices]
                    
                    pred = X_batch @ local_theta
                    grad = X_batch.T @ (pred - y_batch) / len(X_batch)
                    
                    local_momentum = momentum * local_momentum + grad
                    local_theta -= learning_rate * local_momentum
                
                client_updates.append(local_theta)
            
            # æœåŠ¡å™¨èšåˆ
            new_theta = np.mean(client_updates, axis=0)
            delta_theta = new_theta - theta
            
            # åŠ¨é‡æŠ•å½± (MP)
            server_momentum = Pi @ (momentum * server_momentum + delta_theta)
            theta = theta + server_momentum
            
            # è®°å½•æŸå¤±
            loss = self.compute_global_loss(theta)
            loss_history.append(loss)
        
        return {
            'theta': theta,
            'loss_history': loss_history,
            'final_loss': loss_history[-1],
            'param_error': np.linalg.norm(theta - self.true_theta),
            'comm_cost_per_round': r * 8 if delta < 1.0 else self.d * 8,
            'delta': delta
        }
    
    def hyperparameter_search(self, method_name, method_func, **kwargs):
        """è¶…å‚æ•°æœç´¢"""
        print(f"\nğŸ” å¯¹ {method_name} è¿›è¡Œè¶…å‚æ•°æœç´¢...")
        
        best_loss = float('inf')
        best_params = None
        best_result = None
        
        search_count = 0
        total_searches = len(self.lr_candidates) * len(self.momentum_candidates)
        
        for lr, mom in product(self.lr_candidates, self.momentum_candidates):
            search_count += 1
            
            # è®¾ç½®éšæœºç§å­ç¡®ä¿ä¸€è‡´æ€§
            np.random.seed(42)
            
            try:
                if 'delta' in kwargs:
                    result = method_func(kwargs['delta'], lr, mom)
                else:
                    result = method_func(lr, mom)
                
                if result['final_loss'] < best_loss:
                    best_loss = result['final_loss']
                    best_params = {'learning_rate': lr, 'momentum': mom}
                    best_result = result
                    best_result['best_params'] = best_params
                
                print(f"  [{search_count}/{total_searches}] lr={lr:.3f}, mom={mom:.1f} â†’ loss={result['final_loss']:.6f}")
                
            except Exception as e:
                print(f"  [{search_count}/{total_searches}] lr={lr:.3f}, mom={mom:.1f} â†’ ERROR: {e}")
                continue
        
        print(f"  âœ… æœ€ä½³å‚æ•°: lr={best_params['learning_rate']:.3f}, mom={best_params['momentum']:.1f}")
        print(f"  âœ… æœ€ä½³æŸå¤±: {best_loss:.6f}")
        
        return best_result, best_params
    
    def run_experiment_with_hyperparameter_search(self):
        """è¿è¡ŒåŒ…å«è¶…å‚æ•°æœç´¢çš„å®Œæ•´å®éªŒ"""
        print("=" * 80)
        print("SFedAvg vs Baselines: åŒ…å«è¶…å‚æ•°æœç´¢çš„å®éªŒ")
        print("=" * 80)
        
        print(f"\nå®éªŒé…ç½®:")
        print(f"  ç»´åº¦d={self.d}, å®¢æˆ·ç«¯={self.num_clients}, è½®æ•°={self.num_rounds}")
        print(f"  æœ¬åœ°æ­¥æ•°Ï„={self.local_steps}, æ‰¹æ¬¡å¤§å°={self.batch_size}")
        print(f"  å­¦ä¹ ç‡å€™é€‰: {self.lr_candidates}")
        print(f"  åŠ¨é‡å€™é€‰: {self.momentum_candidates}")
        print(f"  çœŸå®å‚æ•°èŒƒæ•°: {np.linalg.norm(self.true_theta):.4f}")
        
        results = {}
        best_params = {}
        
        # 1. FedAvgè¶…å‚æ•°æœç´¢
        print(f"\n{'='*60}")
        print("1. FedAvg è¶…å‚æ•°æœç´¢")
        print(f"{'='*60}")
        
        fedavg_result, fedavg_params = self.hyperparameter_search(
            "FedAvg", self.fedavg_method
        )
        results['FedAvg'] = fedavg_result
        best_params['FedAvg'] = fedavg_params
        
        # 2. FedAvgMè¶…å‚æ•°æœç´¢ (æ’é™¤momentum=0çš„æƒ…å†µ)
        print(f"\n{'='*60}")
        print("2. FedAvgM è¶…å‚æ•°æœç´¢")
        print(f"{'='*60}")
        
        # ä¸´æ—¶ä¿®æ”¹momentumå€™é€‰ï¼Œæ’é™¤0
        original_momentum = self.momentum_candidates
        self.momentum_candidates = [m for m in self.momentum_candidates if m > 0]
        
        fedavgm_result, fedavgm_params = self.hyperparameter_search(
            "FedAvgM", self.fedavg_method
        )
        results['FedAvgM'] = fedavgm_result
        best_params['FedAvgM'] = fedavgm_params
        
        # æ¢å¤åŸå§‹momentumå€™é€‰
        self.momentum_candidates = original_momentum
        
        # 3. SFedAvgä¸åŒÎ´å€¼çš„è¶…å‚æ•°æœç´¢
        for delta in [1.0, 0.5, 0.25]:
            method_name = f'SFedAvg-Î´{delta:.2f}'
            print(f"\n{'='*60}")
            print(f"3.{int(delta*4)}. {method_name} è¶…å‚æ•°æœç´¢")
            print(f"{'='*60}")
            
            sfedavg_result, sfedavg_params = self.hyperparameter_search(
                method_name, self.sfedavg_method, delta=delta
            )
            results[method_name] = sfedavg_result
            best_params[method_name] = sfedavg_params
        
        return results, best_params
    
    def analyze_results_with_hyperparams(self, results, best_params):
        """åˆ†æåŒ…å«è¶…å‚æ•°çš„å®éªŒç»“æœ"""
        print("\n" + "=" * 100)
        print("å®éªŒç»“æœåˆ†æ (åŒ…å«æœ€ä¼˜è¶…å‚æ•°)")
        print("=" * 100)
        
        # 1. æœ€ä¼˜è¶…å‚æ•°è¡¨
        print(f"\n1. å„æ–¹æ³•çš„æœ€ä¼˜è¶…å‚æ•°:")
        print(f"{'æ–¹æ³•':<15} {'å­¦ä¹ ç‡':<10} {'åŠ¨é‡':<8} {'æœ€ç»ˆæŸå¤±':<12} {'å‚æ•°è¯¯å·®':<12}")
        print("-" * 65)
        
        for method, result in results.items():
            params = best_params[method]
            print(f"{method:<15} {params['learning_rate']:<10.3f} {params['momentum']:<8.1f} "
                  f"{result['final_loss']:<12.6f} {result['param_error']:<12.6f}")
        
        # 2. æ€§èƒ½æ‘˜è¦è¡¨
        print(f"\n2. æ€§èƒ½æ‘˜è¦ (ä½¿ç”¨æœ€ä¼˜è¶…å‚æ•°):")
        print(f"{'æ–¹æ³•':<15} {'æœ€ç»ˆæŸå¤±':<12} {'å‚æ•°è¯¯å·®':<12} {'é€šä¿¡/è½®(KB)':<15} {'æ€»é€šä¿¡(KB)':<12}")
        print("-" * 75)
        
        for method, result in results.items():
            comm_per_round = result['comm_cost_per_round'] / 1024
            total_comm = comm_per_round * self.num_rounds
            
            print(f"{method:<15} {result['final_loss']:<12.6f} "
                  f"{result['param_error']:<12.6f} {comm_per_round:<15.2f} {total_comm:<12.1f}")
        
        # 3. æ•ˆç‡åˆ†æ
        print(f"\n3. ç›¸å¯¹äºFedAvgçš„æ•ˆç‡åˆ†æ:")
        fedavg_result = results['FedAvg']
        
        print(f"{'æ–¹æ³•':<15} {'æ€§èƒ½æ¯”ç‡':<12} {'é€šä¿¡èŠ‚çœ':<12} {'æƒè¡¡æ•ˆç‡':<12}")
        print("-" * 55)
        
        for method, result in results.items():
            if method == 'FedAvg':
                continue
                
            perf_ratio = result['final_loss'] / fedavg_result['final_loss']
            comm_saving = 1 - (result['comm_cost_per_round'] / fedavg_result['comm_cost_per_round'])
            tradeoff = comm_saving / max(0.01, abs(perf_ratio - 1)) if perf_ratio != 1.0 else float('inf')
            
            print(f"{method:<15} {perf_ratio:<12.4f} {comm_saving:<12.1%} {tradeoff:<12.1f}")
        
        # 4. è¶…å‚æ•°æ•æ„Ÿæ€§åˆ†æ
        print(f"\n4. è¶…å‚æ•°æ•æ„Ÿæ€§åˆ†æ:")
        lr_usage = {}
        mom_usage = {}
        
        for method, params in best_params.items():
            lr = params['learning_rate']
            mom = params['momentum']
            
            lr_usage[lr] = lr_usage.get(lr, 0) + 1
            mom_usage[mom] = mom_usage.get(mom, 0) + 1
        
        print(f"  æœ€å¸¸ç”¨å­¦ä¹ ç‡:")
        for lr, count in sorted(lr_usage.items(), key=lambda x: x[1], reverse=True):
            print(f"    lr={lr:.3f}: {count} ä¸ªæ–¹æ³•")
        
        print(f"  æœ€å¸¸ç”¨åŠ¨é‡:")
        for mom, count in sorted(mom_usage.items(), key=lambda x: x[1], reverse=True):
            print(f"    momentum={mom:.1f}: {count} ä¸ªæ–¹æ³•")
        
        return results
    
    def plot_results_with_hyperparams(self, results, best_params):
        """ç»˜åˆ¶åŒ…å«è¶…å‚æ•°ä¿¡æ¯çš„ç»“æœå›¾è¡¨"""
        print(f"\nç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
        
        # åˆ›å»ºå›¾è¡¨
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle('SFedAvg vs Baselines: Results with Hyperparameter Search', 
                     fontsize=16, fontweight='bold')
        
        colors = {
            'FedAvg': '#1f77b4',
            'FedAvgM': '#ff7f0e',
            'SFedAvg-Î´1.00': '#2ca02c',
            'SFedAvg-Î´0.50': '#d62728',
            'SFedAvg-Î´0.25': '#9467bd'
        }
        
        rounds = range(1, self.num_rounds + 1)
        
        # å›¾1: æŸå¤±æ”¶æ•› (ä½¿ç”¨æœ€ä¼˜è¶…å‚æ•°)
        ax1.set_title('Loss Convergence (Optimal Hyperparameters)')
        for method, result in results.items():
            params = best_params[method]
            label = f"{method} (lr={params['learning_rate']:.3f}, Î¼={params['momentum']:.1f})"
            ax1.plot(rounds, result['loss_history'], 
                    color=colors.get(method, 'gray'), 
                    linewidth=2, label=label)
        ax1.set_xlabel('Communication Round')
        ax1.set_ylabel('Global Loss (MSE)')
        ax1.set_yscale('log')
        ax1.grid(True, alpha=0.3)
        ax1.legend(fontsize=9)
        
        # å›¾2: é€šä¿¡-æ€§èƒ½æƒè¡¡
        ax2.set_title('Communication-Performance Trade-off')
        
        comm_costs = []
        final_losses = []
        method_labels = []
        
        for method, result in results.items():
            comm_costs.append(result['comm_cost_per_round'] / 1024)  # KB
            final_losses.append(result['final_loss'])
            method_labels.append(method)
        
        colors_list = [colors.get(method, 'gray') for method in method_labels]
        scatter = ax2.scatter(comm_costs, final_losses, c=colors_list, s=120, alpha=0.8)
        
        for i, method in enumerate(method_labels):
            ax2.annotate(method, (comm_costs[i], final_losses[i]),
                        xytext=(5, 5), textcoords='offset points', fontsize=10)
        
        ax2.set_xlabel('Communication per Round (KB)')
        ax2.set_ylabel('Final Loss (MSE)')
        ax2.set_yscale('log')
        ax2.grid(True, alpha=0.3)
        
        # å›¾3: è¶…å‚æ•°åˆ†å¸ƒ
        ax3.set_title('Optimal Hyperparameter Distribution')
        
        lrs = [best_params[method]['learning_rate'] for method in results.keys()]
        moms = [best_params[method]['momentum'] for method in results.keys()]
        method_names = list(results.keys())
        
        colors_hp = [colors.get(method, 'gray') for method in method_names]
        scatter = ax3.scatter(lrs, moms, c=colors_hp, s=150, alpha=0.8)
        
        for i, method in enumerate(method_names):
            ax3.annotate(method, (lrs[i], moms[i]),
                        xytext=(5, 5), textcoords='offset points', fontsize=9)
        
        ax3.set_xlabel('Optimal Learning Rate')
        ax3.set_ylabel('Optimal Momentum')
        ax3.grid(True, alpha=0.3)
        
        # å›¾4: æ€§èƒ½æ”¹è¿›vsé€šä¿¡èŠ‚çœ
        ax4.set_title('Performance Improvement vs Communication Saving')
        
        fedavg_loss = results['FedAvg']['final_loss']
        fedavg_comm = results['FedAvg']['comm_cost_per_round']
        
        improvements = []
        comm_savings = []
        sfedavg_methods = []
        
        for method, result in results.items():
            if 'SFedAvg' in method:
                improvement = (fedavg_loss - result['final_loss']) / fedavg_loss * 100
                comm_saving = (1 - result['comm_cost_per_round'] / fedavg_comm) * 100
                improvements.append(improvement)
                comm_savings.append(comm_saving)
                sfedavg_methods.append(method)
        
        colors_sfed = [colors.get(method, 'gray') for method in sfedavg_methods]
        bars = ax4.scatter(comm_savings, improvements, c=colors_sfed, s=150, alpha=0.8)
        
        for i, method in enumerate(sfedavg_methods):
            ax4.annotate(method, (comm_savings[i], improvements[i]),
                        xytext=(5, 5), textcoords='offset points', fontsize=10)
        
        # æ·»åŠ å‚è€ƒçº¿
        ax4.axhline(y=0, color='black', linestyle='--', alpha=0.5, label='No improvement')
        ax4.axvline(x=0, color='black', linestyle='--', alpha=0.5, label='No communication saving')
        
        ax4.set_xlabel('Communication Saving (%)')
        ax4.set_ylabel('Performance Improvement (%)')
        ax4.grid(True, alpha=0.3)
        ax4.legend()
        
        plt.tight_layout()
        plt.savefig('improved_linear_regression_results.png', dpi=300, bbox_inches='tight')
        print("å›¾è¡¨å·²ä¿å­˜ä¸º 'improved_linear_regression_results.png'")
        
        plt.show()


def main():
    """ä¸»å‡½æ•°"""
    print("SFedAvg vs Baselines - æ”¹è¿›ç‰ˆçº¿æ€§å›å½’å®éªŒ")
    print("åŒ…å«è¶…å‚æ•°æœç´¢çš„å…¬å¹³å¯¹æ¯”")
    
    start_time = time.time()
    
    # åˆ›å»ºå¹¶è¿è¡Œå®éªŒ
    experiment = ImprovedExperiment()
    results, best_params = experiment.run_experiment_with_hyperparameter_search()
    
    # åˆ†æç»“æœ
    experiment.analyze_results_with_hyperparams(results, best_params)
    
    # ç”Ÿæˆå›¾è¡¨
    experiment.plot_results_with_hyperparams(results, best_params)
    
    total_time = time.time() - start_time
    
    print("\n" + "=" * 80)
    print("âœ“ æ”¹è¿›å®éªŒå®Œæˆï¼")
    print(f"âœ“ æ€»è€—æ—¶: {total_time:.1f}ç§’")
    print("âœ“ é€šè¿‡è¶…å‚æ•°æœç´¢ç¡®ä¿äº†å…¬å¹³å¯¹æ¯”")
    print("âœ“ å‘ç°äº†å„ç®—æ³•çš„æœ€ä¼˜é…ç½®")
    print("=" * 80)


if __name__ == "__main__":
    main()