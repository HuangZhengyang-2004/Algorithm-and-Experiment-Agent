"""
å®éªŒæ‰§è¡Œå¼•æ“ (Experiment Execution Engine)
==========================================

æœ¬æ¨¡å—è´Ÿè´£æ‰§è¡Œ AI é©±åŠ¨çš„è‡ªåŠ¨åŒ–å®éªŒæµç¨‹ï¼ŒåŒ…æ‹¬ï¼š
0. ä»£ç ç”Ÿæˆé˜¶æ®µï¼ˆæ ¹æ®ä¼ªä»£ç ç”Ÿæˆåˆå§‹å®éªŒä»£ç ï¼‰
1. è¿­ä»£å®éªŒå¾ªç¯ï¼ˆAI ä¿®æ”¹ä»£ç  â†’ æ‰§è¡Œ â†’ åé¦ˆï¼‰
2. å¯è§†åŒ–ç”Ÿæˆ
3. æ–‡æ¡£æ›´æ–°

æ ¸å¿ƒæµç¨‹ï¼š
  é˜¶æ®µ 0: ä»£ç ç”Ÿæˆä¸éªŒè¯ (generate_code_from_pseudocode)
    - AI æ ¹æ®ä¼ªä»£ç ç”Ÿæˆ experiment.py å’Œ plot.py
    - ç³»ç»ŸéªŒè¯ä»£ç å¯è¿è¡Œæ€§ï¼ˆè¯­æ³•æ£€æŸ¥ + è¯•è¿è¡Œï¼‰
    - è¿­ä»£ä¿®å¤ç›´åˆ°ä»£ç å¯ä»¥æ­£å¸¸è¿è¡Œ
    - æ£€éªŒä»£ç æ˜¯å¦ç¬¦åˆç®—æ³•ä¼ªä»£ç ï¼Œè¿­ä»£ä¿®å¤çŸ¥é“è¿è¡ŒæˆåŠŸ --new
  
  é˜¶æ®µ 1: å®éªŒå¾ªç¯ (perform_experiments)
    - AI æ ¹æ®æƒ³æ³•ç”Ÿæˆ/ä¿®æ”¹ experiment.py
    - ç³»ç»Ÿæ‰§è¡Œå®éªŒå¹¶æ”¶é›†ç»“æœ
    - å°†ç»“æœåé¦ˆç»™ AIï¼Œè¿›è¡Œä¸‹ä¸€è½®è¿­ä»£
  
  é˜¶æ®µ 2: å¯è§†åŒ–ç”Ÿæˆ (run_plotting)
    - AI ä¿®æ”¹ plot.py ç”Ÿæˆå¯¹æ¯”å›¾è¡¨
    - ç³»ç»Ÿæ‰§è¡Œç»˜å›¾è„šæœ¬
  
  é˜¶æ®µ 3: æ–‡æ¡£æ›´æ–°
    - AI æ›´æ–° notes.txtï¼Œè®°å½•å®éªŒå‘ç°å’Œç»“è®º
"""

import json
import os
import os.path as osp
import shutil
import subprocess
import sys
from subprocess import TimeoutExpired

# ============================================================================
# é…ç½®å¸¸é‡
# ============================================================================

MAX_ITERS = 4           # æ¯æ¬¡è¿è¡Œå¤±è´¥åçš„æœ€å¤§é‡è¯•æ¬¡æ•°
MAX_RUNS = 5            # æ€»å…±å…è®¸çš„æœ€å¤§å®éªŒè¿è¡Œæ¬¡æ•°
MAX_STDERR_OUTPUT = 1500  # é”™è¯¯ä¿¡æ¯çš„æœ€å¤§æ˜¾ç¤ºé•¿åº¦ï¼ˆå­—ç¬¦æ•°ï¼‰
MAX_CODE_GEN_ITERS = 10  # ä»£ç ç”Ÿæˆé˜¶æ®µçš„æœ€å¤§è¿­ä»£æ¬¡æ•°

# ============================================================================
# AI æç¤ºè¯æ¨¡æ¿ - é˜¶æ®µ 0: ä»£ç ç”Ÿæˆ
# ============================================================================

code_generation_prompt = """You are an expert algorithm researcher and programmer. Your task is to implement a complete experimental framework based on the following algorithm pseudocode.

# Algorithm Title
{title}

# Algorithm Pseudocode
{pseudocode}

# Experiment Description
{experiment_description}

# Your Task
The files experiment.py and plot.py currently contain only basic skeleton code. You need to REPLACE the TODO sections with complete implementations of the algorithm and experimental framework.

Implement the following:

## 1. experiment.py
This file must contain:
- Complete implementation of the algorithm described in the pseudocode
- Data generation/loading functions
- Training/optimization loop
- Evaluation metrics computation
- Command-line argument parsing with **required** --out_dir parameter
- Result saving to {{out_dir}}/final_info.json

**Critical Requirements for experiment.py:**
```python
import argparse
import json
import os

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--out_dir', type=str, required=True)
    args = parser.parse_args()
    
    # Create output directory
    os.makedirs(args.out_dir, exist_ok=True)
    
    # ... your algorithm implementation ...
    
    # Save results in the required format
    results = {{
        "metric_name": {{
            "means": [value1, value2, ...],  # List of values over iterations/epochs
            "stds": [std1, std2, ...]        # Standard deviations (can be zeros)
        }}
    }}
    
    with open(f"{{args.out_dir}}/final_info.json", "w") as f:
        json.dump(results, f, indent=2)

if __name__ == "__main__":
    main()
```

**CRITICAL OUTPUT STRUCTURE REQUIREMENT:**
- Results MUST be saved DIRECTLY to: {{out_dir}}/final_info.json
- DO NOT create subdirectories like {{out_dir}}/r_0.1/final_info.json
- If testing multiple parameter values, do ONE value per run
- Multiple parameter values will be tested in SEPARATE runs (run_1, run_2, etc.)
- Each run should test a SINGLE configuration and save to {{out_dir}}/final_info.json

**CRITICAL IMPLEMENTATION REQUIREMENTS:**

1. **NO PLACEHOLDER CODE:**
   - NEVER use `np.random.randn()` or `np.random.rand()` for gradients, losses, or training data
   - NEVER leave `TODO` comments or `Placeholder` text in the final code
   - ALL functions must be FULLY implemented with real computations
   - Every value must be computed from actual data and model predictions

2. **REAL GRADIENT COMPUTATION:**
   - Gradients MUST be computed from actual model predictions and data
   - For regression: `gradient = X.T @ (predictions - y) / n_samples`
   - For classification: use appropriate loss function derivatives
   - NEVER use random values as gradients

3. **PROPER MODEL INITIALIZATION:**
   - In federated learning: local models MUST copy global model weights
   - Use: `local_model.weights = global_model.weights.copy()`
   - DO NOT create new random models for each client

4. **LEARNING VERIFICATION:**
   - Your implementation MUST show learning progress
   - Loss/error values SHOULD decrease over training iterations
   - If metrics don't change, your implementation is WRONG
   - For optimization: final values should be significantly better than initial values

5. **DATA USAGE:**
   - Use the ACTUAL training data provided or generated
   - Compute predictions using the model: `predictions = model.predict(X)`
   - Compute errors/losses from predictions and true labels
   - Update model based on these real computations

## 2. plot.py
This file must contain:
- Functions to read results from multiple run directories
- Plotting code for convergence curves and comparisons
- A `labels` dictionary to specify which runs to plot
- MUST accept --out_dir command-line argument for saving plots
- Save plots as PNG files

**Example structure:**
```python
import matplotlib.pyplot as plt
import json
import os
import argparse
import numpy as np

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--out_dir', type=str, required=True, help='Directory to save plots')
    args = parser.parse_args()
    
    # Create output directory
    os.makedirs(args.out_dir, exist_ok=True)
    
    # Dictionary mapping run directories to labels
    labels = {{
        "run_1": "Baseline",
        "run_2": "Variant 1",
        # Will be updated later with more runs
    }}
    
    # Initialize matplotlib with professional style
    plt.style.use('seaborn-v0_8-whitegrid')
    fig, axes = plt.subplots(1, 1, figsize=(10, 6))
    
    # Colors for different runs
    colors = plt.cm.tab10(np.linspace(0, 1, len(labels)))
    
    for i, (run_dir, label) in enumerate(labels.items()):
        if os.path.exists(f"{{run_dir}}/final_info.json"):
            with open(f"{{run_dir}}/final_info.json") as f:
                data = json.load(f)
            
            # Plot each metric
            for metric_name, metric_data in data.items():
                means = metric_data["means"]
                stds = metric_data["stds"]
                iterations = range(len(means))
                
                # Plot mean with standard deviation shading
                axes.plot(iterations, means, label=f"{{label}} - {{metric_name}}", 
                         color=colors[i], linewidth=2)
                axes.fill_between(iterations, 
                                np.array(means) - np.array(stds),
                                np.array(means) + np.array(stds),
                                alpha=0.2, color=colors[i])
    
    # Customize plot
    axes.set_xlabel('Iterations/Epochs', fontsize=12)
    axes.set_ylabel('Metric Value', fontsize=12)
    axes.set_title('Algorithm Performance Comparison', fontsize=14)
    axes.legend()
    axes.grid(True, alpha=0.3)
    
    # Save plots to the specified output directory
    plot_path = os.path.join(args.out_dir, "comparison.png")
    plt.tight_layout()
    plt.savefig(plot_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"Saved plot to: {{plot_path}}")

if __name__ == "__main__":
    main()
```
**CRITICAL REQUIREMENTS for plot.py:**
    - MUST use matplotlib for all visualizations
    - MUST accept --out_dir argument for specifying where to save plots
    - MUST save plots to {{args.out_dir}}/plot_name.png
    - Should create professional-looking plots with:
        - Clear labels and legends
        - Grid for readability
        - Error bars or shaded regions for standard deviations
        - High resolution (dpi=300)
        - Should handle multiple metrics and multiple runs appropriately
**IMPORTANT:**
Always test your plot.py locally before submission
- Run: python plot.py --out_dir=test_plots
- Check that it creates the directory and saves PNG files
- Ensure no import errors or runtime errors

## 3. Key Implementation Guidelines
- Follow the pseudocode logic closely
- Use appropriate libraries (numpy, scipy, sklearn, torch, etc.)
- Include error handling and input validation
- Add comments explaining key algorithmic steps
- Make the code production-ready and well-documented
- Ensure experiment.py can run standalone with: `python experiment.py --out_dir=test_run`

## 4. Testing Requirements
The generated code must:
- Pass Python syntax validation (no syntax errors)
- Run without crashing (handle imports, data generation, etc.)
- Generate the required output file (final_info.json) with correct format
- Complete execution within reasonable time (for initial test)

Please generate BOTH experiment.py and plot.py now. Make sure they are complete, runnable, and follow all requirements above.
"""

# ============================================================================
# AI æç¤ºè¯æ¨¡æ¿ - é˜¶æ®µ 0: ä»£ç é€»è¾‘éªŒè¯
# ============================================================================

logic_validation_prompt = """Now I need you to verify that the current implementation in experiment.py correctly implements the algorithm pseudocode.

# Algorithm Title
{title}

# Algorithm Pseudocode
{pseudocode}

# Current experiment.py Code
{current_code}

Please carefully compare the implementation with the pseudocode and check for:

1. **Algorithm Fidelity:**
   - Does the code correctly implement all steps from the pseudocode?
   - Are there any missing algorithmic components?
   - Are there any deviations from the pseudocode logic?

2. **Key Components:**
   - Are all variables, functions, and procedures from the pseudocode properly implemented?
   - Are the data structures and control flows consistent with the pseudocode?
   - Are the mathematical formulas and computations correctly translated?

3. **Critical Checks:**
   - If the pseudocode mentions specific conditions, are they properly handled?
   - If there are loops or iterations, do they match the pseudocode's structure?
   - Are the termination conditions and convergence criteria correctly implemented?

4. **Implementation Quality:**
   - Is the code using appropriate data types and operations?
   - Are there any placeholder or dummy implementations that should be real computations?
   - Does the implementation show meaningful learning progress (not constant values)?

Please respond with:
- "LOGIC_VALIDATION_PASSED" if the code correctly implements the pseudocode
- Otherwise, explain what needs to be fixed and make the necessary changes

If you find issues, please fix them and regenerate the experiment.py file.
"""

# ============================================================================
# AI æç¤ºè¯æ¨¡æ¿ - é˜¶æ®µ 1: å®éªŒè¿­ä»£
# ============================================================================

coder_prompt = """Your goal is to implement the following idea: {title}.
The proposed experiment is as follows: {idea}.
You are given a total of up to {max_runs} runs to complete the necessary experiments. You do not need to use all {max_runs}.

First, plan the list of experiments you would like to run. For example, if you are sweeping over a specific hyperparameter, plan each value you would like to test for each run.

Note that we already provide the vanilla baseline results, so you do not need to re-run it.

For reference, the baseline results are as follows:

{baseline_results}

After you complete each change, we will run the command `python experiment.py --out_dir=run_i' where i is the run number and evaluate the results.
YOUR PROPOSED CHANGE MUST USE THIS COMMAND FORMAT, DO NOT ADD ADDITIONAL COMMAND LINE ARGS.
You can then implement the next thing on your list."""


# ============================================================================
# æ–°å¢ï¼šAI è‡ªä¸»åœºæ™¯è®¾è®¡é˜¶æ®µï¼ˆé˜¶æ®µ 0.5ï¼‰
# ============================================================================

def design_experiment_scenarios(idea, folder_name, coder, baseline_results=None, algorithm_tex_path=None):
    """
    AI è‡ªä¸»è®¾è®¡å®éªŒåœºæ™¯ - æ–°å¢é˜¶æ®µ 0.5
    åœ¨ä»£ç ç”Ÿæˆä¹‹åã€å®éªŒå¾ªç¯ä¹‹å‰æ‰§è¡Œ

    æœ¬å‡½æ•°å°†åŸºäº algorithm.texï¼ˆä¼˜å…ˆï¼‰è¯»å–ä¼ªä»£ç ï¼›è‹¥æä¾›äº† idea["Experiment"]
    åˆ™å¯ä½œä¸ºå‚è€ƒä½†ä¸æ˜¯ä¸»è¦æ¥æºã€‚
    """
    print("\n" + "="*80)
    print("ğŸ¯ é˜¶æ®µ 0.5: AI è‡ªä¸»åœºæ™¯è®¾è®¡")
    print("="*80 + "\n")
    
    # ä» algorithm.tex è¯»å–ä¼ªä»£ç ï¼ˆä¸ä½¿ç”¨ idea['Pseudocode']ï¼‰
    pseudocode_text = read_pseudocode_from_tex(folder_name=folder_name, tex_path=algorithm_tex_path) or ""
    
    # å‡†å¤‡ç®—æ³•ä¿¡æ¯
    algorithm_info = {
        "title": idea.get("Title", "Unknown Algorithm"),
        "description": idea.get("Experiment", ""),
        "pseudocode": pseudocode_text,
        "key_parameters": extract_available_parameters(folder_name)
    }
    
    # è®© AI è®¾è®¡å®éªŒåœºæ™¯
    scenario_prompt = generate_scenario_design_prompt(algorithm_info, baseline_results)
    print("ğŸ¤– AI æ­£åœ¨è®¾è®¡å®éªŒåœºæ™¯...")
    ai_response = coder.run(scenario_prompt)
    print(ai_response)
    
    # è§£æ AI è®¾è®¡çš„åœºæ™¯
    scenario_design = parse_ai_scenario_design(ai_response)
    
    if not scenario_design or "scenarios" not in scenario_design:
        print("âŒ AI æœªèƒ½æ­£ç¡®è®¾è®¡å®éªŒåœºæ™¯ï¼Œä½¿ç”¨é»˜è®¤åœºæ™¯")
        scenario_design = get_default_scenarios()
    
    scenarios = scenario_design["scenarios"]
    
    # æ˜¾ç¤ºè®¾è®¡çš„åœºæ™¯
    print(f"\nğŸ“‹ AI è®¾è®¡äº† {len(scenarios)} ä¸ªå®éªŒåœºæ™¯:")
    for i, scenario in enumerate(scenarios, 1):
        print(f"   {i}. {scenario.get('name', f'Scenario_{i}')}")
        print(f"      æè¿°: {scenario.get('description', 'No description')}")
        if 'parameters' in scenario:
            params_str = ' '.join([f"{k}={v}" for k, v in scenario['parameters'].items()])
            print(f"      å‚æ•°: {params_str}")
        if 'expected_insight' in scenario:
            print(f"      é¢„æœŸå‘ç°: {scenario['expected_insight']}")
        print()
    
    # ä¿å­˜åœºæ™¯è®¾è®¡åˆ°æ–‡ä»¶
    scenarios_file = osp.join(folder_name, "ai_designed_scenarios.json")
    with open(scenarios_file, 'w', encoding='utf-8') as f:
        json.dump(scenario_design, f, indent=2, ensure_ascii=False)
    
    print(f"ğŸ’¾ åœºæ™¯è®¾è®¡å·²ä¿å­˜åˆ°: {scenarios_file}")
    
    return scenario_design


def extract_available_parameters(folder_name):
    """
    ä» experiment.py ä¸­æå–å¯ç”¨çš„å‘½ä»¤è¡Œå‚æ•°
    """
    exp_file = osp.join(folder_name, "experiment.py")
    
    if not osp.exists(exp_file):
        return ["--learning_rate", "--num_iterations", "--dataset_size"]  # é»˜è®¤å‚æ•°
    
    try:
        with open(exp_file, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # æå– argparse å‚æ•°
        import re
        param_pattern = r'parser\.add_argument\([^)]*?--([a-zA-Z_]+)[^)]*?\)'
        params = re.findall(param_pattern, content)
        
        # æ’é™¤ out_dir å’Œ scenarioï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        excluded_params = ['out_dir', 'scenario']
        params = [f"--{p}" for p in params if p not in excluded_params]
        
        return params if params else ["--learning_rate", "--num_iterations", "--dataset_size"]
        
    except Exception as e:
        print(f"âš ï¸ æå–å‚æ•°æ—¶å‡ºé”™: {e}")
        return ["--learning_rate", "--num_iterations", "--dataset_size"]


# ...existing code...
def generate_scenario_design_prompt(algorithm_info, existing_results=None):
    """
    Generate an English prompt that instructs the AI to design experimental scenarios.
    Primary sources: algorithm title and pseudocode. The optional 'description' field
    (idea["Experiment"]) MAY be used as a reference for dataset preferences, constraints,
    or evaluation priorities, but DO NOT use it as the primary source of scenario design.

    The prompt asks for 3-5 scenarios covering heterogeneity, robustness, scalability,
    and hyperparameter sensitivity, and requests output in a strict JSON format.
    """
    title = algorithm_info.get('title', 'Unknown Algorithm')
    pseudocode = algorithm_info.get('pseudocode', '').strip()
    description = algorithm_info.get('description', '').strip()
    key_params = algorithm_info.get('key_parameters', [])
    params_str = ', '.join(key_params) if key_params else 'no specific CLI parameters detected'

    prompt = f"""
You are an expert algorithm researcher. DESIGN experimental scenarios PRIMARILY based on the
Algorithm TITLE and its PSEUDOCODE below. You MAY consult the optional "Experiment description"
for helpful context (e.g., suggested datasets, constraints, or evaluation preferences), but
DO NOT rely on it as the main source of scenario design. The scenarios must be justified by
the TITLE and PSEUDOCODE.

Algorithm Title:
{title}

Algorithm Pseudocode:
{pseudocode if pseudocode else '<NO PSEUDOCODE PROVIDED>'}

Optional Experiment description (use only as reference, not primary source):
{description if description else '<NO EXPERIMENT DESCRIPTION PROVIDED>'}

Available command-line parameters (if any):
{params_str}

Your task:
Produce 3 to 5 well-motivated experimental scenarios that thoroughly evaluate the algorithm.
Each scenario must include:
- A concise scenario name
- A one-sentence objective describing what aspect is tested
- Specific command-line parameter settings (use only the available parameters above; if none, propose sensible parameter names and values)
- A short description of the dataset or data modifications to use (e.g., heterogeneity across clients, added label noise, different data sizes)
- The expected outcome or insight
- Any special evaluation metrics or plots that should be produced

Ensure the set of scenarios collectively covers (but is not limited to):
- Heterogeneity: performance when data distributions differ across splits/clients
- Robustness: sensitivity to noise, outliers, or corrupted labels
- Scalability: behavior with increasing dataset size or model capacity
- Hyperparameter sensitivity: learning rate, number of iterations, regularization, etc.
- Edge cases: extreme settings that may reveal failure modes

Output format (MUST be valid JSON). Provide only JSON in your response (no extra explanation):

{{
  "scenarios": [
    {{
      "name": "scenario_name",
      "description": "brief description of what this scenario tests",
      "parameters": {{
        "--learning_rate": 0.01,
        "--num_iterations": 100,
        "--dataset_size": 1000
      }},
      "dataset": "brief description of dataset / data modifications",
      "expected_insight": "what you expect to observe",
      "metrics": ["metric1", "metric2"]
    }}
  ],
  "rationale": "Short explanation why these scenarios were chosen and how they complement each other"
}}

If existing_results are provided, you may incorporate them to suggest follow-up or targeted scenarios, but primary scenarios must still be justified from the TITLE and PSEUDOCODE.
"""
    if existing_results:
        prompt += "\n# Existing results (for reference):\n" + json.dumps(existing_results, indent=2) + "\n"

    return prompt
# ...existing code...

def parse_ai_scenario_design(ai_response):
    """
    è§£æ AI è¿”å›çš„åœºæ™¯è®¾è®¡
    """
    try:
    # å°è¯•ä» AI å“åº”ä¸­æå– JSON
        import re
        json_match = re.search(r'{.*}', ai_response, re.DOTALL)
        if json_match:
            json_str = json_match.group()
            scenario_data = json.loads(json_str)
            return scenario_data
        else:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ° JSONï¼Œå°è¯•è§£ææ–‡æœ¬æ ¼å¼
            return extract_scenarios_from_text(ai_response)
    except (json.JSONDecodeError, AttributeError) as e:
        print(f"âŒ Failed to parse AI scenario design as JSON: {e}")
        return extract_scenarios_from_text(ai_response)

def extract_scenarios_from_text(text):
    """
    ä»æ–‡æœ¬ä¸­æå–åœºæ™¯ä¿¡æ¯ï¼ˆå¤‡é€‰æ–¹æ¡ˆï¼‰
    """
    scenarios = []
    lines = text.split('\n')
    current_scenario = None

    for line in lines:
        line = line.strip()
        
        if line.startswith('Scenario:') or line.startswith('Scenario Name:'):
            if current_scenario:
                scenarios.append(current_scenario)
            current_scenario = {
                'name': line.split(':', 1)[1].strip(), 
                'parameters': {},
                'description': '',
                'expected_insight': ''
            }
        
        elif line.startswith('Description:') and current_scenario:
            current_scenario['description'] = line.split(':', 1)[1].strip()
        
        elif line.startswith('Parameters:') and current_scenario:
            param_part = line.split(':', 1)[1].strip()
            params = param_part.split()
            for param in params:
                if param.startswith('--'):
                    if '=' in param:
                        key, value = param.split('=', 1)
                        current_scenario['parameters'][key] = try_parse_value(value)
        
        elif line.startswith('Expected:') and current_scenario:
            current_scenario['expected_insight'] = line.split(':', 1)[1].strip()

    if current_scenario:
        scenarios.append(current_scenario)

    return {
        "scenarios": scenarios, 
        "rationale": "Extracted from text response - please check the formatting"
    }

def try_parse_value(value_str):
    """
    å°è¯•è§£æå‚æ•°å€¼
    """
    try:
        return float(value_str)
    except ValueError:
        try:
            return int(value_str)
        except ValueError:
            return value_str

def get_default_scenarios():
    """
    å¤‡ç”¨é»˜è®¤åœºæ™¯
    """
    return {
    "scenarios": [
    {
    "name": "baseline",
    "description": "Standard baseline configuration",
    "parameters": {
    "--learning_rate": 0.01,
    "--num_iterations": 100,
    "--dataset_size": 1000
    },
    "expected_insight": "Establish baseline performance for comparison"
    },
    {
    "name": "high_learning_rate",
    "description": "Test with higher learning rate",
    "parameters": {
    "--learning_rate": 0.1,
    "--num_iterations": 100,
    "--dataset_size": 1000
    },
    "expected_insight": "Observe convergence speed and stability with high learning rate"
    },
    {
    "name": "noisy_data",
    "description": "Test robustness to noisy data",
    "parameters": {
    "--learning_rate": 0.01,
    "--num_iterations": 100,
    "--noise_level": 0.5
    },
    "expected_insight": "Evaluate algorithm robustness under noisy conditions"
    }
    ],
    "rationale": "Default scenarios for basic algorithm testing covering learning rate sensitivity and robustness"
    }

# ============================================================================
# è¾…åŠ©å‡½æ•°ï¼šéªŒè¯ Python ä»£ç è¯­æ³•
# ============================================================================

def validate_python_syntax(file_path):
    """
    éªŒè¯ Python æ–‡ä»¶çš„è¯­æ³•æ˜¯å¦æ­£ç¡®
    
    å‚æ•°ï¼š
      file_path: Python æ–‡ä»¶è·¯å¾„
    
    è¿”å›ï¼š
      (is_valid, error_message): æ˜¯å¦æœ‰æ•ˆå’Œé”™è¯¯ä¿¡æ¯
    """
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            code = f.read()
        
        # å°è¯•ç¼–è¯‘ä»£ç 
        compile(code, file_path, 'exec')
        return True, ""
    except SyntaxError as e:
        error_msg = f"Syntax error in {file_path}:\n"
        error_msg += f"  Line {e.lineno}: {e.msg}\n"
        error_msg += f"  {e.text}"
        return False, error_msg
    except Exception as e:
        return False, f"Error validating {file_path}: {str(e)}"


def read_pseudocode_from_tex(folder_name=None, tex_path=None):
    """
    ä»æŒ‡å®šè·¯å¾„æˆ– folder_name/algorithm.tex è¯»å–ä¼ªä»£ç å†…å®¹ï¼Œæ‰¾ä¸åˆ°åˆ™è¿”å› None

    ä¼˜å…ˆä½¿ç”¨ tex_pathï¼ˆå¯ä»¥æ˜¯ç›¸å¯¹æˆ–ç»å¯¹è·¯å¾„ï¼‰ã€‚å¦‚æœæœªæä¾› tex_pathï¼Œåˆ™åœ¨
    folder_name/algorithm.tex ä¸­æŸ¥æ‰¾ã€‚
    """
    # å¦‚æœç›´æ¥ç»™å‡º tex_pathï¼Œåˆ™ä¼˜å…ˆä½¿ç”¨
    if tex_path:
        candidate = tex_path if osp.isabs(tex_path) else osp.abspath(tex_path)
        if osp.exists(candidate):
            try:
                with open(candidate, 'r', encoding='utf-8') as f:
                    content = f.read()
                return content if content.strip() else None
            except Exception as e:
                print(f"âš ï¸ è¯»å– {candidate} æ—¶å‡ºé”™: {e}")
                return None
        else:
            return None

    # å¦åˆ™ä» folder_name ä¸­æŸ¥æ‰¾ algorithm.tex
    if folder_name:
        tex_path_local = osp.join(folder_name, "algorithm.tex")
        if osp.exists(tex_path_local):
            try:
                with open(tex_path_local, 'r', encoding='utf-8') as f:
                    content = f.read()
                return content if content.strip() else None
            except Exception as e:
                print(f"âš ï¸ è¯»å– {tex_path_local} æ—¶å‡ºé”™: {e}")
                return None

    return None

# ============================================================================
# è¾…åŠ©å‡½æ•°ï¼šæµ‹è¯•è¿è¡Œ experiment.py
# ============================================================================

def test_run_experiment(folder_name, timeout=300):
    """
    æµ‹è¯•è¿è¡Œ experiment.pyï¼ŒéªŒè¯å…¶æ˜¯å¦èƒ½æ­£å¸¸æ‰§è¡Œ
    
    å·¥ä½œæµç¨‹ï¼š
      1. æ‰§è¡Œå‘½ä»¤ï¼špython experiment.py --out_dir=test_run
      2. æ£€æŸ¥æ˜¯å¦ç”Ÿæˆ test_run/final_info.json
      3. éªŒè¯ JSON æ ¼å¼æ˜¯å¦æ­£ç¡®
      4. æ¸…ç†æµ‹è¯•ç›®å½•
    
    å‚æ•°ï¼š
      folder_name: å®éªŒæ–‡ä»¶å¤¹è·¯å¾„
      timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤ 5 åˆ†é’Ÿ
    
    è¿”å›ï¼š
      (success, error_message): æ˜¯å¦æˆåŠŸå’Œé”™è¯¯ä¿¡æ¯
    """
    cwd = osp.abspath(folder_name)
    test_dir = osp.join(cwd, "test_run")
    
    # æ¸…ç†ä¹‹å‰çš„æµ‹è¯•ç›®å½•
    if osp.exists(test_dir):
        shutil.rmtree(test_dir)
    
    # æ‰§è¡Œæµ‹è¯•å‘½ä»¤
    command = ["python", "experiment.py", "--out_dir=test_run"]
    
    try:
        result = subprocess.run(
            command, 
            cwd=cwd, 
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE, 
            text=True, 
            timeout=timeout
        )
        
        # æ£€æŸ¥è¿”å›ç 
        if result.returncode != 0:
            error_msg = f"Test run failed with return code {result.returncode}\n"
            error_msg += f"STDERR:\n{result.stderr}"
            return False, error_msg
        
        # æ£€æŸ¥è¾“å‡ºæ–‡ä»¶æ˜¯å¦å­˜åœ¨
        result_file = osp.join(test_dir, "final_info.json")
        if not osp.exists(result_file):
            error_msg = "Test run succeeded but did not generate final_info.json\n"
            error_msg += f"Expected file: {result_file}\n"
            if osp.exists(test_dir):
                contents = os.listdir(test_dir)
                error_msg += f"test_run/ contents: {contents}\n"
                
                # æ£€æŸ¥æ˜¯å¦é”™è¯¯åœ°åˆ›å»ºäº†å­ç›®å½•
                subdirs = [d for d in contents if osp.isdir(osp.join(test_dir, d))]
                if subdirs:
                    error_msg += "\nâš ï¸ CRITICAL ERROR: Results were saved to subdirectories!\n"
                    error_msg += f"Found subdirectories: {subdirs}\n"
                    error_msg += "\nThe experiment.py file MUST save results DIRECTLY to:\n"
                    error_msg += f"  {{out_dir}}/final_info.json\n"
                    error_msg += "\nDO NOT create subdirectories for different parameter values.\n"
                    error_msg += "If you need to test multiple parameters, they should be handled\n"
                    error_msg += "in SEPARATE runs (run_1, run_2, etc.), not in subdirectories.\n"
                    error_msg += "\nPlease fix the code to save results directly to the output directory."
            else:
                error_msg += "test_run/ directory was not created"
            return False, error_msg
        
        # éªŒè¯ JSON æ ¼å¼
        try:
            with open(result_file, 'r') as f:
                results = json.load(f)
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ­£ç¡®çš„æ ¼å¼
            if not isinstance(results, dict):
                return False, "final_info.json must be a dictionary"
            
            # æ£€æŸ¥æ˜¯å¦è‡³å°‘æœ‰ä¸€ä¸ªæŒ‡æ ‡
            if len(results) == 0:
                return False, "final_info.json is empty"
            
            # æ£€æŸ¥æ¯ä¸ªæŒ‡æ ‡æ˜¯å¦æœ‰ means å’Œ stds
            for metric_name, metric_data in results.items():
                if not isinstance(metric_data, dict):
                    return False, f"Metric '{metric_name}' must be a dictionary"
                if "means" not in metric_data:
                    return False, f"Metric '{metric_name}' missing 'means' field"
                if "stds" not in metric_data:
                    return False, f"Metric '{metric_name}' missing 'stds' field"
                
                # æ£€æŸ¥ means æ˜¯å¦æ˜¯åˆ—è¡¨ä¸”éç©º
                means = metric_data["means"]
                if not isinstance(means, list) or len(means) == 0:
                    return False, f"Metric '{metric_name}': 'means' must be a non-empty list"
            
            # ================================================================
            # æ–°å¢éªŒè¯ï¼šæ£€æŸ¥ç»“æœçš„åˆç†æ€§
            # ================================================================
            validation_errors = []
            
            # éªŒè¯ 1ï¼šæ£€æŸ¥æ˜¯å¦æ‰€æœ‰å€¼éƒ½ç›¸åŒï¼ˆå¯èƒ½æ˜¯å¸¸æ•°ï¼Œè¡¨ç¤ºæ²¡æœ‰å­¦ä¹ ï¼‰
            for metric_name, metric_data in results.items():
                means = metric_data["means"]
                if len(means) > 5:
                    # æ£€æŸ¥å‰åå·®å¼‚
                    initial_values = means[:min(5, len(means)//4)]
                    final_values = means[-min(5, len(means)//4):]
                    
                    initial_mean = sum(initial_values) / len(initial_values)
                    final_mean = sum(final_values) / len(final_values)
                    
                    # å¦‚æœåˆå§‹å€¼å’Œæœ€ç»ˆå€¼å‡ ä¹ç›¸åŒï¼ˆå˜åŒ–å°äº1%ï¼‰ï¼Œå¯èƒ½æœ‰é—®é¢˜
                    if abs(initial_mean) > 1e-6:  # é¿å…é™¤é›¶
                        change_ratio = abs(final_mean - initial_mean) / abs(initial_mean)
                        if change_ratio < 0.01:  # å˜åŒ–å°äº1%
                            validation_errors.append(
                                f"âš ï¸ Metric '{metric_name}': Values barely changed "
                                f"(initial: {initial_mean:.4f}, final: {final_mean:.4f}). "
                                f"This may indicate the algorithm is not learning properly."
                            )
            
            # éªŒè¯ 2ï¼šæ‰«æä»£ç ä¸­çš„å¸¸è§é—®é¢˜æ¨¡å¼
            exp_file = osp.join(cwd, "experiment.py")
            if osp.exists(exp_file):
                with open(exp_file, 'r') as f:
                    code_content = f.read()
                
                # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨äº† placeholder æˆ–éšæœºå€¼
                problematic_patterns = [
                    ('np.random.randn', 'using random values for gradients/data'),
                    ('np.random.rand', 'using random values for gradients/data'),
                    ('TODO', 'unfinished implementation (TODO comments)'),
                    ('Placeholder', 'placeholder code that needs implementation'),
                    ('pass  # implement', 'empty implementation'),
                ]
                
                for pattern, description in problematic_patterns:
                    if pattern in code_content:
                        # æ£€æŸ¥æ˜¯å¦åœ¨æ³¨é‡Šä¸­ï¼ˆå…è®¸åœ¨æ³¨é‡Šä¸­å‡ºç°ï¼‰
                        lines_with_pattern = [line for line in code_content.split('\n') 
                                             if pattern in line and not line.strip().startswith('#')]
                        if lines_with_pattern:
                            validation_errors.append(
                                f"âš ï¸ Found '{pattern}' in code ({description}). "
                                f"Example: {lines_with_pattern[0][:80]}..."
                            )
            
            # å¦‚æœæœ‰éªŒè¯é”™è¯¯ï¼Œè¿”å›è­¦å‘Š
            if validation_errors:
                error_msg = "Test run completed but found potential issues:\n\n"
                error_msg += "\n".join(validation_errors)
                error_msg += "\n\n"
                error_msg += "Common causes:\n"
                error_msg += "1. Using random values instead of computing from real data\n"
                error_msg += "2. Not properly implementing the training loop\n"
                error_msg += "3. Incorrect gradient computation or model updates\n"
                error_msg += "4. Model not learning from data\n\n"
                error_msg += "Please review and fix these issues. The code should:\n"
                error_msg += "- Compute gradients from actual model predictions and data\n"
                error_msg += "- Update model parameters based on gradients\n"
                error_msg += "- Show learning progress (metrics should change over iterations)\n"
                return False, error_msg
            
            # æµ‹è¯•æˆåŠŸ
            return True, ""
            
        except json.JSONDecodeError as e:
            return False, f"Invalid JSON in final_info.json: {str(e)}"
        except Exception as e:
            return False, f"Error reading final_info.json: {str(e)}"
            
    except TimeoutExpired:
        return False, f"Test run timed out after {timeout} seconds"
    except Exception as e:
        return False, f"Error during test run: {str(e)}"
    finally:
        # æ¸…ç†æµ‹è¯•ç›®å½•
        if osp.exists(test_dir):
            shutil.rmtree(test_dir)


# ============================================================================
# é˜¶æ®µ 0: ä»£ç ç”Ÿæˆä¸éªŒè¯
# ============================================================================

def generate_code_from_pseudocode(idea, folder_name, coder, algorithm_tex_path=None):
    """
    æ ¹æ® algorithm.tex ä¸­çš„ä¼ªä»£ç ç”Ÿæˆåˆå§‹å®éªŒä»£ç å¹¶éªŒè¯å…¶å¯è¿è¡Œæ€§

    æ³¨æ„ï¼šæœ¬å‡½æ•°ä¸ä½¿ç”¨ idea["Pseudocode"]ï¼Œè€Œæ˜¯ä½¿ç”¨ä¼ å…¥çš„ algorithm_tex_path
    æˆ–è€…åœ¨ folder_name/algorithm.tex ä¸­æŸ¥æ‰¾ã€‚
    """
    
    print("\n" + "="*80)
    print("ğŸ”§ é˜¶æ®µ 0: ä»£ç ç”Ÿæˆä¸éªŒè¯ï¼ˆä½¿ç”¨ algorithm.tex ä½œä¸ºä¼ªä»£ç ï¼‰")
    print("="*80 + "\n")
    
    # ä»æŒ‡å®šè·¯å¾„æˆ– folder ä¸­è¯»å–ä¼ªä»£ç 
    pseudocode = read_pseudocode_from_tex(folder_name=folder_name, tex_path=algorithm_tex_path)
    if not pseudocode:
        print("âš ï¸ è­¦å‘Š: æœªæ‰¾åˆ° algorithm.texï¼ˆæ—¢æœªæä¾› --algorithm-texï¼Œä¹Ÿæœªåœ¨æ–‡ä»¶å¤¹ä¸­æ‰¾åˆ°ï¼‰ï¼Œè·³è¿‡ä»£ç ç”Ÿæˆé˜¶æ®µ")
        return True
    
    title = idea.get("Title", "Unknown")
    experiment_desc = idea.get("Experiment", "")
    
    # ç”Ÿæˆåˆå§‹æç¤ºè¯
    initial_prompt = code_generation_prompt.format(
        title=title,
        pseudocode=pseudocode,
        experiment_description=experiment_desc
    )
    
    print("ğŸ¤– AI æ­£åœ¨æ ¹æ® algorithm.tex ä¸­çš„ä¼ªä»£ç ç”Ÿæˆåˆå§‹ä»£ç ...")
    print(f"   ä¼ªä»£ç é•¿åº¦: {len(pseudocode)} å­—ç¬¦")
    print()
    
    # è¿­ä»£ç”Ÿæˆå’ŒéªŒè¯
    for iteration in range(MAX_CODE_GEN_ITERS):
        print(f"\n--- ä»£ç ç”Ÿæˆè¿­ä»£ {iteration + 1}/{MAX_CODE_GEN_ITERS} ---\n")
        
        # ====================================================================
        # Step 0.1: AI ç”Ÿæˆ/ä¿®æ”¹ä»£ç 
        # ====================================================================
        if iteration == 0:
            # é¦–æ¬¡ç”Ÿæˆ - ç›´æ¥è¦æ±‚ AI ç”Ÿæˆå®Œæ•´æ–‡ä»¶
            print("ğŸ¤– AI æ­£åœ¨ç”Ÿæˆ experiment.py å’Œ plot.py...")
            print("   æç¤ºï¼šç¬¬ä¸€æ¬¡ç”Ÿæˆï¼ŒAI å°†åˆ›å»ºå®Œæ•´çš„ä»£ç æ–‡ä»¶")
            coder_out = coder.run(initial_prompt)
        else:
            # ä¿®å¤ä»£ç  - ä½¿ç”¨ diff æ¨¡å¼è¿›è¡Œå¢é‡ä¿®æ”¹
            print("ğŸ¤– AI æ­£åœ¨æ ¹æ®é”™è¯¯åé¦ˆä¿®å¤ä»£ç ...")
            coder_out = coder.run(next_prompt)
        
        print(coder_out)
        
        # ====================================================================
        # Step 0.2: éªŒè¯ experiment.py è¯­æ³•
        # ====================================================================
        print("\nğŸ“ éªŒè¯ experiment.py è¯­æ³•...")
        exp_file = osp.join(folder_name, "experiment.py")
        
        if not osp.exists(exp_file):
            print("âŒ experiment.py æœªç”Ÿæˆ")
            next_prompt = """experiment.py file was not created. Please generate the complete experiment.py file now.

Remember the critical requirements:
1. Must accept --out_dir command-line argument
2. Must save results to {out_dir}/final_info.json
3. JSON format: {"metric": {"means": [...], "stds": [...]}}
"""
            continue
        
        syntax_valid, syntax_error = validate_python_syntax(exp_file)
        
        if not syntax_valid:
            print(f"âŒ è¯­æ³•é”™è¯¯:\n{syntax_error}")
            next_prompt = f"""The experiment.py file has syntax errors:

{syntax_error}

Please fix these syntax errors and regenerate the file.
"""
            continue
        
        print("âœ… experiment.py è¯­æ³•æ­£ç¡®")
        
        # ====================================================================
        # Step 0.3: éªŒè¯ plot.py è¯­æ³•ï¼ˆå¯é€‰ï¼‰
        # ====================================================================
        print("\nğŸ“ éªŒè¯ plot.py è¯­æ³•...")
        plot_file = osp.join(folder_name, "plot.py")
        
        if osp.exists(plot_file):
            syntax_valid, syntax_error = validate_python_syntax(plot_file)
            if not syntax_valid:
                print(f"âš ï¸ plot.py æœ‰è¯­æ³•é”™è¯¯:\n{syntax_error}")
                print("   (å°†åœ¨åç»­é˜¶æ®µä¿®å¤)")
            else:
                print("âœ… plot.py è¯­æ³•æ­£ç¡®")
        else:
            print("âš ï¸ plot.py æœªç”Ÿæˆï¼ˆå°†åœ¨åç»­é˜¶æ®µç”Ÿæˆï¼‰")
        
        # ====================================================================
        # Step 0.4: æµ‹è¯•è¿è¡Œ experiment.py
        # ====================================================================
        print("\nâš™ï¸ æµ‹è¯•è¿è¡Œ experiment.py...")
        print("   æ‰§è¡Œ: python experiment.py --out_dir=test_run")
        
        test_success, test_error = test_run_experiment(folder_name, timeout=300)
        
        if not test_success:
            print(f"âŒ æµ‹è¯•è¿è¡Œå¤±è´¥:\n{test_error}")
            
            # æˆªæ–­è¿‡é•¿çš„é”™è¯¯ä¿¡æ¯
            if len(test_error) > MAX_STDERR_OUTPUT:
                test_error = "..." + test_error[-MAX_STDERR_OUTPUT:]
            
            next_prompt = f"""The experiment.py file has runtime errors:

{test_error}

Please fix these errors. Common issues to check:
1. Missing import statements
2. Undefined variables or functions
3. Incorrect data types or shapes
4. File I/O errors
5. Missing --out_dir argument handling
6. Incorrect final_info.json format

Please regenerate the corrected experiment.py file.
"""
            continue

        print("âœ… æµ‹è¯•è¿è¡ŒæˆåŠŸï¼")
        print("âœ… final_info.json æ ¼å¼æ­£ç¡®")
        
        # ====================================================================
        # Step 0.5: æµ‹è¯•è¿è¡Œ plot.py
        # ====================================================================
        print("\nğŸ“Š æµ‹è¯• plot.py çš„å¯è§†åŒ–åŠŸèƒ½...")
        plot_test_success, plot_test_error = test_plot_script(folder_name)

        if not plot_test_success:
            print(f"âŒ plot.py æµ‹è¯•å¤±è´¥:\n{plot_test_error}")
            next_prompt = f"""The plot.py file has issues:

        {plot_test_error}

        Please fix plot.py to:
        1. Accept --out_dir command-line argument
        2. Use matplotlib for visualizations
        3. Save plots to the specified output directory as PNG files
        4. Run without errors

        Please regenerate the corrected plot.py file.
        """
            continue
        else:
            print(f"âœ… plot.py æµ‹è¯•æˆåŠŸ: {plot_test_error}")
        
        # ====================================================================
        # Step 0.6: ä»£ç é€»è¾‘éªŒè¯ï¼ˆæ–°å¢æ­¥éª¤ï¼‰
        # ====================================================================
        print("\nğŸ” éªŒè¯ä»£ç é€»è¾‘æ˜¯å¦ç¬¦åˆä¼ªä»£ç ...")
        
        # è¯»å–å½“å‰ç”Ÿæˆçš„ä»£ç 
        with open(exp_file, 'r', encoding='utf-8') as f:
            current_code = f.read()
        
        # ç”Ÿæˆé€»è¾‘éªŒè¯æç¤ºè¯
        logic_prompt = logic_validation_prompt.format(
            title=title,
            pseudocode=pseudocode,
            current_code=current_code
        )
        
        print("ğŸ¤– AI æ­£åœ¨éªŒè¯ä»£ç é€»è¾‘...")
        logic_response = coder.run(logic_prompt)
        print(logic_response)
        
        # æ£€æŸ¥ AI æ˜¯å¦ç¡®è®¤ä»£ç é€»è¾‘æ­£ç¡®
        if "LOGIC_VALIDATION_PASSED" in logic_response:
            print("âœ… ä»£ç é€»è¾‘éªŒè¯é€šè¿‡ï¼")
            print()
            print("="*80)
            print("ğŸ‰ ä»£ç ç”Ÿæˆé˜¶æ®µå®Œæˆï¼")
            print("="*80)
            print()
            return True
        else:
            print("âŒ ä»£ç é€»è¾‘éªŒè¯å¤±è´¥ï¼ŒAI å‘ç°ä¸ä¼ªä»£ç ä¸ç¬¦ä¹‹å¤„")
            print("ğŸ”„ ç»§ç»­è¿­ä»£ä¿®å¤...")
            
            # ç”Ÿæˆä¸‹ä¸€è½®ä¿®å¤çš„æç¤ºè¯
            next_prompt = f"""The code logic validation failed. The AI identified discrepancies between the implementation and the pseudocode.

Please carefully review the pseudocode and fix the implementation:

# Algorithm Pseudocode
{pseudocode}

Key issues identified:
- The implementation does not correctly follow the pseudocode logic
- Some algorithmic steps may be missing or incorrectly implemented
- Mathematical formulas or procedures may not match

Please regenerate experiment.py to correctly implement the pseudocode logic.
"""
            continue
    
    # ========================================================================
    # è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°ä»æœªæˆåŠŸ
    # ========================================================================
    print("\n" + "="*80)
    print(f"âŒ ä»£ç ç”Ÿæˆå¤±è´¥ï¼šè¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•° ({MAX_CODE_GEN_ITERS})")
    print("="*80)
    print()
    print("å»ºè®®:")
    print("  1. æ£€æŸ¥ algorithm.tex ä¸­çš„ä¼ªä»£ç æ˜¯å¦æ¸…æ™°å®Œæ•´")
    print("  2. æ£€æŸ¥å®éªŒæè¿°æ˜¯å¦æä¾›äº†è¶³å¤Ÿçš„ä¸Šä¸‹æ–‡")
    print("  3. æ‰‹åŠ¨æ£€æŸ¥ç”Ÿæˆçš„ experiment.py å¹¶ä¿®å¤")
    print()
    
    return False

# ============================================================================
# è¾…åŠ©å‡½æ•°ï¼šæµ‹è¯•è¿è¡Œplot.py
# ============================================================================
def test_plot_script(folder_name):
    """
    æµ‹è¯• plot.py æ˜¯å¦èƒ½æ­£ç¡®å¤„ç† --out_dir å‚æ•°
    """
    cwd = osp.abspath(folder_name)
    test_plots_dir = osp.join(cwd, "test_plots")
    
    # æ¸…ç†ä¹‹å‰çš„æµ‹è¯•ç›®å½•
    if osp.exists(test_plots_dir):
        shutil.rmtree(test_plots_dir)
    
    # æ‰§è¡Œæµ‹è¯•å‘½ä»¤
    command = ["python", "plot.py", f"--out_dir={test_plots_dir}"]
    
    try:
        result = subprocess.run(
            command, 
            cwd=cwd, 
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE, 
            text=True, 
            timeout=120  # 2åˆ†é’Ÿè¶…æ—¶
        )
        
        # æ£€æŸ¥è¿”å›ç 
        if result.returncode != 0:
            error_msg = f"Plot test failed with return code {result.returncode}\n"
            error_msg += f"STDERR:\n{result.stderr}"
            return False, error_msg
        
        # æ£€æŸ¥æ˜¯å¦åˆ›å»ºäº†è¾“å‡ºç›®å½•å’Œå›¾è¡¨æ–‡ä»¶
        if not osp.exists(test_plots_dir):
            return False, "Plot test failed: output directory was not created"
        
        plot_files = [f for f in os.listdir(test_plots_dir) if f.endswith('.png')]
        if not plot_files:
            return False, "Plot test failed: no PNG files were generated"
        
        return True, f"Successfully generated plot files: {plot_files}"
        
    except TimeoutExpired:
        return False, f"Plot test timed out after 120 seconds"
    except Exception as e:
        return False, f"Error during plot test: {str(e)}"
    finally:
        # æ¸…ç†æµ‹è¯•ç›®å½•
        if osp.exists(test_plots_dir):
            shutil.rmtree(test_plots_dir)


# ============================================================================
# è¾…åŠ©å‡½æ•°ï¼šæ‰§è¡Œå•æ¬¡å®éªŒ
# ============================================================================

def run_experiment(folder_name, run_num, timeout=7200):
    """
    æ‰§è¡Œå•æ¬¡å®éªŒè¿è¡Œ
    
    å·¥ä½œæµç¨‹ï¼š
      1. ä¿å­˜å½“å‰ experiment.py çš„å¿«ç…§ï¼ˆç”¨äºè¿½æº¯ï¼‰
      2. æ‰§è¡Œå‘½ä»¤ï¼špython experiment.py --out_dir=run_{run_num}
      3. æ£€æŸ¥æ‰§è¡Œç»“æœï¼š
         - æˆåŠŸï¼šè¯»å– final_info.jsonï¼Œè¿”å›ç»“æœæ•°æ®
         - å¤±è´¥ï¼šæ¸…ç†å¤±è´¥çš„è¿è¡Œï¼Œè¿”å›é”™è¯¯ä¿¡æ¯
      4. ç”Ÿæˆåé¦ˆæç¤ºè¯ç»™ AI
    
    å‚æ•°ï¼š
      folder_name: å®éªŒæ–‡ä»¶å¤¹è·¯å¾„
      run_num: è¿è¡Œç¼–å·ï¼ˆ1, 2, 3, ...ï¼‰
      timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤ 2 å°æ—¶
    
    è¿”å›ï¼š
      (return_code, next_prompt): è¿”å›ç å’Œä¸‹ä¸€æ­¥çš„ AI æç¤ºè¯
    """
    cwd = osp.abspath(folder_name)
    
    # ========================================================================
    # Step 1: æ„é€ å¹¶æ‰§è¡Œå‘½ä»¤
    # ========================================================================
    # å›ºå®šæ ¼å¼ï¼špython experiment.py --out_dir=run_1
    # experiment.py å¿…é¡»æ”¯æŒ --out_dir å‚æ•°ï¼Œå¹¶ä¼šåˆ›å»º run_1/ ç›®å½•
    command = [
        "python",
        "experiment.py",
        f"--out_dir=run_{run_num}",
    ]
    
    try:
        # æ‰§è¡Œå‘½ä»¤ï¼Œæ•è·æ ‡å‡†é”™è¯¯è¾“å‡º
        result = subprocess.run(
            command, cwd=cwd, stderr=subprocess.PIPE, text=True, timeout=timeout
        )

        # æ‰“å°é”™è¯¯è¾“å‡ºï¼ˆå¦‚æœæœ‰ï¼‰
        if result.stderr:
            print(result.stderr, file=sys.stderr)

        # ====================================================================
        # Step 3: å¤„ç†æ‰§è¡Œç»“æœ
        # ====================================================================
        
        if result.returncode != 0:
            # ----------------------------------------------------------------
            # æƒ…å†µ A: æ‰§è¡Œå¤±è´¥
            # ----------------------------------------------------------------
            print(f"Run {run_num} failed with return code {result.returncode}")
            
            # æ¸…ç†å¤±è´¥çš„è¿è¡Œç›®å½•ï¼ˆé¿å…è„æ•°æ®ï¼‰
            if osp.exists(osp.join(cwd, f"run_{run_num}")):
                shutil.rmtree(osp.join(cwd, f"run_{run_num}"))
            
            print(f"Run failed with the following error {result.stderr}")
            
            # æˆªæ–­è¿‡é•¿çš„é”™è¯¯ä¿¡æ¯
            stderr_output = result.stderr
            if len(stderr_output) > MAX_STDERR_OUTPUT:
                stderr_output = "..." + stderr_output[-MAX_STDERR_OUTPUT:]
            
            # ç”Ÿæˆé”™è¯¯åé¦ˆæç¤ºè¯ï¼ˆAI ä¼šæ ¹æ®é”™è¯¯ä¿®å¤ä»£ç ï¼‰
            next_prompt = f"Run failed with the following error {stderr_output}"
            
        else:
            # ----------------------------------------------------------------
            # æƒ…å†µ B: æ‰§è¡ŒæˆåŠŸ
            # ----------------------------------------------------------------
            
            # ================================================================
            # Step 3.1: ä¿å­˜ä»£ç å¿«ç…§åˆ° run_N/ ç›®å½•
            # ================================================================
            # å°†å½“å‰çš„ experiment.py å’Œ plot.py å¤åˆ¶åˆ° run_N/ ç›®å½•
            # ä½œç”¨ï¼šè¿½æº¯æ¯æ¬¡è¿è¡Œä½¿ç”¨çš„ä»£ç ç‰ˆæœ¬ï¼ˆå› ä¸º AI ä¼šä¸æ–­ä¿®æ”¹è¿™äº›æ–‡ä»¶ï¼‰
            run_dir = osp.join(cwd, f"run_{run_num}")
            
            shutil.copy(
                osp.join(cwd, "experiment.py"),
                osp.join(run_dir, "experiment.py"),
            )
            
            # å¦‚æœ plot.py å­˜åœ¨ï¼Œä¹Ÿå¤åˆ¶ä¸€ä»½
            plot_file = osp.join(cwd, "plot.py")
            if osp.exists(plot_file):
                shutil.copy(plot_file, osp.join(run_dir, "plot.py"))
            
            # ================================================================
            # Step 3.2: è¯»å–å®éªŒç»“æœ
            # ================================================================
            # è¯»å–å®éªŒç»“æœæ–‡ä»¶ run_{run_num}/final_info.json
            # æ ¼å¼è¦æ±‚ï¼š{"metric": {"means": [...], "stds": [...]}}
            with open(osp.join(run_dir, "final_info.json"), "r") as f:
                results = json.load(f)
            
            # æå– means å€¼ï¼ˆä¸»è¦ç»“æœï¼‰
            results = {k: v["means"] for k, v in results.items()}

            # ç”ŸæˆæˆåŠŸåé¦ˆæç¤ºè¯ï¼ˆåŒ…å«ç»“æœæ•°æ®å’Œä¸‹ä¸€æ­¥æŒ‡ç¤ºï¼‰
            next_prompt = f"""Run {run_num} completed. Here are the results:
{results}

Decide if you need to re-plan your experiments given the result (you often will not need to).

Someone else will be using `notes.txt` to perform a writeup on this in the future.
Please include *all* relevant information for the writeup on Run {run_num}, including an experiment description and the run number. Be as verbose as necessary.

Then, implement the next thing on your list.
We will then run the command `python experiment.py --out_dir=run_{run_num + 1}'.
YOUR PROPOSED CHANGE MUST USE THIS COMMAND FORMAT, DO NOT ADD ADDITIONAL COMMAND LINE ARGS.
If you are finished with experiments, respond with 'ALL_COMPLETED'."""
        
        return result.returncode, next_prompt
        
    except TimeoutExpired:
        # ====================================================================
        # æƒ…å†µ C: æ‰§è¡Œè¶…æ—¶
        # ====================================================================
        print(f"Run {run_num} timed out after {timeout} seconds")
        
        # æ¸…ç†è¶…æ—¶çš„è¿è¡Œç›®å½•
        if osp.exists(osp.join(cwd, f"run_{run_num}")):
            shutil.rmtree(osp.join(cwd, f"run_{run_num}"))
        
        # ç”Ÿæˆè¶…æ—¶åé¦ˆæç¤ºè¯ï¼ˆAI ä¼šä¼˜åŒ–ä»£ç ä»¥å‡å°‘è¿è¡Œæ—¶é—´ï¼‰
        next_prompt = f"Run timed out after {timeout} seconds"
        return 1, next_prompt


# ============================================================================
# è¾…åŠ©å‡½æ•°ï¼šæ‰§è¡Œå¯è§†åŒ–ç”Ÿæˆ
# ============================================================================


## 2. ä¿®æ”¹å¯è§†åŒ–æ‰§è¡Œå‡½æ•°


# ============================================================================
# è¾…åŠ©å‡½æ•°ï¼šæ‰§è¡Œå¯è§†åŒ–ç”Ÿæˆ
# ============================================================================

def run_plotting(folder_name, timeout=600):
    """
    æ‰§è¡Œå¯è§†åŒ–è„šæœ¬ç”Ÿæˆå›¾è¡¨
    
    å·¥ä½œæµç¨‹ï¼š
      1. æ‰§è¡Œå‘½ä»¤ï¼špython plot.py --out_dir=plots
      2. plot.py åº”è¯¥è¯»å– run_1/, run_2/ ç­‰ç›®å½•çš„ç»“æœ
      3. ç”Ÿæˆ PNG å›¾è¡¨æ–‡ä»¶åˆ° plots/ ç›®å½•
    
    å‚æ•°ï¼š
      folder_name: å®éªŒæ–‡ä»¶å¤¹è·¯å¾„
      timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤ 10 åˆ†é’Ÿ
    
    è¿”å›ï¼š
      (return_code, next_prompt): è¿”å›ç å’Œåé¦ˆæç¤ºè¯
    """
    cwd = osp.abspath(folder_name)
    
    # åˆ›å»º plots ç›®å½•
    plots_dir = osp.join(cwd, "plots")
    if osp.exists(plots_dir):
        shutil.rmtree(plots_dir)
    os.makedirs(plots_dir)
    
    # æ£€æŸ¥ plot.py æ˜¯å¦å­˜åœ¨
    plot_file = osp.join(cwd, "plot.py")
    if not osp.exists(plot_file):
        return False, "plot.py æ–‡ä»¶ä¸å­˜åœ¨"
    
    # æ‰§è¡Œç»˜å›¾å‘½ä»¤
    command = ["python", "plot.py", f"--out_dir={plots_dir}"]
    
    try:
        print("ğŸ¨ ç”Ÿæˆå¤šåœºæ™¯å¯è§†åŒ–ç»“æœ...")
        result = subprocess.run(
            command, 
            cwd=cwd, 
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE, 
            text=True, 
            timeout=600  # 10åˆ†é’Ÿè¶…æ—¶
        )
        
        if result.returncode == 0:
            # æ£€æŸ¥ç”Ÿæˆçš„å›¾è¡¨æ–‡ä»¶
            plot_files = [f for f in os.listdir(plots_dir) if f.endswith('.png')]
            if plot_files:
                print(f"âœ… æˆåŠŸç”Ÿæˆ {len(plot_files)} ä¸ªå›¾è¡¨æ–‡ä»¶:")
                for plot_file in plot_files:
                    print(f"   ğŸ“Š {plot_file}")
                return True, f"ç”Ÿæˆ {len(plot_files)} ä¸ªå¯è§†åŒ–å›¾è¡¨"
            else:
                return False, "ç»˜å›¾å®Œæˆä½†æœªç”Ÿæˆå›¾è¡¨æ–‡ä»¶"
        else:
            error_msg = result.stderr[:500] if result.stderr else "Unknown error"
            return False, f"ç»˜å›¾å¤±è´¥: {error_msg}"
            
    except TimeoutExpired:
        return False, "ç»˜å›¾è¶…æ—¶"
    except Exception as e:
        return False, f"ç»˜å›¾å¼‚å¸¸: {str(e)}"


# ============================================================================
# ä¸»å‡½æ•°ï¼šæ‰§è¡Œå®Œæ•´çš„å®éªŒæµç¨‹
# ============================================================================

def perform_experiments(idea, folder_name, coder, baseline_results, algorithm_tex_path=None) -> bool:
    """
    æ‰§è¡Œå®Œæ•´çš„ AI é©±åŠ¨å®éªŒæµç¨‹

    æ–°å¢å‚æ•°:
      algorithm_tex_path: å¯é€‰ï¼ŒæŒ‡å‘ algorithm.tex çš„è·¯å¾„ï¼ˆç›¸å¯¹æˆ–ç»å¯¹ï¼‰ã€‚å¦‚æœæä¾›ï¼Œ
                          æ‰€æœ‰ä¼ªä»£ç è¯»å–å°†ä¼˜å…ˆä½¿ç”¨è¯¥è·¯å¾„ã€‚
    """
    # ========================================================================
    # é˜¶æ®µ 0: ä»£ç ç”Ÿæˆä¸éªŒè¯ï¼ˆä½¿ç”¨ algorithm.texï¼‰
    # ========================================================================
    tex_pseudocode = read_pseudocode_from_tex(folder_name=folder_name, tex_path=algorithm_tex_path)
    if tex_pseudocode:
        print("\n" + "="*80)
        print("ğŸ“‹ æ£€æµ‹åˆ° algorithm.texï¼Œå¯åŠ¨ä»£ç ç”Ÿæˆé˜¶æ®µï¼ˆä¸ä½¿ç”¨ idea['Pseudocode']ï¼‰")
        print("="*80 + "\n")
        
        success = generate_code_from_pseudocode(idea, folder_name, coder, algorithm_tex_path=algorithm_tex_path)
        
        if not success:
            print("\nâŒ ä»£ç ç”Ÿæˆé˜¶æ®µå¤±è´¥ï¼Œæ— æ³•ç»§ç»­åç»­å®éªŒ")
            return False
        
        print("âœ… ä»£ç ç”Ÿæˆé˜¶æ®µå®Œæˆï¼Œè¿›å…¥å®éªŒè¿­ä»£é˜¶æ®µ\n")
    else:
        print("\n" + "="*80)
        print("â„¹ï¸  æœªæ£€æµ‹åˆ° algorithm.texï¼Œè·³è¿‡ä»£ç ç”Ÿæˆé˜¶æ®µï¼ˆä¸ä½¿ç”¨ idea['Pseudocode']ï¼‰")
        print("="*80 + "\n")

    # ========================================================================
    # é˜¶æ®µ 0.5: AI è‡ªä¸»åœºæ™¯è®¾è®¡ï¼ˆæ–°å¢ï¼‰
    # ========================================================================
    scenario_design = design_experiment_scenarios(idea, folder_name, coder, baseline_results, algorithm_tex_path=algorithm_tex_path)
    ai_scenarios = scenario_design.get("scenarios", [])
    
    if not ai_scenarios:
        print("âŒ æœªèƒ½è·å¾—æœ‰æ•ˆçš„ AI è®¾è®¡åœºæ™¯ï¼Œæ— æ³•ç»§ç»­")
        return False

    # ========================================================================
    # é˜¶æ®µ 1: è¿­ä»£å®éªŒå¾ªç¯ï¼ˆä¿®æ”¹ä¸ºæ”¯æŒ AI è®¾è®¡åœºæ™¯ï¼‰
    # ========================================================================
    print("\n" + "="*80)
    print("ğŸ”¬ é˜¶æ®µ 1: è¿­ä»£å®éªŒå¾ªç¯ï¼ˆé›†æˆ AI è®¾è®¡åœºæ™¯ï¼‰")
    print("="*80 + "\n")
    
    # æ‰§è¡Œ AI è®¾è®¡çš„åœºæ™¯
    scenario_results = execute_ai_designed_scenarios(folder_name, ai_scenarios)
    
    # åˆ†ææ‰§è¡Œç»“æœ
    successful_scenarios = [s for s in scenario_results if s["status"] == "success"]
    failed_scenarios = [s for s in scenario_results if s["status"] != "success"]
    
    print(f"\nğŸ“Š AI è®¾è®¡åœºæ™¯æ‰§è¡Œæ€»ç»“:")
    print(f"   âœ… æˆåŠŸ: {len(successful_scenarios)}/{len(ai_scenarios)}")
    print(f"   âŒ å¤±è´¥: {len(failed_scenarios)}/{len(ai_scenarios)}")
    
    if successful_scenarios:
        print("\nâœ… æˆåŠŸæ‰§è¡Œçš„ AI è®¾è®¡åœºæ™¯:")
        for scenario in successful_scenarios:
            print(f"   â€¢ {scenario['name']}")
    
    if failed_scenarios:
        print("\nâŒ å¤±è´¥çš„ AI è®¾è®¡åœºæ™¯:")
        for scenario in failed_scenarios:
            print(f"   â€¢ {scenario['name']}: {scenario.get('error', 'Unknown error')}")
    
    # ä¿å­˜åœºæ™¯æ‰§è¡Œç»“æœ
    results_file = osp.join(folder_name, "scenario_execution_results.json")
    with open(results_file, 'w', encoding='utf-8') as f:
        json.dump({
            "scenario_design": scenario_design,
            "execution_results": scenario_results
        }, f, indent=2, ensure_ascii=False)
    
    print(f"ğŸ’¾ åœºæ™¯æ‰§è¡Œç»“æœå·²ä¿å­˜åˆ°: {results_file}")
    
    # ========================================================================
    # åŸæœ‰çš„è¿­ä»£å®éªŒå¾ªç¯ï¼ˆä¿æŒä¸å˜ï¼Œä½œä¸ºå¤‡é€‰ï¼‰
    # ========================================================================
    if not successful_scenarios:
        print("\nâš ï¸  AI è®¾è®¡åœºæ™¯å…¨éƒ¨å¤±è´¥ï¼Œå›é€€åˆ°åŸæœ‰å®éªŒå¾ªç¯")
        return perform_original_experiment_loop(idea, folder_name, coder, baseline_results)
    
    # ========================================================================
    # é˜¶æ®µ 2: å¯è§†åŒ–ç”Ÿæˆï¼ˆå¢å¼ºç‰ˆï¼Œæ”¯æŒå¤šåœºæ™¯ï¼‰
    # ========================================================================
    print("\n" + "="*80)
    print("ğŸ“Š é˜¶æ®µ 2: å¤šåœºæ™¯å¯è§†åŒ–ç”Ÿæˆ")
    print("="*80 + "\n")
    
    plot_success, plot_message = generate_comprehensive_visualization(folder_name)
    
    if plot_success:
        print(f"âœ… {plot_message}")
    else:
        print(f"âŒ {plot_message}")
        
        # è®© AI ä¿®å¤å¯è§†åŒ–ä»£ç 
        print("ğŸ¤– AI æ­£åœ¨ä¿®å¤å¯è§†åŒ–ä»£ç ...")
        fix_plot_prompt = """
The multi-scenario plotting failed. Please fix plot.py to properly visualize all the experimental scenarios.

Key requirements:
1. Read results from all successful scenario directories
2. Generate comprehensive comparison plots showing different scenarios
3. Use clear labels and legends to distinguish different scenarios
4. Create multiple plot types to show different aspects of the results:
   - Convergence curves for each metric across scenarios
   - Scenario comparison bar charts
   - Performance summary plots

Please provide the complete, corrected plot.py file.
"""
        coder.run(fix_plot_prompt)
        
        # é‡æ–°å°è¯•ç»˜å›¾
        print("ğŸ”„ é‡æ–°å°è¯•ç”Ÿæˆå¯è§†åŒ–...")
        plot_success, plot_message = generate_comprehensive_visualization(folder_name)
        
        if plot_success:
            print(f"âœ… ä¿®å¤æˆåŠŸ: {plot_message}")
        else:
            print(f"âŒ ä¿®å¤åä»ç„¶å¤±è´¥: {plot_message}")
    
    # ========================================================================
    # é˜¶æ®µ 3: æ–‡æ¡£æ›´æ–°ï¼ˆå¢å¼ºç‰ˆï¼ŒåŒ…å« AI åœºæ™¯è®¾è®¡ä¿¡æ¯ï¼‰
    # ========================================================================
    print("\n" + "="*80)
    print("ğŸ“ é˜¶æ®µ 3: å®éªŒæ–‡æ¡£æ›´æ–°ï¼ˆåŒ…å« AI åœºæ™¯è®¾è®¡ï¼‰")
    print("="*80 + "\n")
    
    documentation_prompt = f"""
Please update notes.txt with a comprehensive summary of the AI-driven experimental process, including:

# AI-Designed Experimental Scenarios
{json.dumps(scenario_design, indent=2)}

# Execution Results
Successful scenarios: {len(successful_scenarios)}/{len(ai_scenarios)}
Failed scenarios: {len(failed_scenarios)}/{len(ai_scenarios)}

Please include the following sections in your documentation:

1. **Scenario Design Rationale**: Explain why the AI chose these specific scenarios and how they test different aspects of the algorithm
2. **Results Analysis**: Detailed analysis of results from each successful scenario, comparing them with expectations
3. **Algorithm Insights**: What we learned about the algorithm's behavior across different conditions
4. **Visualization Explanation**: Describe what each generated plot shows and the insights it provides
5. **Conclusions and Recommendations**: Overall conclusions and suggestions for further investigation based on the multi-scenario analysis

Make sure to reference specific scenarios and their results in your analysis, and connect the findings back to the original algorithm design.
"""
    
    print("ğŸ¤– AI æ­£åœ¨æ›´æ–°å®éªŒæ–‡æ¡£...")
    coder.run(documentation_prompt)
    print("âœ… æ–‡æ¡£æ›´æ–°å®Œæˆ")
    
    # ========================================================================
    # å®éªŒæµç¨‹å®Œæˆ
    # ========================================================================
    print("\n" + "="*80)
    print("ğŸ‰ AI é©±åŠ¨çš„å¤šåœºæ™¯å®éªŒæµç¨‹å®Œæˆï¼")
    print("="*80 + "\n")
    
    return len(successful_scenarios) > 0

def execute_ai_designed_scenarios(folder_name, scenarios):
    """
    æ‰§è¡Œ AI è®¾è®¡çš„åœºæ™¯
    """
    cwd = osp.abspath(folder_name)
    results = []
    
    print(f"ğŸš€ å¼€å§‹æ‰§è¡Œ {len(scenarios)} ä¸ª AI è®¾è®¡çš„åœºæ™¯")
    
    for i, scenario in enumerate(scenarios, 1):
        scenario_name = scenario.get('name', f'scenario_{i}').replace(' ', '_').lower()
        parameters = scenario.get('parameters', {})
        
        print(f"\n--- åœºæ™¯ {i}/{len(scenarios)}: {scenario_name} ---")
        print(f"   æè¿°: {scenario.get('description', 'No description')}")
        
        # æ„å»ºå‘½ä»¤å‚æ•°åˆ—è¡¨
        args_list = []
        for key, value in parameters.items():
            args_list.append(f"{key}={value}")
        
        # æ‰§è¡Œåœºæ™¯
        success, result_info = execute_single_scenario(folder_name, scenario_name, args_list)
        
        scenario_result = {
            "name": scenario_name,
            "description": scenario.get('description', ''),
            "parameters": parameters,
            "expected_insight": scenario.get('expected_insight', ''),
            "status": "success" if success else "failed",
            "run_dir": f"run_{scenario_name}"
        }
        
        if not success:
            scenario_result["error"] = result_info
        
        results.append(scenario_result)
        
        # æ·»åŠ çŸ­æš‚å»¶è¿Ÿé¿å…èµ„æºå†²çª
        import time
        time.sleep(2)
    
    return results


def execute_single_scenario(folder_name, scenario_name, args_list):
    """
    æ‰§è¡Œå•ä¸ªåœºæ™¯
    """
    cwd = osp.abspath(folder_name)
    run_dir = f"run_{scenario_name}"
    
    # æ„å»ºå®Œæ•´å‘½ä»¤
    command = ["python", "experiment.py", f"--out_dir={run_dir}"] + args_list
    
    try:
        print(f"   æ‰§è¡Œ: {' '.join(command)}")
        result = subprocess.run(
            command, 
            cwd=cwd, 
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE, 
            text=True, 
            timeout=3600  # 1å°æ—¶è¶…æ—¶
        )
        
        if result.returncode == 0:
            print(f"   âœ… åœºæ™¯ '{scenario_name}' å®Œæˆ")
            
            # æ£€æŸ¥æ˜¯å¦ç”Ÿæˆäº†ç»“æœæ–‡ä»¶
            result_file = osp.join(cwd, run_dir, "final_info.json")
            if osp.exists(result_file):
                print(f"   ğŸ“„ ç»“æœæ–‡ä»¶: {result_file}")
            else:
                print(f"   âš ï¸  åœºæ™¯å®Œæˆä½†æœªç”Ÿæˆç»“æœæ–‡ä»¶")
            
            return True, "Success"
        else:
            print(f"   âŒ åœºæ™¯ '{scenario_name}' å¤±è´¥")
            error_msg = result.stderr[:500] if result.stderr else "Unknown error"
            
            # æ¸…ç†å¤±è´¥çš„è¿è¡Œç›®å½•
            failed_dir = osp.join(cwd, run_dir)
            if osp.exists(failed_dir):
                shutil.rmtree(failed_dir)
            
            return False, error_msg
            
    except TimeoutExpired:
        print(f"   â° åœºæ™¯ '{scenario_name}' è¶…æ—¶")
        
        # æ¸…ç†è¶…æ—¶çš„è¿è¡Œç›®å½•
        timeout_dir = osp.join(cwd, run_dir)
        if osp.exists(timeout_dir):
            shutil.rmtree(timeout_dir)
            
        return False, "Timeout"
    except Exception as e:
        print(f"   ğŸ’¥ åœºæ™¯ '{scenario_name}' å¼‚å¸¸: {str(e)}")
        return False, str(e)


def generate_comprehensive_visualization(folder_name):
    """
    ç”Ÿæˆç»¼åˆå¯è§†åŒ–ç»“æœ
    """
    cwd = osp.abspath(folder_name)
    plots_dir = osp.join(cwd, "multi_scenario_plots")
    
    # æ¸…ç†å¹¶åˆ›å»ºç»˜å›¾ç›®å½•
    if osp.exists(plots_dir):
        shutil.rmtree(plots_dir)
    os.makedirs(plots_dir)
    
    # æ£€æŸ¥ plot.py æ˜¯å¦å­˜åœ¨
    plot_file = osp.join(cwd, "plot.py")
    if not osp.exists(plot_file):
        return False, "plot.py æ–‡ä»¶ä¸å­˜åœ¨"
    
    # æ‰§è¡Œç»˜å›¾å‘½ä»¤
    command = ["python", "plot.py", f"--out_dir={plots_dir}"]
    
    try:
        print("ğŸ¨ ç”Ÿæˆå¤šåœºæ™¯å¯è§†åŒ–ç»“æœ...")
        result = subprocess.run(
            command, 
            cwd=cwd, 
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE, 
            text=True, 
            timeout=600  # 10åˆ†é’Ÿè¶…æ—¶
        )
        
        if result.returncode == 0:
            # æ£€æŸ¥ç”Ÿæˆçš„å›¾è¡¨æ–‡ä»¶
            plot_files = [f for f in os.listdir(plots_dir) if f.endswith('.png')]
            if plot_files:
                print(f"âœ… æˆåŠŸç”Ÿæˆ {len(plot_files)} ä¸ªå›¾è¡¨æ–‡ä»¶:")
                for plot_file in plot_files:
                    print(f"   ğŸ“Š {plot_file}")
                return True, f"ç”Ÿæˆ {len(plot_files)} ä¸ªå¯è§†åŒ–å›¾è¡¨"
            else:
                return False, "ç»˜å›¾å®Œæˆä½†æœªç”Ÿæˆå›¾è¡¨æ–‡ä»¶"
        else:
            error_msg = result.stderr[:500] if result.stderr else "Unknown error"
            return False, f"ç»˜å›¾å¤±è´¥: {error_msg}"
            
    except TimeoutExpired:
        return False, "ç»˜å›¾è¶…æ—¶"
    except Exception as e:
        return False, f"ç»˜å›¾å¼‚å¸¸: {str(e)}"


def perform_original_experiment_loop(idea, folder_name, coder, baseline_results):
    """
    åŸæœ‰çš„å®éªŒå¾ªç¯ï¼ˆä½œä¸ºå¤‡é€‰æ–¹æ¡ˆï¼‰
    """
    print("\nğŸ”„ æ‰§è¡ŒåŸæœ‰å®éªŒå¾ªç¯...")
    
    current_iter = 0
    run = 1
    
    # ç”Ÿæˆåˆå§‹æç¤ºè¯
    next_prompt = coder_prompt.format(
        title=idea["Title"],
        idea=idea["Experiment"],
        max_runs=MAX_RUNS,
        baseline_results=baseline_results,
    )
    
    # å®éªŒå¾ªç¯
    while run < MAX_RUNS + 1:
        if current_iter >= MAX_ITERS:
            print("âš ï¸ è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œåœæ­¢å½“å‰è¿è¡Œ")
            break
        
        print(f"\n--- Run {run} (å°è¯• {current_iter + 1}/{MAX_ITERS}) ---")
        print("ğŸ¤– AI æ­£åœ¨åˆ†æå’Œä¿®æ”¹ä»£ç ...")
        coder_out = coder.run(next_prompt)
        print(coder_out)
        
        if "ALL_COMPLETED" in coder_out:
            print("âœ… AI è¡¨ç¤ºæ‰€æœ‰å®éªŒå·²å®Œæˆ")
            break
        
        print(f"âš™ï¸ æ‰§è¡Œå®éªŒ: python experiment.py --out_dir=run_{run}")
        return_code, next_prompt = run_experiment(folder_name, run)
        
        if return_code == 0:
            print(f"âœ… Run {run} æˆåŠŸå®Œæˆ")
            run += 1
            current_iter = 0
        else:
            print(f"âŒ Run {run} å¤±è´¥ï¼Œå‡†å¤‡é‡è¯•")
            current_iter += 1
    
    if current_iter >= MAX_ITERS:
        print("\nâŒ å®éªŒå¾ªç¯æœªèƒ½å®Œæˆæ‰€æœ‰è¿è¡Œï¼ˆè¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼‰")
        return False
    
    print(f"\nâœ… å®éªŒå¾ªç¯å®Œæˆï¼Œå…±å®Œæˆ {run - 1} æ¬¡è¿è¡Œ")
    
    # æ‰§è¡Œå¯è§†åŒ–
    print("\nğŸ“Š æ‰§è¡Œå¯è§†åŒ–ç”Ÿæˆ...")
    current_iter = 0
    next_prompt = """
Please modify `plot.py` to generate the most relevant plots for the final writeup. 
Be sure to fill in the "labels" dictionary with the correct names for each run that you want to plot.
"""
    
    while True:
        print(f"\nğŸ¤– AI æ­£åœ¨ä¿®æ”¹ plot.py (å°è¯• {current_iter + 1}/{MAX_ITERS})...")
        _ = coder.run(next_prompt)
        
        print("âš™ï¸ æ‰§è¡Œç»˜å›¾: python plot.py")
        return_code, next_prompt = run_plotting(folder_name)
        
        current_iter += 1
        
        if return_code == 0:
            print("âœ… å¯è§†åŒ–ç”ŸæˆæˆåŠŸ")
            break
        elif current_iter >= MAX_ITERS:
            print("âš ï¸ å¯è§†åŒ–ç”Ÿæˆå¤±è´¥ï¼ˆè¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼‰")
            break
        else:
            print("âŒ ç»˜å›¾å¤±è´¥ï¼Œå‡†å¤‡é‡è¯•")
    
    # æ–‡æ¡£æ›´æ–°
    print("\nğŸ“ æ›´æ–°å®éªŒæ–‡æ¡£...")
    next_prompt = """
Please modify `notes.txt` with a description of what each plot shows along with the filename of the figure. 
Somebody else will be using `notes.txt` to write a report on this in the future.
"""
    coder.run(next_prompt)
    print("âœ… æ–‡æ¡£æ›´æ–°å®Œæˆ")
    
    return True
