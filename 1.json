{
    "Title": "Experiment: A Comparative Study of Convergence between Federated Averaging (FedAvg) and Subspace Federated Averaging (SFedAvg)",
    "Pseudocode": "Algorithm: Subspace Federated Averaging (SFedAvg) - Simulation based on Low-Rank Gradient Approximation\n\nInput:\n  - T: Total number of communication rounds\n  - K: Number of clients participating in each round\n  - E: Number of local training epochs for each client\n  - η: Learning rate\n  - r: Subspace rank ratio (0 < r <= 1.0)\n  - θ⁰: Initial global model parameters\n\nOutput: The trained global model parameters θᵀ\n\nProcedure:\n1. Initialize global model θ⁰\n2. FOR t = 0 TO T-1 DO: (Main loop for each communication round)\n   a. Server selects a subset of K clients, S_t\n   b. Server broadcasts the current global model θᵗ to all selected clients\n   c. FOR each client i ∈ S_t IN PARALLEL DO:\n      i.   Download global model: θ_local ← θᵗ\n      ii.  FOR e = 1 TO E DO: (Local training)\n          - FOR each minibatch B in local data D_i DO:\n              1. Compute the gradient of the loss function with respect to the current local model parameters: g ← ∇_θ ℓ(θ_local; B)\n              2. // --- Core modification ---\n              3. FOR each parameter p in θ_local DO:\n                 - Get the corresponding gradient g_p\n                 - Find the low-rank approximation of the gradient via Singular Value Decomposition (SVD):\n                   U, S, V = SVD(g_p)\n                   k = floor(r * number_of_singular_values)\n                   g_p_low_rank = U[:, :k] @ diag(S[:k]) @ V.T[:, :k]\n                 - Replace the original gradient with its low-rank approximation: g_p ← g_p_low_rank\n              4. Update the local model using the modified low-rank gradient: θ_local ← θ_local - η * g\n      iii. Return the updated local model θ_i^{t+1} to the server\n   d. Server aggregates the received models:\n      θ^{t+1} ← (1/K) * Σ_{i∈S_t} θ_i^{t+1}\n3. RETURN θᵀ\n\nKey Components:\n- Client Local Update: Clients perform multiple epochs of SGD on their local data.\n- Gradient Low-Rank Approximation: Before each parameter update, the full-dimensional gradient is approximated by a low-rank matrix via SVD. This simulates an update within an 'important subspace' and is the core difference from standard FedAvg.\n- Server Aggregation: The server uses the same parameter averaging strategy as standard FedAvg to aggregate client models.",
    "Experiment": "Compare the convergence behavior of Federated Averaging (FedAvg) and Subspace Federated Averaging (SFedAvg) on a standard dataset (e.g., MNIST or CIFAR-10) under identical settings. Measure metrics such as training loss, test accuracy, and communication efficiency over multiple rounds of training."
}