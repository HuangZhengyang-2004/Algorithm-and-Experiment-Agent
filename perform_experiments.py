"""
å®éªŒæ‰§è¡Œå¼•æ“ (Experiment Execution Engine)
==========================================

æœ¬æ¨¡å—è´Ÿè´£æ‰§è¡Œ AI é©±åŠ¨çš„è‡ªåŠ¨åŒ–å®éªŒæµç¨‹ï¼ŒåŒ…æ‹¬ï¼š
0. ä»£ç ç”Ÿæˆé˜¶æ®µï¼ˆæ ¹æ®ä¼ªä»£ç ç”Ÿæˆåˆå§‹å®éªŒä»£ç ï¼‰
1. è¿­ä»£å®éªŒå¾ªç¯ï¼ˆAI ä¿®æ”¹ä»£ç  â†’ æ‰§è¡Œ â†’ åé¦ˆï¼‰
2. å¯è§†åŒ–ç”Ÿæˆ
3. æ–‡æ¡£æ›´æ–°

æ ¸å¿ƒæµç¨‹ï¼š
  é˜¶æ®µ 0: ä»£ç ç”Ÿæˆä¸éªŒè¯ (generate_code_from_pseudocode)
    - AI æ ¹æ®ä¼ªä»£ç ç”Ÿæˆ experiment.py å’Œ plot.py
    - ç³»ç»ŸéªŒè¯ä»£ç å¯è¿è¡Œæ€§ï¼ˆè¯­æ³•æ£€æŸ¥ + è¯•è¿è¡Œï¼‰
    - è¿­ä»£ä¿®å¤ç›´åˆ°ä»£ç å¯ä»¥æ­£å¸¸è¿è¡Œ
    - æ£€éªŒä»£ç æ˜¯å¦ç¬¦åˆç®—æ³•ä¼ªä»£ç ï¼Œè¿­ä»£ä¿®å¤çŸ¥é“è¿è¡ŒæˆåŠŸ --new
  
  é˜¶æ®µ 1: å®éªŒå¾ªç¯ (perform_experiments)
    - AI æ ¹æ®æƒ³æ³•ç”Ÿæˆ/ä¿®æ”¹ experiment.py
    - ç³»ç»Ÿæ‰§è¡Œå®éªŒå¹¶æ”¶é›†ç»“æœ
    - å°†ç»“æœåé¦ˆç»™ AIï¼Œè¿›è¡Œä¸‹ä¸€è½®è¿­ä»£
  
  é˜¶æ®µ 2: å¯è§†åŒ–ç”Ÿæˆ (run_plotting)
    - AI ä¿®æ”¹ plot.py ç”Ÿæˆå¯¹æ¯”å›¾è¡¨
    - ç³»ç»Ÿæ‰§è¡Œç»˜å›¾è„šæœ¬
  
  é˜¶æ®µ 3: æ–‡æ¡£æ›´æ–°
    - AI æ›´æ–° notes.txtï¼Œè®°å½•å®éªŒå‘ç°å’Œç»“è®º
"""

import json
import os
import os.path as osp
import shutil
import subprocess
import sys
from datetime import datetime
from subprocess import TimeoutExpired

# ============================================================================
# é…ç½®å¸¸é‡
# ============================================================================

MAX_ITERS = 4           # æ¯æ¬¡è¿è¡Œå¤±è´¥åçš„æœ€å¤§é‡è¯•æ¬¡æ•°
MAX_RUNS = 5            # æ€»å…±å…è®¸çš„æœ€å¤§å®éªŒè¿è¡Œæ¬¡æ•°
MAX_STDERR_OUTPUT = 1500  # é”™è¯¯ä¿¡æ¯çš„æœ€å¤§æ˜¾ç¤ºé•¿åº¦ï¼ˆå­—ç¬¦æ•°ï¼‰
MAX_CODE_GEN_ITERS = 10  # ä»£ç ç”Ÿæˆé˜¶æ®µçš„æœ€å¤§è¿­ä»£æ¬¡æ•°

# å‚æ•°è°ƒä¼˜é…ç½®
ENABLE_HYPERPARAMETER_TUNING = True  # æ˜¯å¦å¯ç”¨åœºæ™¯çº§å‚æ•°è°ƒä¼˜ï¼ˆåœºæ™¯æ‰§è¡ŒæˆåŠŸåç«‹å³è°ƒä¼˜ï¼‰
MAX_TUNING_CONFIGS_PER_SCENARIO = 8  # æ¯ä¸ªåœºæ™¯æœ€å¤šæµ‹è¯•çš„å‚æ•°é…ç½®æ•°é‡

# ============================================================================
# AI æç¤ºè¯æ¨¡æ¿ - é˜¶æ®µ 0: ä»£ç ç”Ÿæˆ
# ============================================================================

code_generation_prompt = """You are an expert algorithm researcher and programmer. Your task is to implement a complete experimental framework based on the following algorithm pseudocode.

# Algorithm Title
{title}

# Algorithm Pseudocode
{pseudocode}

# Experiment Description
{experiment_description}

# Your Task
The files experiment.py and plot.py currently contain only basic skeleton code. You need to REPLACE the TODO sections with complete implementations of the algorithm and experimental framework.

Implement the following:

## 1. experiment.py
This file must contain:
- Complete implementation of the algorithm described in the pseudocode
- Data generation/loading functions
- Training/optimization loop
- Evaluation metrics computation
- Command-line argument parsing with **required** --out_dir parameter
- Result saving to {{out_dir}}/final_info.json

**Critical Requirements for experiment.py:**
```python
import argparse
import json
import os

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--out_dir', type=str, required=True)
    
    # CRITICAL: Expose ALL algorithm-specific parameters as command-line arguments!
    # For federated learning / subspace methods, include:
    parser.add_argument('--learning_rate', type=float, default=0.01, help='Learning rate')
    parser.add_argument('--num_iterations', type=int, default=100, help='Number of rounds/epochs')
    parser.add_argument('--batch_size', type=int, default=32, help='Minibatch size')
    parser.add_argument('--momentum', type=float, default=0.9, help='Momentum coefficient')
    # For subspace-based algorithms (SFedAvg, GaLore, etc.):
    parser.add_argument('--subspace_dim', type=int, default=10, help='Subspace dimension r')
    parser.add_argument('--local_steps', type=int, default=5, help='Local SGD steps tau')
    parser.add_argument('--client_fraction', type=float, default=0.5, help='Client sampling fraction')
    # Add other algorithm-specific parameters as needed
    
    args = parser.parse_args()
    
    # Create output directory
    os.makedirs(args.out_dir, exist_ok=True)
    
    # ... your algorithm implementation ...
    
    # Save results in the required format
    results = {{
        "metric_name": {{
            "means": [value1, value2, ...],  # List of values over iterations/epochs
            "stds": [std1, std2, ...]        # Standard deviations (can be zeros)
        }}
    }}
    
    with open(f"{{args.out_dir}}/final_info.json", "w") as f:
        json.dump(results, f, indent=2)

if __name__ == "__main__":
    main()
```

**IMPORTANT: Parameter Exposure**
- DO NOT hardcode algorithm hyperparameters (learning_rate, momentum, subspace_dim, etc.)
- ALWAYS expose them as command-line arguments with reasonable defaults
- This enables hyperparameter tuning to optimize ALL aspects of the algorithm
- For comparison studies (e.g., FedAvg vs SFedAvg), ensure both algorithms can use the same parameter interface

**CRITICAL OUTPUT STRUCTURE REQUIREMENT:**
- Results MUST be saved to: {{out_dir}}/baseline/final_info.json
- Create a 'baseline' subdirectory to store results
- This enables future parameter tuning experiments to be organized separately
- File structure:
  ```
  {{out_dir}}/
      â”œâ”€â”€ baseline/
      â”‚   â””â”€â”€ final_info.json    â† Save results here
      â””â”€â”€ experiment.py (snapshot)
  ```
- Implementation:
  ```python
  baseline_dir = os.path.join(args.out_dir, "baseline")
  os.makedirs(baseline_dir, exist_ok=True)
  
  # ... run experiment ...
  
  with open(os.path.join(baseline_dir, "final_info.json"), "w") as f:
      json.dump(results, f, indent=2)
  ```

**CRITICAL IMPLEMENTATION REQUIREMENTS:**

1. **NO PLACEHOLDER CODE:**
   - NEVER use `np.random.randn()` or `np.random.rand()` for gradients, losses, or training data
   - NEVER leave `TODO` comments or `Placeholder` text in the final code
   - ALL functions must be FULLY implemented with real computations
   - Every value must be computed from actual data and model predictions

2. **REAL GRADIENT COMPUTATION:**
   - Gradients MUST be computed from actual model predictions and data
   - For regression: `gradient = X.T @ (predictions - y) / n_samples`
   - For classification: use appropriate loss function derivatives
   - NEVER use random values as gradients

3. **PROPER MODEL INITIALIZATION:**
   - In federated learning: local models MUST copy global model weights
   - Use: `local_model.weights = global_model.weights.copy()`
   - DO NOT create new random models for each client

4. **LEARNING VERIFICATION:**
   - Your implementation MUST show learning progress
   - Loss/error values SHOULD decrease over training iterations
   - If metrics don't change, your implementation is WRONG
   - For optimization: final values should be significantly better than initial values

5. **DATA USAGE:**
   - Use the ACTUAL training data provided or generated
   - Compute predictions using the model: `predictions = model.predict(X)`
   - Compute errors/losses from predictions and true labels
   - Update model based on these real computations

## 2. plot.py
This file must contain:
- Functions to read results from multiple run directories
- Plotting code for convergence curves and comparisons
- A `labels` dictionary to specify which runs to plot
- MUST accept --out_dir command-line argument for saving plots
- Save plots as PNG files

**Example structure:**
```python
import matplotlib.pyplot as plt
import json
import os
import argparse
import numpy as np

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--out_dir', type=str, required=True, help='Directory to save plots')
    args = parser.parse_args()
    
    # Create output directory
    os.makedirs(args.out_dir, exist_ok=True)
    
    # Dictionary mapping run directories to labels
    labels = {{
        "run_1": "Baseline",
        "run_2": "Variant 1",
        # Will be updated later with more runs
    }}
    
    # Initialize matplotlib with professional style
    plt.style.use('seaborn-v0_8-whitegrid')
    fig, axes = plt.subplots(1, 1, figsize=(10, 6))
    
    # Colors for different runs
    colors = plt.cm.tab10(np.linspace(0, 1, len(labels)))
    
    for i, (run_dir, label) in enumerate(labels.items()):
        if os.path.exists(f"{{run_dir}}/final_info.json"):
            with open(f"{{run_dir}}/final_info.json") as f:
                data = json.load(f)
            
            # Plot each metric
            for metric_name, metric_data in data.items():
                means = metric_data["means"]
                stds = metric_data["stds"]
                iterations = range(len(means))
                
                # Plot mean with standard deviation shading
                axes.plot(iterations, means, label=f"{{label}} - {{metric_name}}", 
                         color=colors[i], linewidth=2)
                axes.fill_between(iterations, 
                                np.array(means) - np.array(stds),
                                np.array(means) + np.array(stds),
                                alpha=0.2, color=colors[i])
    
    # Customize plot
    axes.set_xlabel('Iterations/Epochs', fontsize=12)
    axes.set_ylabel('Metric Value', fontsize=12)
    axes.set_title('Algorithm Performance Comparison', fontsize=14)
    axes.legend()
    axes.grid(True, alpha=0.3)
    
    # Save plots to the specified output directory
    plot_path = os.path.join(args.out_dir, "comparison.png")
    plt.tight_layout()
    plt.savefig(plot_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"Saved plot to: {{plot_path}}")

if __name__ == "__main__":
    main()
```
**CRITICAL REQUIREMENTS for plot.py:**
    - MUST use matplotlib for all visualizations
    - MUST accept --out_dir argument for specifying where to save plots
    - MUST save plots to {{args.out_dir}}/plot_name.png
    - Should create professional-looking plots with:
        - Clear labels and legends
        - Grid for readability
        - Error bars or shaded regions for standard deviations
        - High resolution (dpi=300)
        - Should handle multiple metrics and multiple runs appropriately
**IMPORTANT:**
Always test your plot.py locally before submission
- Run: python plot.py --out_dir=test_plots
- Check that it creates the directory and saves PNG files
- Ensure no import errors or runtime errors

## 3. Key Implementation Guidelines
- Follow the pseudocode logic closely
- Use appropriate libraries (numpy, scipy, sklearn, torch, etc.)
- Include error handling and input validation
- Add comments explaining key algorithmic steps
- Make the code production-ready and well-documented
- Ensure experiment.py can run standalone with: `python experiment.py --out_dir=test_run`

## 4. Testing Requirements
The generated code must:
- Pass Python syntax validation (no syntax errors)
- Run without crashing (handle imports, data generation, etc.)
- Generate the required output file (final_info.json) with correct format
- Complete execution within reasonable time (for initial test)

## 5. Important Note on Future Modifications
After initial generation, all future modifications should be MINIMAL and TARGETED:
- Use search/replace for specific fixes
- Make surgical edits to only the problematic sections
- DO NOT regenerate entire files unless absolutely necessary
- This is to conserve computational resources

Please generate BOTH experiment.py and plot.py now. Make sure they are complete, runnable, and follow all requirements above.

**Note: This is initial generation, so full file content is needed. Future fixes will use targeted edits only.**
"""

# ============================================================================
# AI æç¤ºè¯æ¨¡æ¿ - é˜¶æ®µ 0: ä»£ç é€»è¾‘éªŒè¯
# ============================================================================

logic_validation_prompt = """Now I need you to verify that the current implementation in experiment.py correctly implements the algorithm pseudocode.

# Algorithm Title
{title}

# Algorithm Pseudocode
{pseudocode}

# Current experiment.py Code
{current_code}

Please carefully compare the implementation with the pseudocode and check for:

1. **Algorithm Fidelity:**
   - Does the code correctly implement all steps from the pseudocode?
   - Are there any missing algorithmic components?
   - Are there any deviations from the pseudocode logic?

2. **Key Components:**
   - Are all variables, functions, and procedures from the pseudocode properly implemented?
   - Are the data structures and control flows consistent with the pseudocode?
   - Are the mathematical formulas and computations correctly translated?

3. **Critical Checks:**
   - If the pseudocode mentions specific conditions, are they properly handled?
   - If there are loops or iterations, do they match the pseudocode's structure?
   - Are the termination conditions and convergence criteria correctly implemented?

4. **Implementation Quality:**
   - Is the code using appropriate data types and operations?
   - Are there any placeholder or dummy implementations that should be real computations?
   - Does the implementation show meaningful learning progress (not constant values)?

Please respond with:
- "LOGIC_VALIDATION_PASSED" if the code correctly implements the pseudocode
- Otherwise, explain what needs to be fixed and make MINIMAL, TARGETED changes

**CRITICAL: If fixes are needed, DO NOT rewrite the entire file!**
- Identify the specific algorithmic discrepancies
- Make SURGICAL edits to fix only those parts
- Use search/replace for targeted modifications
- Preserve all correctly implemented sections
"""

# ============================================================================
# AI æç¤ºè¯æ¨¡æ¿ - é˜¶æ®µ 1: å®éªŒè¿­ä»£
# ============================================================================

coder_prompt = """Your goal is to implement the following idea: {title}.
The proposed experiment is as follows: {idea}.
You are given a total of up to {max_runs} runs to complete the necessary experiments. You do not need to use all {max_runs}.

First, plan the list of experiments you would like to run. For example, if you are sweeping over a specific hyperparameter, plan each value you would like to test for each run.

Note that we already provide the vanilla baseline results, so you do not need to re-run it.

For reference, the baseline results are as follows:

{baseline_results}

After you complete each change, we will run the command `python experiment.py --out_dir=run_i' where i is the run number and evaluate the results.
YOUR PROPOSED CHANGE MUST USE THIS COMMAND FORMAT, DO NOT ADD ADDITIONAL COMMAND LINE ARGS.
You can then implement the next thing on your list.

**CRITICAL INSTRUCTION FOR ALL MODIFICATIONS:**
- Use MINIMAL, TARGETED EDITS when modifying experiment.py
- DO NOT rewrite or output the entire file
- Use search/replace or focused edits for specific changes
- Only modify the sections that need to change for each run
- This conserves computational resources and reduces errors"""


# ============================================================================
# æ–°å¢ï¼šAI è‡ªä¸»åœºæ™¯è®¾è®¡é˜¶æ®µï¼ˆé˜¶æ®µ 0.5ï¼‰
# ============================================================================

def design_experiment_scenarios(idea, folder_name, coder, baseline_results=None, algorithm_tex_path=None):
    """
    AI è‡ªä¸»è®¾è®¡å®éªŒåœºæ™¯ - æ–°å¢é˜¶æ®µ 0.5
    åœ¨ä»£ç ç”Ÿæˆä¹‹åã€å®éªŒå¾ªç¯ä¹‹å‰æ‰§è¡Œ

    æœ¬å‡½æ•°å°†åŸºäº algorithm.texï¼ˆä¼˜å…ˆï¼‰è¯»å–ä¼ªä»£ç ï¼›è‹¥æä¾›äº† idea["Experiment"]
    åˆ™å¯ä½œä¸ºå‚è€ƒä½†ä¸æ˜¯ä¸»è¦æ¥æºã€‚
    """
    print("\n" + "="*80)
    print("ğŸ¯ é˜¶æ®µ 0.5: AI è‡ªä¸»åœºæ™¯è®¾è®¡")
    print("="*80 + "\n")
    
    # ä» algorithm.tex è¯»å–ä¼ªä»£ç ï¼ˆä¸ä½¿ç”¨ idea['Pseudocode']ï¼‰
    pseudocode_text = read_pseudocode_from_tex(folder_name=folder_name, tex_path=algorithm_tex_path) or ""
    
    # å‡†å¤‡ç®—æ³•ä¿¡æ¯
    algorithm_info = {
        "title": idea.get("Title", "Unknown Algorithm"),
        "description": idea.get("Experiment", ""),
        "pseudocode": pseudocode_text,
        "key_parameters": extract_available_parameters(folder_name)
    }
    
    # è®© AI è®¾è®¡å®éªŒåœºæ™¯
    scenario_prompt = generate_scenario_design_prompt(algorithm_info, baseline_results)
    print("ğŸ¤– AI æ­£åœ¨è®¾è®¡å®éªŒåœºæ™¯...")
    ai_response = coder.run(scenario_prompt)
    print(ai_response)
    
    # è§£æ AI è®¾è®¡çš„åœºæ™¯
    scenario_design = parse_ai_scenario_design(ai_response)
    
    if not scenario_design or "scenarios" not in scenario_design:
        print("âŒ AI æœªèƒ½æ­£ç¡®è®¾è®¡å®éªŒåœºæ™¯ï¼Œä½¿ç”¨é»˜è®¤åœºæ™¯")
        scenario_design = get_default_scenarios()
    
    scenarios = scenario_design["scenarios"]
    
    # æ˜¾ç¤ºè®¾è®¡çš„åœºæ™¯
    print(f"\nğŸ“‹ AI è®¾è®¡äº† {len(scenarios)} ä¸ªå®éªŒåœºæ™¯:")
    for i, scenario in enumerate(scenarios, 1):
        print(f"   {i}. {scenario.get('name', f'Scenario_{i}')}")
        print(f"      æè¿°: {scenario.get('description', 'No description')}")
        if 'parameters' in scenario:
            params_str = ' '.join([f"{k}={v}" for k, v in scenario['parameters'].items()])
            print(f"      å‚æ•°: {params_str}")
        if 'expected_insight' in scenario:
            print(f"      é¢„æœŸå‘ç°: {scenario['expected_insight']}")
        print()
    
    # ä¿å­˜åœºæ™¯è®¾è®¡åˆ°æ–‡ä»¶
    scenarios_file = osp.join(folder_name, "ai_designed_scenarios.json")
    with open(scenarios_file, 'w', encoding='utf-8') as f:
        json.dump(scenario_design, f, indent=2, ensure_ascii=False)
    
    print(f"ğŸ’¾ åœºæ™¯è®¾è®¡å·²ä¿å­˜åˆ°: {scenarios_file}")
    
    return scenario_design


def extract_available_parameters(folder_name):
    """
    ä» experiment.py ä¸­æå–å¯ç”¨çš„å‘½ä»¤è¡Œå‚æ•°
    """
    exp_file = osp.join(folder_name, "experiment.py")
    
    if not osp.exists(exp_file):
        return ["--learning_rate", "--num_iterations", "--dataset_size"]  # é»˜è®¤å‚æ•°
    
    try:
        with open(exp_file, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # æå– argparse å‚æ•°
        import re
        param_pattern = r'parser\.add_argument\([^)]*?--([a-zA-Z_]+)[^)]*?\)'
        params = re.findall(param_pattern, content)
        
        # æ’é™¤ out_dir å’Œ scenarioï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        excluded_params = ['out_dir', 'scenario']
        params = [f"--{p}" for p in params if p not in excluded_params]
        
        return params if params else ["--learning_rate", "--num_iterations", "--dataset_size"]
        
    except Exception as e:
        print(f"âš ï¸ æå–å‚æ•°æ—¶å‡ºé”™: {e}")
        return ["--learning_rate", "--num_iterations", "--dataset_size"]


# ...existing code...
def generate_scenario_design_prompt(algorithm_info, existing_results=None):
    """
    Generate an English prompt that instructs the AI to design experimental scenarios.
    Primary sources: algorithm title and pseudocode. The optional 'description' field
    (idea["Experiment"]) MAY be used as a reference for dataset preferences, constraints,
    or evaluation priorities, but DO NOT use it as the primary source of scenario design.

    The prompt asks for 3 scenarios covering heterogeneity, robustness, scalability,
    and hyperparameter sensitivity, and requests output in a strict JSON format.

    CHANGED: Require that EVERY scenario is explicitly a comparison between SFedAvg (SfedAvg)
    and FedAvg. Each scenario must include per-algorithm parameter settings and specify
    which metrics will be compared.
    """
    title = algorithm_info.get('title', 'Unknown Algorithm')
    pseudocode = algorithm_info.get('pseudocode', '').strip()
    description = algorithm_info.get('description', '').strip()
    key_params = algorithm_info.get('key_parameters', [])
    params_str = ', '.join(key_params) if key_params else 'no specific CLI parameters detected'

    prompt = f"""
You are an expert algorithm researcher. DESIGN experimental scenarios PRIMARILY based on the
Algorithm TITLE and its PSEUDOCODE below. You MAY consult the optional "Experiment description"
for helpful context (e.g., suggested datasets, constraints, or evaluation preferences), but
DO NOT rely on it as the main source of scenario design. The scenarios must be justified by
the TITLE and PSEUDOCODE.

Algorithm Title:
{title}

Algorithm Pseudocode:
{pseudocode if pseudocode else '<NO PSEUDOCODE PROVIDED>'}

Optional Experiment description (use only as reference, not primary source):
{description if description else '<NO EXPERIMENT DESCRIPTION PROVIDED>'}

Available command-line parameters (if any):
{params_str}

Your task:
Produce 3 well-motivated experimental scenarios that thoroughly evaluate the algorithm.
CRITICAL REQUIREMENT: EACH scenario MUST be an explicit COMPARISON between FedAvg and SFedAvg (also accept 'SfedAvg' spelling).
For every scenario you design, include separate parameter settings and experiment details for BOTH algorithms and specify the comparison metrics and expected relative behavior.

For each scenario include:
- A concise scenario name
- A one-sentence objective describing what aspect is tested
- Specific command-line parameter settings (use only the available parameters above; if none, propose sensible parameter names and values)
- A dataset description or data modification (e.g., Non-IID splits, label noise, varying dataset sizes)
- A per-algorithm configuration block specifying any algorithm-specific parameters (e.g., for SFedAvg: --subspace_dim, --local_steps; for FedAvg: --local_steps, --client_fraction). If a parameter is shared, indicate whether values are identical or different.
- The expected outcome / insight comparing SFedAvg vs FedAvg (which should perform better, in what metric, and why)
- Specific metrics and plots to produce (must enable direct FedAvg vs SFedAvg comparison)

Ensure the set of scenarios collectively covers:
- Heterogeneity: performance when data distributions differ across clients (Non-IID)
- Robustness: sensitivity to label noise, outliers, or corrupted data
- Scalability: behavior with increasing dataset size, number of clients, or model capacity
- Hyperparameter sensitivity: learning_rate, local_steps, subspace_dim, client_fraction, etc.
- Edge cases: extreme settings that reveal failure modes

Output format (MUST be valid JSON). Provide only JSON in your response (no extra explanation).

The JSON schema MUST include per-scenario a 'comparison' object describing both algorithms, for example:

{{
  "scenarios": [
    {{
      "name": "scenario_name",
      "description": "brief description of what this scenario tests",
      "parameters": {{
        "--dataset_size": 1000,
        "--learning_rate": 0.01,
        "--num_iterations": 100
      }},
      "dataset": "brief description of dataset / data modifications",
      "comparison": {{
        "FedAvg": {{
          "parameters": {{
            "--algorithm": "FedAvg",
            "--local_steps": 5,
            "--client_fraction": 0.5
          }}
        }},
        "SFedAvg": {{
          "parameters": {{
            "--algorithm": "SFedAvg",
            "--subspace_dim": 10,
            "--local_steps": 5,
            "--client_fraction": 0.5
          }}
        }},
        "notes": "Specify if any parameter differs between the two runs or if they use identical shared params"
      }},
      "expected_insight": "what you expect to observe when comparing SFedAvg vs FedAvg",
      "metrics": ["test_accuracy", "train_loss", "communication_rounds"],
      "plots": ["convergence_curve", "bar_comparison_final_accuracy"]
    }}
  ],
  "rationale": "Short explanation why these scenarios were chosen and how they complement each other"
}}

If existing_results are provided, you may incorporate them to suggest follow-up or targeted scenarios, but primary scenarios must still be justified from the TITLE and PSEUDOCODE.
"""
    if existing_results:
        prompt += "\n# Existing results (for reference):\n" + json.dumps(existing_results, indent=2) + "\n"

    return prompt
# ...existing code...

def parse_ai_scenario_design(ai_response):
    """
    è§£æ AI è¿”å›çš„åœºæ™¯è®¾è®¡
    """
    try:
    # å°è¯•ä» AI å“åº”ä¸­æå– JSON
        import re
        json_match = re.search(r'{.*}', ai_response, re.DOTALL)
        if json_match:
            json_str = json_match.group()
            scenario_data = json.loads(json_str)
            return scenario_data
        else:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ° JSONï¼Œå°è¯•è§£ææ–‡æœ¬æ ¼å¼
            return extract_scenarios_from_text(ai_response)
    except (json.JSONDecodeError, AttributeError) as e:
        print(f"âŒ Failed to parse AI scenario design as JSON: {e}")
        return extract_scenarios_from_text(ai_response)

def extract_scenarios_from_text(text):
    """
    ä»æ–‡æœ¬ä¸­æå–åœºæ™¯ä¿¡æ¯ï¼ˆå¤‡é€‰æ–¹æ¡ˆï¼‰
    """
    scenarios = []
    lines = text.split('\n')
    current_scenario = None

    for line in lines:
        line = line.strip()
        
        if line.startswith('Scenario:') or line.startswith('Scenario Name:'):
            if current_scenario:
                scenarios.append(current_scenario)
            current_scenario = {
                'name': line.split(':', 1)[1].strip(), 
                'parameters': {},
                'description': '',
                'expected_insight': ''
            }
        
        elif line.startswith('Description:') and current_scenario:
            current_scenario['description'] = line.split(':', 1)[1].strip()
        
        elif line.startswith('Parameters:') and current_scenario:
            param_part = line.split(':', 1)[1].strip()
            params = param_part.split()
            for param in params:
                if param.startswith('--'):
                    if '=' in param:
                        key, value = param.split('=', 1)
                        current_scenario['parameters'][key] = try_parse_value(value)
        
        elif line.startswith('Expected:') and current_scenario:
            current_scenario['expected_insight'] = line.split(':', 1)[1].strip()

    if current_scenario:
        scenarios.append(current_scenario)

    return {
        "scenarios": scenarios, 
        "rationale": "Extracted from text response - please check the formatting"
    }

def try_parse_value(value_str):
    """
    å°è¯•è§£æå‚æ•°å€¼
    """
    try:
        return float(value_str)
    except ValueError:
        try:
            return int(value_str)
        except ValueError:
            return value_str

def get_default_scenarios():
    """
    å¤‡ç”¨é»˜è®¤åœºæ™¯
    """
    return {
    "scenarios": [
    {
    "name": "baseline",
    "description": "Standard baseline configuration",
    "parameters": {
    "--learning_rate": 0.01,
    "--num_iterations": 100,
    "--dataset_size": 1000
    },
    "expected_insight": "Establish baseline performance for comparison"
    },
    {
    "name": "high_learning_rate",
    "description": "Test with higher learning rate",
    "parameters": {
    "--learning_rate": 0.1,
    "--num_iterations": 100,
    "--dataset_size": 1000
    },
    "expected_insight": "Observe convergence speed and stability with high learning rate"
    },
    {
    "name": "noisy_data",
    "description": "Test robustness to noisy data",
    "parameters": {
    "--learning_rate": 0.01,
    "--num_iterations": 100,
    "--noise_level": 0.5
    },
    "expected_insight": "Evaluate algorithm robustness under noisy conditions"
    }
    ],
    "rationale": "Default scenarios for basic algorithm testing covering learning rate sensitivity and robustness"
    }


def reset_and_prime_coder(coder, algorithm_info, stage_description):
    """
    æ¸…ç©º AI å†å²å¹¶é‡æ–°æ³¨å…¥å…³é”®ä¸Šä¸‹æ–‡ã€‚
    é˜²æ­¢ä¸Šä¸‹æ–‡è¶…é•¿ï¼ŒåŒæ—¶ç¡®ä¿ AI çŸ¥é“å½“å‰çš„ä»»åŠ¡ç›®æ ‡ã€‚
    
    Args:
        coder: Aider Coder å¯¹è±¡
        algorithm_info: å­—å…¸ï¼ŒåŒ…å« 'title' å’Œ 'pseudocode'
        stage_description: å½“å‰é˜¶æ®µçš„æè¿°
    
    Returns:
        str: ä¸Šä¸‹æ–‡é‡æ³¨æç¤ºè¯ï¼Œå¯é€‰æ‹©æ€§åœ°æ·»åŠ åˆ°ä¸‹ä¸€ä¸ª prompt å‰é¢
    """
    print(f"\nğŸ§¹ [ä¸Šä¸‹æ–‡ç®¡ç†] æ¸…ç†å†å²ï¼Œå‡†å¤‡è¿›å…¥é˜¶æ®µ: {stage_description}")
    
    # 1. æš´åŠ›æ¸…ç©º Aider çš„å¯¹è¯å†å²
    if hasattr(coder, 'done_messages'):
        cleared_count = len(coder.done_messages)
        coder.done_messages = []
        print(f"   âœ“ å·²æ¸…ç† {cleared_count} æ¡å†å²å¯¹è¯")
    
    if hasattr(coder, 'cur_messages'):
        coder.cur_messages = []
    
    # 2. æ„é€ "ä¸Šä¸‹æ–‡é‡æ³¨"æç¤ºè¯ (Re-priming Prompt)
    # æ³¨æ„ï¼šCoder ä¼šè‡ªåŠ¨è¯»å–å½“å‰æ–‡ä»¶çš„æœ€æ–°å†…å®¹ï¼Œæ‰€ä»¥ä¸éœ€è¦æŠŠä»£ç è´´è¿›å»
    prime_prompt = f"""I have cleared your chat history to free up context window space. 
We are currently working on the following project:

# Algorithm Title
{algorithm_info.get('title', 'Unknown')}

# Algorithm Pseudocode
{algorithm_info.get('pseudocode', 'See algorithm.tex for details')}

# Current Stage
{stage_description}

# Status
The file 'experiment.py' contains the current implementation.
The file 'notes.txt' contains the experiment logs.

Please wait for my next specific instruction for this stage.
"""
    
    print(f"   âœ“ ä¸Šä¸‹æ–‡é‡æ³¨æç¤ºè¯å·²å‡†å¤‡ï¼ˆ{len(prime_prompt)} å­—ç¬¦ï¼‰")
    
    return prime_prompt


# ============================================================================
# è¾…åŠ©å‡½æ•°ï¼šéªŒè¯ Python ä»£ç è¯­æ³•
# ============================================================================

def validate_python_syntax(file_path):
    """
    éªŒè¯ Python æ–‡ä»¶çš„è¯­æ³•æ˜¯å¦æ­£ç¡®
    
    å‚æ•°ï¼š
      file_path: Python æ–‡ä»¶è·¯å¾„
    
    è¿”å›ï¼š
      (is_valid, error_message): æ˜¯å¦æœ‰æ•ˆå’Œé”™è¯¯ä¿¡æ¯
    """
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            code = f.read()
        
        # å°è¯•ç¼–è¯‘ä»£ç 
        compile(code, file_path, 'exec')
        return True, ""
    except SyntaxError as e:
        error_msg = f"Syntax error in {file_path}:\n"
        error_msg += f"  Line {e.lineno}: {e.msg}\n"
        error_msg += f"  {e.text}"
        return False, error_msg
    except Exception as e:
        return False, f"Error validating {file_path}: {str(e)}"


def read_pseudocode_from_tex(folder_name=None, tex_path=None):
    """
    ä»æŒ‡å®šè·¯å¾„æˆ– folder_name/algorithm.tex è¯»å–ä¼ªä»£ç å†…å®¹ï¼Œæ‰¾ä¸åˆ°åˆ™è¿”å› None

    ä¼˜å…ˆä½¿ç”¨ tex_pathï¼ˆå¯ä»¥æ˜¯ç›¸å¯¹æˆ–ç»å¯¹è·¯å¾„ï¼‰ã€‚å¦‚æœæœªæä¾› tex_pathï¼Œåˆ™åœ¨
    folder_name/algorithm.tex ä¸­æŸ¥æ‰¾ã€‚
    """
    # å¦‚æœç›´æ¥ç»™å‡º tex_pathï¼Œåˆ™ä¼˜å…ˆä½¿ç”¨
    if tex_path:
        candidate = tex_path if osp.isabs(tex_path) else osp.abspath(tex_path)
        if osp.exists(candidate):
            try:
                with open(candidate, 'r', encoding='utf-8') as f:
                    content = f.read()
                return content if content.strip() else None
            except Exception as e:
                print(f"âš ï¸ è¯»å– {candidate} æ—¶å‡ºé”™: {e}")
                return None
        else:
            return None

    # å¦åˆ™ä» folder_name ä¸­æŸ¥æ‰¾ algorithm.tex
    if folder_name:
        tex_path_local = osp.join(folder_name, "algorithm.tex")
        if osp.exists(tex_path_local):
            try:
                with open(tex_path_local, 'r', encoding='utf-8') as f:
                    content = f.read()
                return content if content.strip() else None
            except Exception as e:
                print(f"âš ï¸ è¯»å– {tex_path_local} æ—¶å‡ºé”™: {e}")
                return None

    return None

# ============================================================================
# è¾…åŠ©å‡½æ•°ï¼šæµ‹è¯•è¿è¡Œ experiment.py
# ============================================================================

def test_run_experiment(folder_name, timeout=300):
    """
    æµ‹è¯•è¿è¡Œ experiment.pyï¼ŒéªŒè¯å…¶æ˜¯å¦èƒ½æ­£å¸¸æ‰§è¡Œ
    
    å·¥ä½œæµç¨‹ï¼š
      1. æ‰§è¡Œå‘½ä»¤ï¼špython experiment.py --out_dir=test_run
      2. æ£€æŸ¥æ˜¯å¦ç”Ÿæˆ test_run/final_info.json
      3. éªŒè¯ JSON æ ¼å¼æ˜¯å¦æ­£ç¡®
      4. æ¸…ç†æµ‹è¯•ç›®å½•
    
    å‚æ•°ï¼š
      folder_name: å®éªŒæ–‡ä»¶å¤¹è·¯å¾„
      timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤ 5 åˆ†é’Ÿ
    
    è¿”å›ï¼š
      (success, error_message): æ˜¯å¦æˆåŠŸå’Œé”™è¯¯ä¿¡æ¯
    """
    cwd = osp.abspath(folder_name)
    test_dir = osp.join(cwd, "test_run")
    
    # æ¸…ç†ä¹‹å‰çš„æµ‹è¯•ç›®å½•
    if osp.exists(test_dir):
        shutil.rmtree(test_dir)
    
    # æ‰§è¡Œæµ‹è¯•å‘½ä»¤
    command = ["python", "experiment.py", "--out_dir=test_run"]
    
    try:
        result = subprocess.run(
            command, 
            cwd=cwd, 
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE, 
            text=True, 
            timeout=timeout
        )
        
        # æ£€æŸ¥è¿”å›ç 
        if result.returncode != 0:
            error_msg = f"Test run failed with return code {result.returncode}\n"
            error_msg += f"STDERR:\n{result.stderr}"
            return False, error_msg
        
        # æ£€æŸ¥è¾“å‡ºæ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼ˆæ–°ç»“æ„ï¼šbaseline/final_info.jsonï¼‰
        result_file = osp.join(test_dir, "baseline", "final_info.json")
        if not osp.exists(result_file):
            error_msg = "Test run succeeded but did not generate baseline/final_info.json\n"
            error_msg += f"Expected file: {result_file}\n"
            if osp.exists(test_dir):
                contents = os.listdir(test_dir)
                error_msg += f"test_run/ contents: {contents}\n"
                
                # æ£€æŸ¥æ˜¯å¦æœ‰ baseline å­ç›®å½•
                baseline_dir = osp.join(test_dir, "baseline")
                if not osp.exists(baseline_dir):
                    error_msg += "\nâš ï¸ CRITICAL ERROR: baseline/ subdirectory was not created!\n"
                    error_msg += "\nThe experiment.py file MUST save results to:\n"
                    error_msg += "  {{out_dir}}/baseline/final_info.json\n"
                    error_msg += "\nImplementation:\n"
                    error_msg += "  baseline_dir = os.path.join(args.out_dir, 'baseline')\n"
                    error_msg += "  os.makedirs(baseline_dir, exist_ok=True)\n"
                    error_msg += "  with open(os.path.join(baseline_dir, 'final_info.json'), 'w') as f:\n"
                    error_msg += "      json.dump(results, f)\n"
                elif osp.exists(baseline_dir):
                    baseline_contents = os.listdir(baseline_dir)
                    error_msg += f"\nbaseline/ exists but final_info.json not found.\n"
                    error_msg += f"baseline/ contents: {baseline_contents}\n"
            else:
                error_msg += "test_run/ directory was not created"
            return False, error_msg
        
        # éªŒè¯ JSON æ ¼å¼
        try:
            with open(result_file, 'r') as f:
                results = json.load(f)
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ­£ç¡®çš„æ ¼å¼
            if not isinstance(results, dict):
                return False, "final_info.json must be a dictionary"
            
            # æ£€æŸ¥æ˜¯å¦è‡³å°‘æœ‰ä¸€ä¸ªæŒ‡æ ‡
            if len(results) == 0:
                return False, "final_info.json is empty"
            
            # æ£€æŸ¥æ¯ä¸ªæŒ‡æ ‡æ˜¯å¦æœ‰ means å’Œ stds
            for metric_name, metric_data in results.items():
                if not isinstance(metric_data, dict):
                    return False, f"Metric '{metric_name}' must be a dictionary"
                if "means" not in metric_data:
                    return False, f"Metric '{metric_name}' missing 'means' field"
                if "stds" not in metric_data:
                    return False, f"Metric '{metric_name}' missing 'stds' field"
                
                # æ£€æŸ¥ means æ˜¯å¦æ˜¯åˆ—è¡¨ä¸”éç©º
                means = metric_data["means"]
                if not isinstance(means, list) or len(means) == 0:
                    return False, f"Metric '{metric_name}': 'means' must be a non-empty list"
            
            # ================================================================
            # æ–°å¢éªŒè¯ï¼šæ£€æŸ¥ç»“æœçš„åˆç†æ€§
            # ================================================================
            validation_errors = []
            
            # éªŒè¯ 1ï¼šæ£€æŸ¥æ˜¯å¦æ‰€æœ‰å€¼éƒ½ç›¸åŒï¼ˆå¯èƒ½æ˜¯å¸¸æ•°ï¼Œè¡¨ç¤ºæ²¡æœ‰å­¦ä¹ ï¼‰
            for metric_name, metric_data in results.items():
                means = metric_data["means"]
                if len(means) > 5:
                    # æ£€æŸ¥å‰åå·®å¼‚
                    initial_values = means[:min(5, len(means)//4)]
                    final_values = means[-min(5, len(means)//4):]
                    
                    initial_mean = sum(initial_values) / len(initial_values)
                    final_mean = sum(final_values) / len(final_values)
                    
                    # å¦‚æœåˆå§‹å€¼å’Œæœ€ç»ˆå€¼å‡ ä¹ç›¸åŒï¼ˆå˜åŒ–å°äº1%ï¼‰ï¼Œå¯èƒ½æœ‰é—®é¢˜
                    if abs(initial_mean) > 1e-6:  # é¿å…é™¤é›¶
                        change_ratio = abs(final_mean - initial_mean) / abs(initial_mean)
                        if change_ratio < 0.01:  # å˜åŒ–å°äº1%
                            validation_errors.append(
                                f"âš ï¸ Metric '{metric_name}': Values barely changed "
                                f"(initial: {initial_mean:.4f}, final: {final_mean:.4f}). "
                                f"This may indicate the algorithm is not learning properly."
                            )
            
            # éªŒè¯ 2ï¼šæ‰«æä»£ç ä¸­çš„å¸¸è§é—®é¢˜æ¨¡å¼
            exp_file = osp.join(cwd, "experiment.py")
            if osp.exists(exp_file):
                with open(exp_file, 'r') as f:
                    code_content = f.read()
                
                # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨äº† placeholder æˆ–éšæœºå€¼
                problematic_patterns = [
                    ('np.random.randn', 'using random values for gradients/data'),
                    ('np.random.rand', 'using random values for gradients/data'),
                    ('TODO', 'unfinished implementation (TODO comments)'),
                    ('Placeholder', 'placeholder code that needs implementation'),
                    ('pass  # implement', 'empty implementation'),
                ]
                
                for pattern, description in problematic_patterns:
                    if pattern in code_content:
                        # æ£€æŸ¥æ˜¯å¦åœ¨æ³¨é‡Šä¸­ï¼ˆå…è®¸åœ¨æ³¨é‡Šä¸­å‡ºç°ï¼‰
                        lines_with_pattern = [line for line in code_content.split('\n') 
                                             if pattern in line and not line.strip().startswith('#')]
                        if lines_with_pattern:
                            validation_errors.append(
                                f"âš ï¸ Found '{pattern}' in code ({description}). "
                                f"Example: {lines_with_pattern[0][:80]}..."
                            )
            
            # å¦‚æœæœ‰éªŒè¯é”™è¯¯ï¼Œè¿”å›è­¦å‘Š
            if validation_errors:
                error_msg = "Test run completed but found potential issues:\n\n"
                error_msg += "\n".join(validation_errors)
                error_msg += "\n\n"
                error_msg += "Common causes:\n"
                error_msg += "1. Using random values instead of computing from real data\n"
                error_msg += "2. Not properly implementing the training loop\n"
                error_msg += "3. Incorrect gradient computation or model updates\n"
                error_msg += "4. Model not learning from data\n\n"
                error_msg += "Please review and fix these issues. The code should:\n"
                error_msg += "- Compute gradients from actual model predictions and data\n"
                error_msg += "- Update model parameters based on gradients\n"
                error_msg += "- Show learning progress (metrics should change over iterations)\n"
                return False, error_msg
            
            # æµ‹è¯•æˆåŠŸ
            return True, ""
            
        except json.JSONDecodeError as e:
            return False, f"Invalid JSON in final_info.json: {str(e)}"
        except Exception as e:
            return False, f"Error reading final_info.json: {str(e)}"
            
    except TimeoutExpired:
        return False, f"Test run timed out after {timeout} seconds"
    except Exception as e:
        return False, f"Error during test run: {str(e)}"
    finally:
        # æ¸…ç†æµ‹è¯•ç›®å½•
        if osp.exists(test_dir):
            shutil.rmtree(test_dir)


# ============================================================================
# é˜¶æ®µ 0: ä»£ç ç”Ÿæˆä¸éªŒè¯
# ============================================================================

def generate_code_from_pseudocode(idea, folder_name, coder, algorithm_tex_path=None):
    """
    æ ¹æ® algorithm.tex ä¸­çš„ä¼ªä»£ç ç”Ÿæˆåˆå§‹å®éªŒä»£ç å¹¶éªŒè¯å…¶å¯è¿è¡Œæ€§

    æ³¨æ„ï¼šæœ¬å‡½æ•°ä¸ä½¿ç”¨ idea["Pseudocode"]ï¼Œè€Œæ˜¯ä½¿ç”¨ä¼ å…¥çš„ algorithm_tex_path
    æˆ–è€…åœ¨ folder_name/algorithm.tex ä¸­æŸ¥æ‰¾ã€‚
    """
    
    print("\n" + "="*80)
    print("ğŸ”§ é˜¶æ®µ 0: ä»£ç ç”Ÿæˆä¸éªŒè¯ï¼ˆä½¿ç”¨ algorithm.tex ä½œä¸ºä¼ªä»£ç ï¼‰")
    print("="*80 + "\n")
    
    # ä»æŒ‡å®šè·¯å¾„æˆ– folder ä¸­è¯»å–ä¼ªä»£ç 
    pseudocode = read_pseudocode_from_tex(folder_name=folder_name, tex_path=algorithm_tex_path)
    if not pseudocode:
        print("âš ï¸ è­¦å‘Š: æœªæ‰¾åˆ° algorithm.texï¼ˆæ—¢æœªæä¾› --algorithm-texï¼Œä¹Ÿæœªåœ¨æ–‡ä»¶å¤¹ä¸­æ‰¾åˆ°ï¼‰ï¼Œè·³è¿‡ä»£ç ç”Ÿæˆé˜¶æ®µ")
        return True
    
    title = idea.get("Title", "Unknown")
    experiment_desc = idea.get("Experiment", "")
    
    # ç”Ÿæˆåˆå§‹æç¤ºè¯
    initial_prompt = code_generation_prompt.format(
        title=title,
        pseudocode=pseudocode,
        experiment_description=experiment_desc
    )
    
    print("ğŸ¤– AI æ­£åœ¨æ ¹æ® algorithm.tex ä¸­çš„ä¼ªä»£ç ç”Ÿæˆåˆå§‹ä»£ç ...")
    print(f"   ä¼ªä»£ç é•¿åº¦: {len(pseudocode)} å­—ç¬¦")
    print()
    
    # è¿­ä»£ç”Ÿæˆå’ŒéªŒè¯
    for iteration in range(MAX_CODE_GEN_ITERS):
        print(f"\n--- ä»£ç ç”Ÿæˆè¿­ä»£ {iteration + 1}/{MAX_CODE_GEN_ITERS} ---\n")
        
        # ====================================================================
        # Step 0.1: AI ç”Ÿæˆ/ä¿®æ”¹ä»£ç 
        # ====================================================================
        if iteration == 0:
            # é¦–æ¬¡ç”Ÿæˆ - ç›´æ¥è¦æ±‚ AI ç”Ÿæˆå®Œæ•´æ–‡ä»¶
            print("ğŸ¤– AI æ­£åœ¨ç”Ÿæˆ experiment.py å’Œ plot.py...")
            print("   æç¤ºï¼šç¬¬ä¸€æ¬¡ç”Ÿæˆï¼ŒAI å°†åˆ›å»ºå®Œæ•´çš„ä»£ç æ–‡ä»¶")
            coder_out = coder.run(initial_prompt)
        else:
            # ä¿®å¤ä»£ç  - ä½¿ç”¨ diff æ¨¡å¼è¿›è¡Œå¢é‡ä¿®æ”¹
            print("ğŸ¤– AI æ­£åœ¨æ ¹æ®é”™è¯¯åé¦ˆä¿®å¤ä»£ç ...")
            coder_out = coder.run(next_prompt)
        
        print(coder_out)
        
        # ====================================================================
        # Step 0.2: éªŒè¯ experiment.py è¯­æ³•
        # ====================================================================
        print("\nğŸ“ éªŒè¯ experiment.py è¯­æ³•...")
        exp_file = osp.join(folder_name, "experiment.py")
        
        if not osp.exists(exp_file):
            print("âŒ experiment.py æœªç”Ÿæˆ")
            next_prompt = """experiment.py file was not created. Please CREATE the experiment.py file now.

Remember the critical requirements:
1. Must accept --out_dir command-line argument
2. Must save results to {out_dir}/baseline/final_info.json
3. JSON format: {"metric": {"means": [...], "stds": [...]}}

NOTE: This is initial file creation, so full file content is needed.
"""
            continue
        
        syntax_valid, syntax_error = validate_python_syntax(exp_file)
        
        if not syntax_valid:
            print(f"âŒ è¯­æ³•é”™è¯¯:\n{syntax_error}")
            next_prompt = f"""The experiment.py file has syntax errors:

{syntax_error}

Please fix these syntax errors using TARGETED EDITS ONLY.

**CRITICAL: DO NOT output the entire file!**
- Identify the exact lines with errors
- Make minimal, surgical fixes to those specific lines
- Use the edit/replace functionality to modify only the problematic code sections

Focus on fixing ONLY the syntax errors mentioned above, nothing else.
"""
            continue
        
        print("âœ… experiment.py è¯­æ³•æ­£ç¡®")
        
        # ====================================================================
        # Step 0.3: éªŒè¯ plot.py è¯­æ³•ï¼ˆå¯é€‰ï¼‰
        # ====================================================================
        print("\nğŸ“ éªŒè¯ plot.py è¯­æ³•...")
        plot_file = osp.join(folder_name, "plot.py")
        
        if osp.exists(plot_file):
            syntax_valid, syntax_error = validate_python_syntax(plot_file)
            if not syntax_valid:
                print(f"âš ï¸ plot.py æœ‰è¯­æ³•é”™è¯¯:\n{syntax_error}")
                print("   (å°†åœ¨åç»­é˜¶æ®µä¿®å¤)")
            else:
                print("âœ… plot.py è¯­æ³•æ­£ç¡®")
        else:
            print("âš ï¸ plot.py æœªç”Ÿæˆï¼ˆå°†åœ¨åç»­é˜¶æ®µç”Ÿæˆï¼‰")
        
        # ====================================================================
        # Step 0.4: æµ‹è¯•è¿è¡Œ experiment.py
        # ====================================================================
        print("\nâš™ï¸ æµ‹è¯•è¿è¡Œ experiment.py...")
        print("   æ‰§è¡Œ: python experiment.py --out_dir=test_run")
        
        test_success, test_error = test_run_experiment(folder_name, timeout=300)
        
        if not test_success:
            print(f"âŒ æµ‹è¯•è¿è¡Œå¤±è´¥:\n{test_error}")
            
            # æˆªæ–­è¿‡é•¿çš„é”™è¯¯ä¿¡æ¯
            if len(test_error) > MAX_STDERR_OUTPUT:
                test_error = "..." + test_error[-MAX_STDERR_OUTPUT:]
            
            next_prompt = f"""The experiment.py file has runtime errors:

{test_error}

Please fix these errors using MINIMAL, TARGETED EDITS.

**CRITICAL: DO NOT rewrite or output the entire file!**

Common issues to check and fix:
1. Missing import statements â†’ Add only the missing imports at the top
2. Undefined variables or functions â†’ Fix the specific line/function
3. Incorrect data types or shapes â†’ Adjust the problematic operation
4. File I/O errors â†’ Fix the file handling code
5. Missing --out_dir argument handling â†’ Add argument parsing if missing
6. Incorrect final_info.json format â†’ Fix the save logic

**Instructions:**
- Analyze the error traceback to identify the EXACT problematic lines
- Make SURGICAL fixes to only those specific locations
- Use search/replace or edit commands for targeted modifications
- DO NOT output code that's already working correctly
"""
            continue

        print("âœ… æµ‹è¯•è¿è¡ŒæˆåŠŸï¼")
        print("âœ… final_info.json æ ¼å¼æ­£ç¡®")
        
        # ====================================================================
        # Step 0.5: æµ‹è¯•è¿è¡Œ plot.py
        # ====================================================================
        print("\nğŸ“Š æµ‹è¯• plot.py çš„å¯è§†åŒ–åŠŸèƒ½...")
        plot_test_success, plot_test_error = test_plot_script(folder_name)

        if not plot_test_success:
            print(f"âŒ plot.py æµ‹è¯•å¤±è´¥:\n{plot_test_error}")
            next_prompt = f"""The plot.py file has issues:

        {plot_test_error}

        Please fix plot.py using MINIMAL, TARGETED EDITS.

        **CRITICAL: DO NOT rewrite the entire file!**

        Required fixes:
        1. Accept --out_dir command-line argument (add if missing)
        2. Use matplotlib for visualizations (fix import/usage if broken)
        3. Save plots to the specified output directory as PNG files
        4. Fix any runtime errors

        **Instructions:**
        - Identify the EXACT issue from the error message
        - Make SURGICAL fixes to only the problematic code
        - Use search/replace for targeted modifications
        - DO NOT output working code sections
        """
            continue
        else:
            print(f"âœ… plot.py æµ‹è¯•æˆåŠŸ: {plot_test_error}")
        
        # ====================================================================
        # Step 0.6: ä»£ç é€»è¾‘éªŒè¯ï¼ˆæ–°å¢æ­¥éª¤ï¼‰
        # ====================================================================
        print("\nğŸ” éªŒè¯ä»£ç é€»è¾‘æ˜¯å¦ç¬¦åˆä¼ªä»£ç ...")
        
        # è¯»å–å½“å‰ç”Ÿæˆçš„ä»£ç 
        with open(exp_file, 'r', encoding='utf-8') as f:
            current_code = f.read()
        
        # ç”Ÿæˆé€»è¾‘éªŒè¯æç¤ºè¯
        logic_prompt = logic_validation_prompt.format(
            title=title,
            pseudocode=pseudocode,
            current_code=current_code
        )
        
        print("ğŸ¤– AI æ­£åœ¨éªŒè¯ä»£ç é€»è¾‘...")
        logic_response = coder.run(logic_prompt)
        print(logic_response)
        
        # æ£€æŸ¥ AI æ˜¯å¦ç¡®è®¤ä»£ç é€»è¾‘æ­£ç¡®
        if "LOGIC_VALIDATION_PASSED" in logic_response:
            print("âœ… ä»£ç é€»è¾‘éªŒè¯é€šè¿‡ï¼")
            print()
            print("="*80)
            print("ğŸ‰ ä»£ç ç”Ÿæˆé˜¶æ®µå®Œæˆï¼")
            print("="*80)
            print()
            return True
        else:
            print("âŒ ä»£ç é€»è¾‘éªŒè¯å¤±è´¥ï¼ŒAI å‘ç°ä¸ä¼ªä»£ç ä¸ç¬¦ä¹‹å¤„")
            print("ğŸ”„ ç»§ç»­è¿­ä»£ä¿®å¤...")
            
            # ç”Ÿæˆä¸‹ä¸€è½®ä¿®å¤çš„æç¤ºè¯
            next_prompt = f"""The code logic validation failed. The AI identified discrepancies between the implementation and the pseudocode.

# Algorithm Pseudocode
{pseudocode}

Key issues identified:
- The implementation does not correctly follow the pseudocode logic
- Some algorithmic steps may be missing or incorrectly implemented
- Mathematical formulas or procedures may not match

**CRITICAL: Make TARGETED FIXES ONLY - DO NOT rewrite the entire file!**

**Instructions:**
1. Identify the SPECIFIC algorithmic components that don't match the pseudocode
2. Make SURGICAL edits to fix only those components
3. Use search/replace to modify specific functions/sections
4. Preserve all correctly implemented parts

Focus on the algorithmic logic discrepancies, not cosmetic changes.
"""
            continue
    
    # ========================================================================
    # è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°ä»æœªæˆåŠŸ
    # ========================================================================
    print("\n" + "="*80)
    print(f"âŒ ä»£ç ç”Ÿæˆå¤±è´¥ï¼šè¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•° ({MAX_CODE_GEN_ITERS})")
    print("="*80)
    print()
    print("å»ºè®®:")
    print("  1. æ£€æŸ¥ algorithm.tex ä¸­çš„ä¼ªä»£ç æ˜¯å¦æ¸…æ™°å®Œæ•´")
    print("  2. æ£€æŸ¥å®éªŒæè¿°æ˜¯å¦æä¾›äº†è¶³å¤Ÿçš„ä¸Šä¸‹æ–‡")
    print("  3. æ‰‹åŠ¨æ£€æŸ¥ç”Ÿæˆçš„ experiment.py å¹¶ä¿®å¤")
    print()
    
    return False

# ============================================================================
# è¾…åŠ©å‡½æ•°ï¼šæµ‹è¯•è¿è¡Œplot.py
# ============================================================================
def test_plot_script(folder_name):
    """
    æµ‹è¯• plot.py æ˜¯å¦èƒ½æ­£ç¡®å¤„ç† --out_dir å‚æ•°
    """
    cwd = osp.abspath(folder_name)
    test_plots_dir = osp.join(cwd, "test_plots")
    
    # æ¸…ç†ä¹‹å‰çš„æµ‹è¯•ç›®å½•
    if osp.exists(test_plots_dir):
        shutil.rmtree(test_plots_dir)
    
    # æ‰§è¡Œæµ‹è¯•å‘½ä»¤
    command = ["python", "plot.py", f"--out_dir={test_plots_dir}"]
    
    try:
        result = subprocess.run(
            command, 
            cwd=cwd, 
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE, 
            text=True, 
            timeout=120  # 2åˆ†é’Ÿè¶…æ—¶
        )
        
        # æ£€æŸ¥è¿”å›ç 
        if result.returncode != 0:
            error_msg = f"Plot test failed with return code {result.returncode}\n"
            error_msg += f"STDERR:\n{result.stderr}"
            return False, error_msg
        
        # æ£€æŸ¥æ˜¯å¦åˆ›å»ºäº†è¾“å‡ºç›®å½•å’Œå›¾è¡¨æ–‡ä»¶
        if not osp.exists(test_plots_dir):
            return False, "Plot test failed: output directory was not created"
        
        plot_files = [f for f in os.listdir(test_plots_dir) if f.endswith('.png')]
        if not plot_files:
            return False, "Plot test failed: no PNG files were generated"
        
        return True, f"Successfully generated plot files: {plot_files}"
        
    except TimeoutExpired:
        return False, f"Plot test timed out after 120 seconds"
    except Exception as e:
        return False, f"Error during plot test: {str(e)}"
    finally:
        # æ¸…ç†æµ‹è¯•ç›®å½•
        if osp.exists(test_plots_dir):
            shutil.rmtree(test_plots_dir)


# ============================================================================
# è¾…åŠ©å‡½æ•°ï¼šæ‰§è¡Œå•æ¬¡å®éªŒ
# ============================================================================

def run_experiment(folder_name, run_num, timeout=7200):
    """
    æ‰§è¡Œå•æ¬¡å®éªŒè¿è¡Œ
    
    å·¥ä½œæµç¨‹ï¼š
      1. ä¿å­˜å½“å‰ experiment.py çš„å¿«ç…§ï¼ˆç”¨äºè¿½æº¯ï¼‰
      2. æ‰§è¡Œå‘½ä»¤ï¼špython experiment.py --out_dir=run_{run_num}
      3. æ£€æŸ¥æ‰§è¡Œç»“æœï¼š
         - æˆåŠŸï¼šè¯»å– final_info.jsonï¼Œè¿”å›ç»“æœæ•°æ®
         - å¤±è´¥ï¼šæ¸…ç†å¤±è´¥çš„è¿è¡Œï¼Œè¿”å›é”™è¯¯ä¿¡æ¯
      4. ç”Ÿæˆåé¦ˆæç¤ºè¯ç»™ AI
    
    å‚æ•°ï¼š
      folder_name: å®éªŒæ–‡ä»¶å¤¹è·¯å¾„
      run_num: è¿è¡Œç¼–å·ï¼ˆ1, 2, 3, ...ï¼‰
      timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤ 2 å°æ—¶
    
    è¿”å›ï¼š
      (return_code, next_prompt): è¿”å›ç å’Œä¸‹ä¸€æ­¥çš„ AI æç¤ºè¯
    """
    cwd = osp.abspath(folder_name)
    
    # ========================================================================
    # Step 1: æ„é€ å¹¶æ‰§è¡Œå‘½ä»¤
    # ========================================================================
    # å›ºå®šæ ¼å¼ï¼špython experiment.py --out_dir=run_1
    # experiment.py å¿…é¡»æ”¯æŒ --out_dir å‚æ•°ï¼Œå¹¶ä¼šåˆ›å»º run_1/ ç›®å½•
    command = [
        "python",
        "experiment.py",
        f"--out_dir=run_{run_num}",
    ]
    
    try:
        # æ‰§è¡Œå‘½ä»¤ï¼Œæ•è·æ ‡å‡†é”™è¯¯è¾“å‡º
        result = subprocess.run(
            command, cwd=cwd, stderr=subprocess.PIPE, text=True, timeout=timeout
        )

        # æ‰“å°é”™è¯¯è¾“å‡ºï¼ˆå¦‚æœæœ‰ï¼‰
        if result.stderr:
            print(result.stderr, file=sys.stderr)

        # ====================================================================
        # Step 3: å¤„ç†æ‰§è¡Œç»“æœ
        # ====================================================================
        
        if result.returncode != 0:
            # ----------------------------------------------------------------
            # æƒ…å†µ A: æ‰§è¡Œå¤±è´¥
            # ----------------------------------------------------------------
            print(f"Run {run_num} failed with return code {result.returncode}")
            
            # æ¸…ç†å¤±è´¥çš„è¿è¡Œç›®å½•ï¼ˆé¿å…è„æ•°æ®ï¼‰
            if osp.exists(osp.join(cwd, f"run_{run_num}")):
                shutil.rmtree(osp.join(cwd, f"run_{run_num}"))
            
            print(f"Run failed with the following error {result.stderr}")
            
            # æˆªæ–­è¿‡é•¿çš„é”™è¯¯ä¿¡æ¯
            stderr_output = result.stderr
            if len(stderr_output) > MAX_STDERR_OUTPUT:
                stderr_output = "..." + stderr_output[-MAX_STDERR_OUTPUT:]
            
            # ç”Ÿæˆé”™è¯¯åé¦ˆæç¤ºè¯ï¼ˆAI ä¼šæ ¹æ®é”™è¯¯ä¿®å¤ä»£ç ï¼‰
            next_prompt = f"Run failed with the following error {stderr_output}"
            
        else:
            # ----------------------------------------------------------------
            # æƒ…å†µ B: æ‰§è¡ŒæˆåŠŸ
            # ----------------------------------------------------------------
            
            # ================================================================
            # Step 3.1: ä¿å­˜ä»£ç å¿«ç…§åˆ° run_N/ ç›®å½•
            # ================================================================
            # å°†å½“å‰çš„ experiment.py å’Œ plot.py å¤åˆ¶åˆ° run_N/ ç›®å½•
            # ä½œç”¨ï¼šè¿½æº¯æ¯æ¬¡è¿è¡Œä½¿ç”¨çš„ä»£ç ç‰ˆæœ¬ï¼ˆå› ä¸º AI ä¼šä¸æ–­ä¿®æ”¹è¿™äº›æ–‡ä»¶ï¼‰
            run_dir = osp.join(cwd, f"run_{run_num}")
            
            shutil.copy(
                osp.join(cwd, "experiment.py"),
                osp.join(run_dir, "experiment.py"),
            )
            
            # å¦‚æœ plot.py å­˜åœ¨ï¼Œä¹Ÿå¤åˆ¶ä¸€ä»½
            plot_file = osp.join(cwd, "plot.py")
            if osp.exists(plot_file):
                shutil.copy(plot_file, osp.join(run_dir, "plot.py"))
            
            # ================================================================
            # Step 3.2: è¯»å–å®éªŒç»“æœ
            # ================================================================
            # è¯»å–å®éªŒç»“æœæ–‡ä»¶ run_{run_num}/baseline/final_info.json
            # æ ¼å¼è¦æ±‚ï¼š{"metric": {"means": [...], "stds": [...]}}
            result_file = osp.join(run_dir, "baseline", "final_info.json")
            if not osp.exists(result_file):
                # å…¼å®¹æ€§ï¼šå°è¯•æ—§ç»“æ„
                result_file = osp.join(run_dir, "final_info.json")
            
            with open(result_file, "r") as f:
                results = json.load(f)
            
            # æå– means å€¼ï¼ˆä¸»è¦ç»“æœï¼‰
            results = {k: v["means"] for k, v in results.items()}

            # ç”ŸæˆæˆåŠŸåé¦ˆæç¤ºè¯ï¼ˆåŒ…å«ç»“æœæ•°æ®å’Œä¸‹ä¸€æ­¥æŒ‡ç¤ºï¼‰
            next_prompt = f"""Run {run_num} completed. Here are the results:
{results}

Decide if you need to re-plan your experiments given the result (you often will not need to).

Someone else will be using `notes.txt` to perform a writeup on this in the future.
Please include *all* relevant information for the writeup on Run {run_num}, including an experiment description and the run number. Be as verbose as necessary.

Then, implement the next thing on your list.
We will then run the command `python experiment.py --out_dir=run_{run_num + 1}'.
YOUR PROPOSED CHANGE MUST USE THIS COMMAND FORMAT, DO NOT ADD ADDITIONAL COMMAND LINE ARGS.
If you are finished with experiments, respond with 'ALL_COMPLETED'."""
        
        return result.returncode, next_prompt
        
    except TimeoutExpired:
        # ====================================================================
        # æƒ…å†µ C: æ‰§è¡Œè¶…æ—¶
        # ====================================================================
        print(f"Run {run_num} timed out after {timeout} seconds")
        
        # æ¸…ç†è¶…æ—¶çš„è¿è¡Œç›®å½•
        if osp.exists(osp.join(cwd, f"run_{run_num}")):
            shutil.rmtree(osp.join(cwd, f"run_{run_num}"))
        
        # ç”Ÿæˆè¶…æ—¶åé¦ˆæç¤ºè¯ï¼ˆAI ä¼šä¼˜åŒ–ä»£ç ä»¥å‡å°‘è¿è¡Œæ—¶é—´ï¼‰
        next_prompt = f"Run timed out after {timeout} seconds"
        return 1, next_prompt


# ============================================================================
# è¾…åŠ©å‡½æ•°ï¼šæ‰§è¡Œå¯è§†åŒ–ç”Ÿæˆ
# ============================================================================


## 2. ä¿®æ”¹å¯è§†åŒ–æ‰§è¡Œå‡½æ•°


# ============================================================================
# è¾…åŠ©å‡½æ•°ï¼šæ‰§è¡Œå¯è§†åŒ–ç”Ÿæˆ
# ============================================================================

def run_plotting(folder_name, timeout=600):
    """
    æ‰§è¡Œå¯è§†åŒ–è„šæœ¬ç”Ÿæˆå›¾è¡¨
    
    å·¥ä½œæµç¨‹ï¼š
      1. æ‰§è¡Œå‘½ä»¤ï¼špython plot.py --out_dir=plots
      2. plot.py åº”è¯¥è¯»å– run_1/, run_2/ ç­‰ç›®å½•çš„ç»“æœ
      3. ç”Ÿæˆ PNG å›¾è¡¨æ–‡ä»¶åˆ° plots/ ç›®å½•
    
    å‚æ•°ï¼š
      folder_name: å®éªŒæ–‡ä»¶å¤¹è·¯å¾„
      timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤ 10 åˆ†é’Ÿ
    
    è¿”å›ï¼š
      (return_code, next_prompt): è¿”å›ç å’Œåé¦ˆæç¤ºè¯
    """
    cwd = osp.abspath(folder_name)
    
    # åˆ›å»º plots ç›®å½•
    plots_dir = osp.join(cwd, "plots")
    if osp.exists(plots_dir):
        shutil.rmtree(plots_dir)
    os.makedirs(plots_dir)
    
    # æ£€æŸ¥ plot.py æ˜¯å¦å­˜åœ¨
    plot_file = osp.join(cwd, "plot.py")
    if not osp.exists(plot_file):
        return False, "plot.py æ–‡ä»¶ä¸å­˜åœ¨"
    
    # æ‰§è¡Œç»˜å›¾å‘½ä»¤
    command = ["python", "plot.py", f"--out_dir={plots_dir}"]
    
    try:
        print("ğŸ¨ ç”Ÿæˆå¤šåœºæ™¯å¯è§†åŒ–ç»“æœ...")
        result = subprocess.run(
            command, 
            cwd=cwd, 
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE, 
            text=True, 
            timeout=600  # 10åˆ†é’Ÿè¶…æ—¶
        )
        
        if result.returncode == 0:
            # æ£€æŸ¥ç”Ÿæˆçš„å›¾è¡¨æ–‡ä»¶
            plot_files = [f for f in os.listdir(plots_dir) if f.endswith('.png')]
            if plot_files:
                print(f"âœ… æˆåŠŸç”Ÿæˆ {len(plot_files)} ä¸ªå›¾è¡¨æ–‡ä»¶:")
                for plot_file in plot_files:
                    print(f"   ğŸ“Š {plot_file}")
                return True, f"ç”Ÿæˆ {len(plot_files)} ä¸ªå¯è§†åŒ–å›¾è¡¨"
            else:
                return False, "ç»˜å›¾å®Œæˆä½†æœªç”Ÿæˆå›¾è¡¨æ–‡ä»¶"
        else:
            error_msg = result.stderr[:500] if result.stderr else "Unknown error"
            return False, f"ç»˜å›¾å¤±è´¥: {error_msg}"
            
    except TimeoutExpired:
        return False, "ç»˜å›¾è¶…æ—¶"
    except Exception as e:
        return False, f"ç»˜å›¾å¼‚å¸¸: {str(e)}"


# ============================================================================
# ä¸»å‡½æ•°ï¼šæ‰§è¡Œå®Œæ•´çš„å®éªŒæµç¨‹
# ============================================================================

def perform_experiments(idea, folder_name, coder, baseline_results, algorithm_tex_path=None) -> bool:
    """
    æ‰§è¡Œå®Œæ•´çš„ AI é©±åŠ¨å®éªŒæµç¨‹

    æ–°å¢å‚æ•°:
      algorithm_tex_path: å¯é€‰ï¼ŒæŒ‡å‘ algorithm.tex çš„è·¯å¾„ï¼ˆç›¸å¯¹æˆ–ç»å¯¹ï¼‰ã€‚å¦‚æœæä¾›ï¼Œ
                          æ‰€æœ‰ä¼ªä»£ç è¯»å–å°†ä¼˜å…ˆä½¿ç”¨è¯¥è·¯å¾„ã€‚
    """
    # ========================================================================
    # é˜¶æ®µ 0: ä»£ç ç”Ÿæˆä¸éªŒè¯ï¼ˆä½¿ç”¨ algorithm.texï¼‰
    # ========================================================================
    tex_pseudocode = read_pseudocode_from_tex(folder_name=folder_name, tex_path=algorithm_tex_path)
    
    # å‡†å¤‡ç®—æ³•ä¿¡æ¯å¯¹è±¡ï¼ˆç”¨äºä¸Šä¸‹æ–‡é‡æ³¨ï¼‰
    algo_info = {
        "title": idea.get("Title", "Algorithm Experiment"),
        "pseudocode": tex_pseudocode if tex_pseudocode else "Refer to algorithm.tex or idea JSON"
    }
    
    if tex_pseudocode:
        print("\n" + "="*80)
        print("ğŸ“‹ æ£€æµ‹åˆ° algorithm.texï¼Œå¯åŠ¨ä»£ç ç”Ÿæˆé˜¶æ®µï¼ˆä¸ä½¿ç”¨ idea['Pseudocode']ï¼‰")
        print("="*80 + "\n")
        
        success = generate_code_from_pseudocode(idea, folder_name, coder, algorithm_tex_path=algorithm_tex_path)
        
        if not success:
            print("\nâŒ ä»£ç ç”Ÿæˆé˜¶æ®µå¤±è´¥ï¼Œæ— æ³•ç»§ç»­åç»­å®éªŒ")
            return False
        
        print("âœ… ä»£ç ç”Ÿæˆé˜¶æ®µå®Œæˆï¼Œè¿›å…¥å®éªŒè¿­ä»£é˜¶æ®µ\n")
        
        # [ä¸Šä¸‹æ–‡ç®¡ç†] ä»£ç ç”Ÿæˆå®Œæˆåï¼Œæ¸…ç†å†å²ï¼Œå‡†å¤‡è¿›å…¥åœºæ™¯è®¾è®¡é˜¶æ®µ
        # æ­¤æ—¶ experiment.py å·²ç»ç”Ÿæˆå¥½äº†ï¼ŒAI åªè¦è¯»æ–‡ä»¶å°±è¡Œï¼Œä¸éœ€è¦çŸ¥é“ç”Ÿæˆçš„æ›²æŠ˜è¿‡ç¨‹
        # reset_and_prime_coder(coder, algo_info, "Phase 0.5: AI Scenario Design")  # ä¸´æ—¶æ³¨é‡Šæ‰ç”¨äºæµ‹è¯•
    else:
        print("\n" + "="*80)
        print("â„¹ï¸  æœªæ£€æµ‹åˆ° algorithm.texï¼Œè·³è¿‡ä»£ç ç”Ÿæˆé˜¶æ®µï¼ˆä¸ä½¿ç”¨ idea['Pseudocode']ï¼‰")
        print("="*80 + "\n")

    # ========================================================================
    # é˜¶æ®µ 0.5: AI è‡ªä¸»åœºæ™¯è®¾è®¡ï¼ˆæ–°å¢ï¼‰
    # ========================================================================
    scenario_design = design_experiment_scenarios(idea, folder_name, coder, baseline_results, algorithm_tex_path=algorithm_tex_path)
    ai_scenarios = scenario_design.get("scenarios", [])
    
    if not ai_scenarios:
        print("âŒ æœªèƒ½è·å¾—æœ‰æ•ˆçš„ AI è®¾è®¡åœºæ™¯ï¼Œæ— æ³•ç»§ç»­")
        return False
    
    # [ä¸Šä¸‹æ–‡ç®¡ç†] åœºæ™¯è®¾è®¡å®Œæˆåï¼Œæ¸…ç†å†å²ï¼Œå‡†å¤‡è¿›å…¥æ­£å¼æ‰§è¡Œé˜¶æ®µ
    # æ­¤æ—¶æˆ‘ä»¬æœ‰äº† scenarios åˆ—è¡¨ï¼Œä¸éœ€è¦ AI è®°å¾—å®ƒæ˜¯æ€ä¹ˆæƒ³å‡ºè¿™äº›åœºæ™¯çš„
    # reset_and_prime_coder(coder, algo_info, "Phase 1: Experiment Execution")  # ä¸´æ—¶æ³¨é‡Šæ‰ç”¨äºæµ‹è¯•

    # ========================================================================
    # é˜¶æ®µ 1: è¿­ä»£å®éªŒå¾ªç¯ï¼ˆä¿®æ”¹ä¸ºæ”¯æŒ AI è®¾è®¡åœºæ™¯ï¼‰
    # ========================================================================
    print("\n" + "="*80)
    print("ğŸ”¬ é˜¶æ®µ 1: è¿­ä»£å®éªŒå¾ªç¯ï¼ˆé›†æˆ AI è®¾è®¡åœºæ™¯ï¼‰")
    print("="*80 + "\n")
    
    # æ‰§è¡Œ AI è®¾è®¡çš„åœºæ™¯ï¼ˆæ¯ä¸ªåœºæ™¯å¤±è´¥æ—¶è‡ªåŠ¨è¿­ä»£ä¿®å¤ï¼‰
    # ä¼ å…¥ algo_info ä»¥ä¾¿åœ¨åœºæ™¯åˆ‡æ¢æ—¶ä½¿ç”¨
    scenario_results = execute_ai_designed_scenarios(folder_name, ai_scenarios, coder, algo_info=algo_info)
    
    # åˆ†ææ‰§è¡Œç»“æœ
    successful_scenarios = [s for s in scenario_results if s["status"] == "success"]
    failed_scenarios = [s for s in scenario_results if s["status"] != "success"]
    
    print(f"\nğŸ“Š AI è®¾è®¡åœºæ™¯æ‰§è¡Œæ€»ç»“:")
    print(f"   âœ… æˆåŠŸ: {len(successful_scenarios)}/{len(ai_scenarios)}")
    print(f"   âŒ å¤±è´¥: {len(failed_scenarios)}/{len(ai_scenarios)}")
    
    if successful_scenarios:
        print("\nâœ… æˆåŠŸæ‰§è¡Œçš„ AI è®¾è®¡åœºæ™¯:")
        for scenario in successful_scenarios:
            print(f"   â€¢ {scenario['name']}")
    
    if failed_scenarios:
        print("\nâŒ å¤±è´¥çš„ AI è®¾è®¡åœºæ™¯:")
        for scenario in failed_scenarios:
            print(f"   â€¢ {scenario['name']}: {scenario.get('error', 'Unknown error')}")
    
    # ä¿å­˜åœºæ™¯æ‰§è¡Œç»“æœ
    results_file = osp.join(folder_name, "scenario_execution_results.json")
    with open(results_file, 'w', encoding='utf-8') as f:
        json.dump({
            "scenario_design": scenario_design,
            "execution_results": scenario_results
        }, f, indent=2, ensure_ascii=False)
    
    print(f"ğŸ’¾ åœºæ™¯æ‰§è¡Œç»“æœå·²ä¿å­˜åˆ°: {results_file}")
    
    # ========================================================================
    # åŸæœ‰çš„è¿­ä»£å®éªŒå¾ªç¯ï¼ˆä¿æŒä¸å˜ï¼Œä½œä¸ºå¤‡é€‰ï¼‰
    # ========================================================================
    if not successful_scenarios:
        print("\nâš ï¸  AI è®¾è®¡åœºæ™¯å…¨éƒ¨å¤±è´¥ï¼Œå›é€€åˆ°åŸæœ‰å®éªŒå¾ªç¯")
        return perform_original_experiment_loop(idea, folder_name, coder, baseline_results)
    
    # [ä¸Šä¸‹æ–‡ç®¡ç†] åœºæ™¯æ‰§è¡Œå®Œæˆåï¼Œæ¸…ç†å†å²ï¼Œå‡†å¤‡è¿›å…¥å¯è§†åŒ–é˜¶æ®µ
    # æ­¤æ—¶å·²æœ‰æ‰€æœ‰å®éªŒç»“æœï¼Œä¸éœ€è¦ AI è®°å¾—å®éªŒæ‰§è¡Œçš„æ›²æŠ˜è¿‡ç¨‹
    # reset_and_prime_coder(coder, algo_info, "Phase 2: Visualization Generation")  # ä¸´æ—¶æ³¨é‡Šæ‰ç”¨äºæµ‹è¯•
    
    # ========================================================================
    # é˜¶æ®µ 2: å¯è§†åŒ–ç”Ÿæˆï¼ˆå¢å¼ºç‰ˆï¼Œæ”¯æŒå¤šåœºæ™¯ï¼‰
    # ========================================================================
    print("\n" + "="*80)
    print("ğŸ“Š é˜¶æ®µ 2: å¤šåœºæ™¯å¯è§†åŒ–ç”Ÿæˆ")
    print("="*80 + "\n")
    
    plot_success, plot_message = generate_comprehensive_visualization(
        folder_name, 
        coder=coder, 
        scenario_results=scenario_results
    )
    
    if plot_success:
        print(f"âœ… {plot_message}")
    else:
        print(f"âŒ {plot_message}")
        
        # è®© AI ä¿®å¤å¯è§†åŒ–ä»£ç 
        print("ğŸ¤– AI æ­£åœ¨ä¿®å¤å¯è§†åŒ–ä»£ç ...")
        fix_plot_prompt = """
The multi-scenario plotting failed. Please fix plot.py using MINIMAL, TARGETED EDITS.

**CRITICAL: DO NOT rewrite the entire file!**

Key requirements to fix:
1. Read results from all successful scenario directories
2. Generate comprehensive comparison plots showing different scenarios
3. Use clear labels and legends to distinguish different scenarios
4. Create multiple plot types to show different aspects of the results:
   - Convergence curves for each metric across scenarios
   - Scenario comparison bar charts
   - Performance summary plots

**Instructions:**
- Identify the specific issue causing the failure
- Make SURGICAL fixes to only the problematic section
- Use search/replace for targeted modifications
- DO NOT output code that's working correctly

Please provide TARGETED fixes to plot.py.
"""
        coder.run(fix_plot_prompt)
        
        # é‡æ–°å°è¯•ç»˜å›¾
        print("ğŸ”„ é‡æ–°å°è¯•ç”Ÿæˆå¯è§†åŒ–...")
        plot_success, plot_message = generate_comprehensive_visualization(
            folder_name, 
            coder=coder, 
            scenario_results=scenario_results
        )
        
        if plot_success:
            print(f"âœ… ä¿®å¤æˆåŠŸ: {plot_message}")
        else:
            print(f"âŒ ä¿®å¤åä»ç„¶å¤±è´¥: {plot_message}")
    
    # ========================================================================
    # é˜¶æ®µ 3: æ–‡æ¡£æ›´æ–°ï¼ˆå¢å¼ºç‰ˆï¼ŒåŒ…å« AI åœºæ™¯è®¾è®¡ä¿¡æ¯ï¼‰
    # ========================================================================
    print("\n" + "="*80)
    print("ğŸ“ é˜¶æ®µ 3: å®éªŒæ–‡æ¡£æ›´æ–°ï¼ˆåŒ…å« AI åœºæ™¯è®¾è®¡ï¼‰")
    print("="*80 + "\n")
    
    documentation_prompt = f"""
Please update notes.txt with a comprehensive summary of the AI-driven experimental process, including:

# AI-Designed Experimental Scenarios
{json.dumps(scenario_design, indent=2)}

# Execution Results
Successful scenarios: {len(successful_scenarios)}/{len(ai_scenarios)}
Failed scenarios: {len(failed_scenarios)}/{len(ai_scenarios)}

Please include the following sections in your documentation:

1. **Scenario Design Rationale**: Explain why the AI chose these specific scenarios and how they test different aspects of the algorithm
2. **Results Analysis**: Detailed analysis of results from each successful scenario, comparing them with expectations
3. **Algorithm Insights**: What we learned about the algorithm's behavior across different conditions
4. **Visualization Explanation**: Describe what each generated plot shows and the insights it provides
5. **Conclusions and Recommendations**: Overall conclusions and suggestions for further investigation based on the multi-scenario analysis

Make sure to reference specific scenarios and their results in your analysis, and connect the findings back to the original algorithm design.
"""
    
    print("ğŸ¤– AI æ­£åœ¨æ›´æ–°å®éªŒæ–‡æ¡£...")
    coder.run(documentation_prompt)
    print("âœ… æ–‡æ¡£æ›´æ–°å®Œæˆ")
    
    # ========================================================================
    # å®éªŒæµç¨‹å®Œæˆ
    # ========================================================================
    print("\n" + "="*80)
    print("ğŸ‰ AI é©±åŠ¨çš„å¤šåœºæ™¯å®éªŒæµç¨‹å®Œæˆï¼")
    print("="*80 + "\n")
    
    return len(successful_scenarios) > 0

def execute_ai_designed_scenarios(folder_name, scenarios, coder, max_retries=MAX_ITERS, algo_info=None):
    """
    æ‰§è¡Œ AI è®¾è®¡çš„åœºæ™¯ï¼Œä¸ºæ¯ä¸ªåœºæ™¯å®šåˆ¶ä»£ç å®ç°
    
    æ ¸å¿ƒæ€æƒ³ï¼šå…¬å…±ä»£ç åŸºç¡€ + åœºæ™¯ç‰¹å®šå®šåˆ¶
    
    å‚æ•°:
        folder_name: å®éªŒæ–‡ä»¶å¤¹è·¯å¾„
        scenarios: AI è®¾è®¡çš„åœºæ™¯åˆ—è¡¨
        coder: Aider Coder å¯¹è±¡ï¼ˆç”¨äºå®šåˆ¶æ¯ä¸ªåœºæ™¯çš„ä»£ç ï¼‰
        max_retries: æ¯ä¸ªåœºæ™¯å¤±è´¥æ—¶çš„æœ€å¤§é‡è¯•æ¬¡æ•°ï¼ˆé»˜è®¤ä½¿ç”¨ MAX_ITERSï¼‰
        algo_info: ç®—æ³•ä¿¡æ¯å­—å…¸ï¼ˆåŒ…å« title å’Œ pseudocodeï¼‰ï¼Œç”¨äºä¸Šä¸‹æ–‡é‡æ³¨
    
    å·¥ä½œæµç¨‹ï¼š
        1. åœºæ™¯1: ä½¿ç”¨åˆå§‹ä»£ç è¿è¡Œ
        2. åœºæ™¯2: AIæ ¹æ®åœºæ™¯2éœ€æ±‚è°ƒæ•´ä»£ç ï¼ˆå¦‚æ·»åŠ Non-IIDé€»è¾‘ï¼‰â†’ è¿è¡Œ
        3. åœºæ™¯3: AIæ ¹æ®åœºæ™¯3éœ€æ±‚è°ƒæ•´ä»£ç ï¼ˆå¦‚æ·»åŠ æ ‡ç­¾å™ªå£°ï¼‰â†’ è¿è¡Œ
        ...
        æ¯ä¸ªåœºæ™¯éƒ½å¯ä»¥æœ‰è‡ªå·±å®šåˆ¶çš„å®ç°ï¼Œè€Œéä»…é€šè¿‡å‚æ•°åŒºåˆ†
    """
    cwd = osp.abspath(folder_name)
    results = []
    
    # å‡†å¤‡ç®—æ³•ä¿¡æ¯ç”¨äºä¸Šä¸‹æ–‡é‡æ³¨
    if algo_info is None:
        # å°è¯•è¯»å–ç®—æ³•ä¼ªä»£ç 
        tex_content = read_pseudocode_from_tex(folder_name=folder_name)
        algo_info = {
            "title": "Algorithm Experiment",
            "pseudocode": tex_content if tex_content else "Refer to algorithm.tex"
        }
    
    print(f"ğŸš€ å¼€å§‹æ‰§è¡Œ {len(scenarios)} ä¸ª AI è®¾è®¡çš„åœºæ™¯ï¼ˆå¤±è´¥æ—¶è‡ªåŠ¨è¿­ä»£ä¿®å¤ï¼‰")
    
    for i, scenario in enumerate(scenarios, 1):
        scenario_name = scenario.get('name', f'scenario_{i}').replace(' ', '_').lower()
        parameters = scenario.get('parameters', {})
        
        print(f"\n{'='*80}")
        print(f"åœºæ™¯ {i}/{len(scenarios)}: {scenario_name}")
        print(f"{'='*80}")
        print(f"æè¿°: {scenario.get('description', 'No description')}")
        print(f"æ•°æ®é›†: {scenario.get('dataset', 'No dataset info')}")
        print(f"å‚æ•°: {json.dumps(parameters, indent=2)}")
        print()
        
        # ====================================================================
        # åœºæ™¯å‡†å¤‡ï¼šä¸ºå½“å‰åœºæ™¯è°ƒæ•´ä»£ç ï¼ˆåœºæ™¯ç‰¹å®šå®šåˆ¶ï¼‰
        # ====================================================================
        if i > 1:  # ä»ç¬¬äºŒä¸ªåœºæ™¯å¼€å§‹ï¼Œéœ€è¦æ ¹æ®åœºæ™¯éœ€æ±‚è°ƒæ•´ä»£ç 
            print(f"ğŸ”§ ä¸ºåœºæ™¯ {i} è°ƒæ•´ä»£ç å®ç°...")
            
            # [ä¸Šä¸‹æ–‡ç®¡ç†] åœ¨åœºæ™¯åˆ‡æ¢æ—¶æ¸…ç†å†å²å¹¶é‡æ³¨ä¸Šä¸‹æ–‡
            # context_prefix = reset_and_prime_coder(  # ä¸´æ—¶æ³¨é‡Šæ‰ç”¨äºæµ‹è¯•
            #     coder, 
            #     algo_info, 
            #     stage_description=f"Scenario {i}/{len(scenarios)}: {scenario_name}"
            # )
            
            # å°†ä¸Šä¸‹æ–‡é‡æ³¨æç¤ºè¯æ·»åŠ åˆ° preparation_prompt å‰é¢
            # preparation_prompt = context_prefix + f"""  # ä¸´æ—¶æ³¨é‡Šæ‰ç”¨äºæµ‹è¯•
            preparation_prompt = f"""

Now prepare the code for the next scenario: {scenario_name}

# Scenario Details
Description: {scenario.get('description', '')}
Dataset: {scenario.get('dataset', '')}
Expected Insight: {scenario.get('expected_insight', '')}
Parameters: {json.dumps(parameters, indent=2)}

# Your Task
Modify experiment.py to properly implement this scenario's requirements:

## Critical Implementation Requirements

1. **Data Partition Strategy**:
   - If the scenario mentions "IID" or "uniform": Use simple random partition
   - If the scenario mentions "Non-IID", "label skew", "heterogeneous", or "each client has 2-3 classes":
     ```python
     def partition_non_iid(X, y, n_clients, classes_per_client=2, seed=None):
         # Assign specific classes to each client
         # Each client predominantly holds samples from classes_per_client classes
     ```
   
2. **Label Noise Injection**:
   - If the scenario mentions "label noise", "label flip", "corrupted labels", or "X% noise":
     ```python
     def add_label_noise(y, noise_rate=0.2, n_classes=10, seed=None):
         # Randomly flip noise_rate proportion of labels
         # Return corrupted labels
     ```
   
3. **Data Generation/Loading**:
   - Adjust data generation parameters if needed (samples, features, classes)
   - Ensure the dataset matches the scenario description

4. **Algorithm Selection**:
   - Ensure the algorithm parameter (--algo or --algorithm) is properly handled
   - Support both FedAvg and SFedAvg if needed

## Important Notes
- Keep the command-line interface consistent (--out_dir is required)
- Maintain the output format (final_info.json with means and stds)
- Add any new parameters needed for this scenario
- The code should be FULLY FUNCTIONAL for this specific scenario

**CRITICAL: Use MINIMAL, TARGETED EDITS - DO NOT rewrite the entire file!**

**Modification Strategy:**
1. Identify which specific functions/sections need changes for this scenario
2. Add new functions if needed (e.g., partition_non_iid, add_label_noise)
3. Modify only the data generation/loading section to call these new functions
4. Add new command-line arguments if required
5. DO NOT touch working code sections (model, training loop, etc.)

**Use search/replace or focused edits - preserve all existing working code!**

Please modify experiment.py now with TARGETED changes for this scenario."""

            print(f"ğŸ¤– AI æ­£åœ¨è°ƒæ•´ä»£ç ä»¥åŒ¹é…åœºæ™¯éœ€æ±‚...")
            coder_out = coder.run(preparation_prompt)
            print(coder_out)
            print()
            print(f"âœ… ä»£ç è°ƒæ•´å®Œæˆï¼Œå‡†å¤‡è¿è¡Œåœºæ™¯ {i}")
            print()
        
        # æ„å»ºå‘½ä»¤å‚æ•°åˆ—è¡¨
        args_list = []
        for key, value in parameters.items():
            args_list.append(f"{key}={value}")
        
        # ====================================================================
        # è¿­ä»£æ‰§è¡Œåœºæ™¯ï¼ˆå¤±è´¥æ—¶ä¿®å¤ï¼‰
        # ====================================================================
        success = False
        final_error = None
        
        for attempt in range(max_retries):
            print(f"å°è¯• {attempt + 1}/{max_retries}: æ‰§è¡Œåœºæ™¯ '{scenario_name}'")
            
            # æ‰§è¡Œåœºæ™¯
            success, result_info = execute_single_scenario(folder_name, scenario_name, args_list)
            
            if success:
                print(f"âœ… åœºæ™¯ '{scenario_name}' æ‰§è¡ŒæˆåŠŸï¼")
                
                # è®°å½•åœºæ™¯å®ç°æ‘˜è¦
                print(f"ğŸ“ ä¸ºåœºæ™¯ '{scenario_name}' ç”Ÿæˆå®ç°æ‘˜è¦...")
                summary_prompt = f"""Please document what special implementations were made for scenario '{scenario_name}'.

Scenario description: {scenario.get('description', '')}
Dataset: {scenario.get('dataset', '')}

Please append to notes.txt a brief summary of:
1. What data partition strategy was used (IID, Non-IID, etc.)
2. Whether label noise was added and at what rate
3. Any special data processing or algorithm modifications
4. The key implementation details that make this scenario different from others

Keep it concise (3-5 lines) and factual."""
                
                coder.run(summary_prompt)
                print(f"âœ… å®ç°æ‘˜è¦å·²æ·»åŠ åˆ° notes.txt")
                
                break
            else:
                print(f"âŒ åœºæ™¯ '{scenario_name}' æ‰§è¡Œå¤±è´¥")
                final_error = result_info
                
                # å¦‚æœè¿˜æœ‰é‡è¯•æœºä¼šï¼Œè®© AI ä¿®å¤ä»£ç 
                if attempt < max_retries - 1:
                    print(f"ğŸ¤– è®© AI æ ¹æ®é”™è¯¯ä¿®å¤ä»£ç ...")
                    
                    # æˆªæ–­è¿‡é•¿çš„é”™è¯¯ä¿¡æ¯
                    error_msg = result_info
                    if len(error_msg) > MAX_STDERR_OUTPUT:
                        error_msg = "..." + error_msg[-MAX_STDERR_OUTPUT:]
                    
                    # ç”Ÿæˆä¿®å¤æç¤ºè¯
                    fix_prompt = f"""The scenario '{scenario_name}' failed with the following error:

{error_msg}

Scenario description:
{scenario.get('description', 'No description')}

Parameters used:
{json.dumps(parameters, indent=2)}

Expected insight:
{scenario.get('expected_insight', 'No expected insight provided')}

**CRITICAL: Fix using MINIMAL, TARGETED EDITS - DO NOT rewrite the entire file!**

Common issues to check and fix:
1. Parameter validation and handling â†’ Fix argument parsing/validation
2. Data generation/loading â†’ Fix the specific data preparation function
3. Model compatibility â†’ Adjust model initialization if needed
4. Edge cases â†’ Add boundary checks where missing
5. File I/O and result saving â†’ Fix save operations

**Instructions:**
- Analyze the error to pinpoint the EXACT problematic function/line
- Make SURGICAL fixes to only that specific location
- Use search/replace for targeted modifications
- DO NOT modify code that's working correctly

Please make TARGETED changes to experiment.py now.
"""
                    
                    # AI ä¿®å¤ä»£ç 
                    coder_out = coder.run(fix_prompt)
                    print(coder_out)
                    print()
                    print(f"ğŸ”„ é‡è¯•åœºæ™¯ '{scenario_name}'...")
                    
                    # æ·»åŠ çŸ­æš‚å»¶è¿Ÿ
                    import time
                    time.sleep(2)
                else:
                    print(f"âš ï¸ åœºæ™¯ '{scenario_name}' è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° ({max_retries})ï¼Œæ ‡è®°ä¸ºå¤±è´¥")
        
        # ====================================================================
        # è®°å½•åœºæ™¯æ‰§è¡Œç»“æœ
        # ====================================================================
        scenario_result = {
            "name": scenario_name,
            "description": scenario.get('description', ''),
            "dataset": scenario.get('dataset', ''),
            "parameters": parameters,
            "expected_insight": scenario.get('expected_insight', ''),
            "status": "success" if success else "failed",
            "run_dir": f"run_{scenario_name}",
            "attempts": attempt + 1,  # è®°å½•å®é™…å°è¯•æ¬¡æ•°
            "implementation_note": "Code was customized for this scenario's specific requirements" if i > 1 else "Uses initial generated code"
        }
        
        if not success:
            scenario_result["error"] = final_error
        
        # ====================================================================
        # åœºæ™¯æˆåŠŸåç«‹å³è¿›è¡Œå‚æ•°è°ƒä¼˜ï¼ˆæ–°å¢ï¼‰
        # ====================================================================
        if success and ENABLE_HYPERPARAMETER_TUNING:
            try:
                print(f"\n{'='*80}")
                print(f"ğŸ¯ åœºæ™¯ '{scenario_name}' æ‰§è¡ŒæˆåŠŸï¼Œç«‹å³å¼€å§‹å‚æ•°è°ƒä¼˜")
                print(f"{'='*80}\n")
                
                tuning_report = tune_scenario_immediately(
                    folder_name=folder_name,
                    scenario_info=scenario_result,
                    coder=coder,
                    algo_info=algo_info
                )
                
                if tuning_report:
                    scenario_result["tuning_report"] = tuning_report
                    print(f"\nâœ… åœºæ™¯ '{scenario_name}' è°ƒä¼˜æˆåŠŸå®Œæˆ")
                else:
                    print(f"\nâš ï¸  åœºæ™¯ '{scenario_name}' æœªæ‰§è¡Œè°ƒä¼˜")
                    
            except Exception as e:
                print(f"\nâŒ åœºæ™¯ '{scenario_name}' è°ƒä¼˜å¤±è´¥: {str(e)}")
                import traceback
                traceback.print_exc()
                scenario_result["tuning_error"] = str(e)
        
        results.append(scenario_result)
        
        print()
        if success:
            print(f"âœ… åœºæ™¯ {i}/{len(scenarios)} å®Œæˆ")
        else:
            print(f"âŒ åœºæ™¯ {i}/{len(scenarios)} å¤±è´¥")
        print()
        
        # æ·»åŠ çŸ­æš‚å»¶è¿Ÿé¿å…èµ„æºå†²çª
        import time
        time.sleep(2)
    
    return results


def execute_single_scenario(folder_name, scenario_name, args_list):
    """
    æ‰§è¡Œå•ä¸ªåœºæ™¯
    """
    cwd = osp.abspath(folder_name)
    run_dir = f"run_{scenario_name}"
    
    # æ„å»ºå®Œæ•´å‘½ä»¤
    command = ["python", "experiment.py", f"--out_dir={run_dir}"] + args_list
    
    try:
        print(f"   æ‰§è¡Œ: {' '.join(command)}")
        result = subprocess.run(
            command, 
            cwd=cwd, 
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE, 
            text=True, 
            timeout=3600  # 1å°æ—¶è¶…æ—¶
        )
        
        if result.returncode == 0:
            print(f"   âœ… åœºæ™¯ '{scenario_name}' å®Œæˆ")
            
            # ================================================================
            # ä¿å­˜ä»£ç å¿«ç…§åˆ°åœºæ™¯ç›®å½•ï¼ˆä¸ run_experiment ä¿æŒä¸€è‡´ï¼‰
            # ================================================================
            scenario_dir = osp.join(cwd, run_dir)
            
            # å¤åˆ¶ experiment.py
            exp_file = osp.join(cwd, "experiment.py")
            if osp.exists(exp_file):
                shutil.copy(exp_file, osp.join(scenario_dir, "experiment.py"))
                print(f"   ğŸ“„ å·²ä¿å­˜ experiment.py å¿«ç…§")
            
            # å¤åˆ¶ plot.py
            plot_file = osp.join(cwd, "plot.py")
            if osp.exists(plot_file):
                shutil.copy(plot_file, osp.join(scenario_dir, "plot.py"))
                print(f"   ğŸ“„ å·²ä¿å­˜ plot.py å¿«ç…§")
            
            # å¤åˆ¶ notes.txt
            notes_file = osp.join(cwd, "notes.txt")
            if osp.exists(notes_file):
                shutil.copy(notes_file, osp.join(scenario_dir, "notes.txt"))
            
            # æ£€æŸ¥æ˜¯å¦ç”Ÿæˆäº†ç»“æœæ–‡ä»¶
            result_file = osp.join(scenario_dir, "final_info.json")
            if osp.exists(result_file):
                print(f"   ğŸ“„ ç»“æœæ–‡ä»¶: {result_file}")
            else:
                print(f"   âš ï¸  åœºæ™¯å®Œæˆä½†æœªç”Ÿæˆç»“æœæ–‡ä»¶")
            
            return True, "Success"
        else:
            print(f"   âŒ åœºæ™¯ '{scenario_name}' å¤±è´¥")
            error_msg = result.stderr[:500] if result.stderr else "Unknown error"
            
            # æ¸…ç†å¤±è´¥çš„è¿è¡Œç›®å½•
            failed_dir = osp.join(cwd, run_dir)
            if osp.exists(failed_dir):
                shutil.rmtree(failed_dir)
            
            return False, error_msg
            
    except TimeoutExpired:
        print(f"   â° åœºæ™¯ '{scenario_name}' è¶…æ—¶")
        
        # æ¸…ç†è¶…æ—¶çš„è¿è¡Œç›®å½•
        timeout_dir = osp.join(cwd, run_dir)
        if osp.exists(timeout_dir):
            shutil.rmtree(timeout_dir)
            
        return False, "Timeout"
    except Exception as e:
        print(f"   ğŸ’¥ åœºæ™¯ '{scenario_name}' å¼‚å¸¸: {str(e)}")
        return False, str(e)


# ============================================================================
# å‚æ•°è°ƒä¼˜åŠŸèƒ½æ¨¡å—ï¼ˆåœºæ™¯çº§å³æ—¶è°ƒä¼˜ï¼‰
# ============================================================================

def extract_tunable_parameters_from_code(folder_name):
    """
    ä» experiment.py ä¸­æå–å¯è°ƒå‚æ•°ï¼ˆæ”¹è¿›ç‰ˆï¼šæ”¯æŒæ›´çµæ´»çš„å‚æ•°å®šä¹‰æ ¼å¼ï¼‰
    
    è¿”å›: {param_name: {"type": str, "current_value": value, "description": str}}
    """
    exp_file = osp.join(folder_name, "experiment.py")
    
    if not osp.exists(exp_file):
        return {}
    
    try:
        with open(exp_file, 'r', encoding='utf-8') as f:
            content = f.read()
        
        import re
        params = {}
        
        # æ”¹è¿›çš„æ­£åˆ™è¡¨è¾¾å¼ï¼šæ›´çµæ´»åœ°åŒ¹é…å„ç§æ ¼å¼
        # åŒ¹é… parser.add_argument('--param', ..., type=int, ..., default=10, ...)
        # æ”¯æŒå¤šè¡Œã€ç©ºæ ¼å˜åŒ–ã€é¡ºåºå˜åŒ–
        
        # é¦–å…ˆæ‰¾åˆ°æ‰€æœ‰ add_argument è°ƒç”¨
        arg_pattern = r"parser\.add_argument\(\s*['\"]--([a-zA-Z_][a-zA-Z0-9_]*)['\"]\s*,([^)]+)\)"
        arg_matches = re.findall(arg_pattern, content)
        
        # æ’é™¤ä¸åº”è°ƒä¼˜çš„å‚æ•°
        excluded_params = ['out_dir', 'seed', 'dataset', 'enable_tuning', 'algorithm', 'scenario']
        
        for param_name, arg_content in arg_matches:
            if param_name in excluded_params:
                continue
            
            # è·³è¿‡ action='store_true' ç±»å‹çš„å‚æ•°ï¼ˆå¸ƒå°”æ ‡å¿—ï¼‰
            if "action=" in arg_content and "store_true" in arg_content:
                continue
            
            # æå– type
            type_match = re.search(r"type\s*=\s*(\w+)", arg_content)
            if not type_match:
                continue
            param_type = type_match.group(1)
            
            # åªå¤„ç† int å’Œ float
            if param_type not in ['int', 'float']:
                continue
            
            # æå– default
            default_match = re.search(r"default\s*=\s*([\d.eE+-]+|None)", arg_content)
            if not default_match:
                continue
            default_str = default_match.group(1)
            
            # è·³è¿‡ default=None
            if default_str == 'None':
                continue
            
            # æå– help (å¯é€‰)
            help_match = re.search(r"help\s*=\s*['\"]([^'\"]+)['\"]", arg_content)
            description = help_match.group(1) if help_match else f"Parameter {param_name}"
            
            # è§£æé»˜è®¤å€¼
            try:
                if param_type == 'int':
                    params[param_name] = {
                        "type": "int",
                        "current_value": int(float(default_str)),  # å…ˆè½¬ float å†è½¬ intï¼Œæ”¯æŒç§‘å­¦è®¡æ•°æ³•
                        "description": description
                    }
                elif param_type == 'float':
                    params[param_name] = {
                        "type": "float",
                        "current_value": float(default_str),
                        "description": description
                    }
            except (ValueError, TypeError) as e:
                print(f"   âš ï¸ è·³è¿‡å‚æ•° {param_name}: æ— æ³•è§£æé»˜è®¤å€¼ '{default_str}' ({e})")
                continue
        
        print(f"   âœ“ æˆåŠŸæå– {len(params)} ä¸ªå¯è°ƒå‚æ•°")
        if params:
            print(f"   å‚æ•°åˆ—è¡¨: {', '.join(params.keys())}")
        
        return params
        
    except Exception as e:
        print(f"âš ï¸ æå–å‚æ•°æ—¶å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return {}


def clean_json_from_response(text):
    """
    ä» AI å“åº”ä¸­æ¸…ç†å¹¶æå–çº¯ JSON
    
    å¤„ç†å¤šç§æ ¼å¼ï¼š
    - ä»£ç å—åŒ…è£¹çš„ JSON
    - åŒ…å« diff æ ‡è®°çš„å“åº”
    - çº¯ JSON å¯¹è±¡
    """
    import re
    
    # 1. ç§»é™¤ diff æ ‡è®°
    if '<<<<<<< SEARCH' in text or '=======' in text or '>>>>>>> REPLACE' in text:
        # æå– ======= å’Œ >>>>>>> ä¹‹é—´çš„å†…å®¹
        match = re.search(r'=======\s*(.*?)\s*>>>>>>>', text, re.DOTALL)
        if match:
            text = match.group(1).strip()
    
    # 2. ç§»é™¤ä»£ç å—æ ‡è®°
    text = re.sub(r'```json\s*', '', text)
    text = re.sub(r'```\s*', '', text)
    
    # 3. æå– JSON å¯¹è±¡ï¼ˆæ”¯æŒåµŒå¥—ï¼‰
    # ä½¿ç”¨æ›´å¥å£®çš„æ–¹æ³•ï¼šä»ç¬¬ä¸€ä¸ª { å¼€å§‹ï¼ŒåŒ¹é…å®Œæ•´çš„ JSON
    stack = []
    start_idx = -1
    
    for i, char in enumerate(text):
        if char == '{':
            if not stack:
                start_idx = i
            stack.append(char)
        elif char == '}':
            if stack and stack[-1] == '{':
                stack.pop()
                if not stack and start_idx != -1:
                    # æ‰¾åˆ°å®Œæ•´çš„ JSON å¯¹è±¡
                    return text[start_idx:i+1]
    
    # å¦‚æœä¸Šé¢çš„æ–¹æ³•å¤±è´¥ï¼Œä½¿ç”¨æ­£åˆ™
    match = re.search(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', text, re.DOTALL)
    if match:
        return match.group().strip()
    
    return None


def analyze_algorithm_characteristics(scenario_info, tunable_params, algo_info, coder, scenario_dir, folder_name):
    """
    è®© LLM æ·±åº¦åˆ†æç®—æ³•ç‰¹æ€§å’Œè°ƒä¼˜éœ€æ±‚ï¼ˆç¬¬ä¸€é˜¶æ®µï¼šæ™ºèƒ½åˆ†æï¼‰
    
    å‚æ•°:
        scenario_info: åœºæ™¯ä¿¡æ¯
        tunable_params: å¯è°ƒå‚æ•°åˆ—è¡¨
        algo_info: ç®—æ³•ä¿¡æ¯ï¼ˆä» algorithm.tex æˆ– idea.json æå–ï¼‰
        coder: Aider Coder å¯¹è±¡
        scenario_dir: åœºæ™¯ç›®å½•ï¼ˆä¿å­˜åˆ†ææŠ¥å‘Šï¼‰
        folder_name: å®éªŒæ–‡ä»¶å¤¹è·¯å¾„ï¼ˆè¯»å– experiment.pyï¼‰
    
    è¿”å›: {
        "key_parameters": [...],  # å…³é”®å‚æ•°åŠå…¶çº¦æŸ
        "ablation_required": [...],  # éœ€è¦æ¶ˆèå®éªŒçš„å‚æ•°
        "parameter_constraints": {...},  # å‚æ•°çº¦æŸï¼ˆå¦‚ subspace_dim >= 0.25 * input_dimï¼‰
        "task_suitability": str,  # ä»»åŠ¡é€‚é…æ€§åˆ†æ
        "tuning_strategy": str  # æ¨èçš„è°ƒä¼˜ç­–ç•¥
    }
    """
    print(f"\n{'='*80}")
    print("ğŸ§  é˜¶æ®µ1: LLM è‡ªä¸»åˆ†æç®—æ³•ç‰¹æ€§å’Œè°ƒä¼˜éœ€æ±‚")
    print(f"{'='*80}\n")
    
    # è¯»å–å®é™…çš„å®éªŒä»£ç 
    exp_file = osp.join(folder_name, "experiment.py")
    experiment_code = ""
    if osp.exists(exp_file):
        with open(exp_file, 'r', encoding='utf-8') as f:
            experiment_code = f.read()
        print(f"   âœ“ å·²è¯»å– experiment.py ({len(experiment_code)} å­—ç¬¦)")
    else:
        print(f"   âš ï¸ æœªæ‰¾åˆ° experiment.py")
    
    # æ„å»ºåˆ†ææç¤ºè¯
    analysis_prompt = f"""You are an expert in federated learning and hyperparameter optimization. 
Your task is to analyze this algorithm implementation and provide strategic insights for parameter tuning.

# Algorithm Description (from algorithm.tex)
{algo_info.get('description', 'No description available')}

# Actual Experiment Implementation
Below is the complete experiment.py code that implements the algorithm:

```python
{experiment_code[:15000]}  # é™åˆ¶é•¿åº¦é¿å…è¶…è¿‡ token é™åˆ¶
```

**IMPORTANT: Analyze the ACTUAL CODE to understand:**
- What dataset is being used (synthetic classification/regression, MNIST, CIFAR, etc.)
- Problem complexity (binary/multi-class classification, regression, feature dimensions)
- Data distribution (IID or Non-IID)
- Whether there's label noise
- Gradient space complexity (low-rank binary classification vs. high-rank multi-class/regression)

# Current Scenario Context
Name: {scenario_info.get('name', 'Unknown')}
Description: {scenario_info.get('description', '')}
Parameters: {json.dumps(scenario_info.get('parameters', {}), indent=2)}

# Available Tunable Parameters
{json.dumps(tunable_params, indent=2)}

# Your Analysis Tasks

## 1. Task Complexity Assessment (CRITICAL!)
From the experiment code, determine:
- Dataset type: classification (how many classes?) or regression?
- Feature dimensions: How many input features?
- Data distribution: IID or Non-IID (label skew)?
- Label noise: Is there any?

**Most Important**: Assess gradient space complexity:
- Binary classification (2 classes): LOW complexity, simple decision boundary, low effective gradient dimension
- Multi-class (3-4 classes): MEDIUM complexity
- Multi-class (â‰¥5 classes): MEDIUM-HIGH complexity, richer gradient space
- Regression: HIGH complexity, full-rank gradient space

## 2. Algorithm-Task Compatibility Analysis
Based on the algorithm's core mechanism and the task complexity:
- Does the task have sufficient complexity for this algorithm to show benefits?
- Red flags: Compression algorithms on low-rank problems (e.g., SFedAvg on binary classification)
- Green lights: Complex tasks that can benefit from the algorithm's mechanism

## 3. Key Parameter Identification
For EACH tunable parameter, identify:
- Is it a CRITICAL parameter (core to algorithm mechanism)?
- Is it IMPORTANT (significant impact on performance)?
- Is it MINOR (fine-tuning only)?

## 4. Parameter Constraint Analysis
For critical parameters, derive mathematical/physical constraints FROM ALGORITHM THEORY:
- Example: "subspace_dim should be >= 0.25 * input_dim to preserve gradient information (Johnson-Lindenstrauss lemma)"
- Example: "momentum should be < 1.0 for convergence stability"
- Example: "local_steps should balance communication efficiency and gradient staleness"

**Think about the algorithm's THEORETICAL PROPERTIES to derive these constraints!**

## 5. Ablation Study Requirements
Which parameters MUST be systematically tested?
- Parameters that directly control the algorithm's core mechanism
- For compression algorithms: compression ratio, subspace dimension
- Recommend specific test points based on input dimensions (e.g., Î´ = 0.25, 0.50, 0.75, 1.00)

## 6. Tuning Strategy Recommendation
Based on your analysis:
- Should we do ablation study first or direct random search?
- What's the priority order of parameters to tune?
- Any parameter interactions to consider?
- If task is not suitable for algorithm, how to adjust?

# Output Format (JSON ONLY, NO OTHER TEXT)

Please output a JSON object with this EXACT structure:

{{
  "task_analysis": {{
    "dataset_type": "binary_classification|multi_class_classification|regression",
    "n_classes": number or null,
    "input_dim": number,
    "data_distribution": "IID|Non-IID",
    "gradient_space_complexity": "low|medium|high",
    "complexity_rationale": "Explain why this complexity level"
  }},
  "algorithm_mechanism": "Brief description of core mechanism",
  "key_parameters": [
    {{
      "name": "parameter_name",
      "priority": "critical|important|minor",
      "reason": "Why this parameter is important",
      "constraints": {{
        "type": "ratio|absolute|categorical",
        "min": value or null,
        "max": value or null,
        "recommended_values": [list of specific values to test],
        "constraint_description": "Mathematical/physical constraint explanation with theoretical basis"
      }},
      "ablation_required": true or false
    }}
  ],
  "task_suitability": {{
    "is_suitable": true or false,
    "confidence": "high|medium|low",
    "reason": "Why suitable or not, considering BOTH algorithm mechanism AND actual task complexity from code",
    "recommendations": "Specific suggestions for improvement if not suitable"
  }},
  "tuning_strategy": {{
    "approach": "ablation_first|random_search|mixed",
    "rationale": "Why this approach based on algorithm properties and task",
    "parameter_priority": ["param1", "param2", "..."]
  }},
  "special_notes": "Any other important considerations based on the actual implementation"
}}

**CRITICAL REQUIREMENTS:**
1. Output PURE JSON only - no markdown, no code blocks, no explanations outside JSON
2. MUST analyze the actual experiment.py code to assess task complexity
3. Think deeply about algorithm-task compatibility (e.g., compression on low-rank problems is problematic)
4. Be specific with numerical constraints computed from actual input dimensions
5. Base constraints on THEORY (e.g., information theory, optimization theory), not just heuristics

Now, please provide your analysis:

**CRITICAL REQUIREMENTS:**
1. Output PURE JSON only - no markdown, no code blocks, no explanations outside JSON
2. Think deeply about the algorithm's theoretical properties
3. Be specific with numerical constraints (not vague ranges)
4. If you identify parameters like "subspace_dim" or "compression_ratio", compute their valid ranges based on input dimensions

Now, please provide your analysis:"""
    
    print("ğŸ¤– æ­£åœ¨è®© LLM åˆ†æç®—æ³•ç‰¹æ€§...")
    
    # ä½¿ç”¨èŠå¤©æ¨¡å¼è·å–åˆ†æï¼ˆä¸ä¿®æ”¹æ–‡ä»¶ï¼‰
    try:
        # ä¸´æ—¶ä½¿ç”¨ coder çš„èŠå¤©åŠŸèƒ½
        response = coder.run(analysis_prompt)
        
        # ä¿å­˜åŸå§‹å“åº”
        debug_file = osp.join(scenario_dir, "algorithm_analysis_raw.txt")
        with open(debug_file, 'w', encoding='utf-8') as f:
            f.write(response)
        print(f"   ğŸ“„ åŸå§‹åˆ†æå·²ä¿å­˜: {debug_file}")
        
        # æ¸…ç†å¹¶è§£æ JSON
        cleaned_response = clean_json_from_response(response)
        
        if not cleaned_response:
            raise ValueError("æ— æ³•ä» AI å“åº”ä¸­æå– JSON")
        
        analysis = json.loads(cleaned_response)
        
        # ä¿å­˜æ ¼å¼åŒ–çš„åˆ†ææŠ¥å‘Š
        report_file = osp.join(scenario_dir, "algorithm_analysis.json")
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(analysis, f, indent=2, ensure_ascii=False)
        print(f"   âœ… åˆ†ææŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        
        # æ‰“å°å…³é”®å‘ç°
        print(f"\nğŸ“Š åˆ†æç»“æœæ‘˜è¦:")
        print(f"   ç®—æ³•æœºåˆ¶: {analysis.get('algorithm_mechanism', 'N/A')}")
        print(f"   ä»»åŠ¡é€‚é…æ€§: {'âœ… åˆé€‚' if analysis.get('task_suitability', {}).get('is_suitable', False) else 'âš ï¸ ä¸å¤ªåˆé€‚'}")
        print(f"   æ¨èç­–ç•¥: {analysis.get('tuning_strategy', {}).get('approach', 'N/A')}")
        
        critical_params = [p for p in analysis.get('key_parameters', []) 
                          if p.get('priority') == 'critical']
        if critical_params:
            print(f"   å…³é”®å‚æ•°: {', '.join([p['name'] for p in critical_params])}")
        
        return analysis
        
    except json.JSONDecodeError as e:
        print(f"âŒ JSON è§£æå¤±è´¥: {e}")
        print(f"   æ¸…ç†åçš„å“åº”: {cleaned_response[:500]}")
        return None
    except Exception as e:
        print(f"âŒ åˆ†æå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None


def design_tuning_strategy_for_scenario(scenario_info, current_results, tunable_params, algorithm_analysis, coder, scenario_dir):
    """
    è®© AI ä¸ºå½“å‰åœºæ™¯è®¾è®¡å‚æ•°è°ƒä¼˜æœç´¢ç©ºé—´ï¼ˆç¬¬äºŒé˜¶æ®µï¼šåŸºäºåˆ†æè®¾è®¡è°ƒä¼˜ï¼‰
    
    å‚æ•°:
        scenario_info: åœºæ™¯ä¿¡æ¯
        current_results: å½“å‰åŸºçº¿ç»“æœ
        tunable_params: å¯è°ƒå‚æ•°åˆ—è¡¨
        algorithm_analysis: ç¬¬ä¸€é˜¶æ®µçš„ç®—æ³•åˆ†æç»“æœ
        coder: Aider Coder å¯¹è±¡
        scenario_dir: åœºæ™¯ç›®å½•ï¼ˆä¿å­˜è°ƒè¯•æ—¥å¿—ï¼‰
    
    è¿”å›: {"search_space": {...}, "rationale": str, "num_trials": int, "ablation_configs": [...]}
    """
    print(f"\n{'='*80}")
    print("ğŸ¯ é˜¶æ®µ2: åŸºäºç®—æ³•åˆ†æè®¾è®¡è°ƒä¼˜ç­–ç•¥")
    print(f"{'='*80}\n")
    
    # æå–å…³é”®ä¿¡æ¯
    task_suitable = algorithm_analysis.get('task_suitability', {}).get('is_suitable', True) if algorithm_analysis else True
    tuning_approach = algorithm_analysis.get('tuning_strategy', {}).get('approach', 'random_search') if algorithm_analysis else 'random_search'
    
    # æ„å»ºå¢å¼ºçš„æç¤ºè¯
    prompt = f"""You are an expert in hyperparameter optimization. Design a comprehensive tuning strategy based on the algorithm analysis.

# Previous Algorithm Analysis (Phase 1 Results)
{json.dumps(algorithm_analysis, indent=2) if algorithm_analysis else "No analysis available"}

# Scenario Information
Name: {scenario_info.get('name', 'Unknown')}
Description: {scenario_info.get('description', '')}

# Current Performance (Baseline)
{json.dumps(current_results, indent=2)}

# Available Tunable Parameters
{json.dumps(tunable_params, indent=2)}

# Your Task: Design TWO-STAGE Tuning Strategy

## Stage 1: Ablation Study (if needed)
Based on the algorithm analysis, if any parameters are marked as "ablation_required":
- Design specific test configurations for systematic parameter exploration
- For example: if subspace_dim needs ablation, test Î´ = [0.25, 0.50, 0.75, 1.00]

Output format for ablation:
{{
  "ablation_required": true/false,
  "ablation_configs": [
    {{
      "config_name": "descriptive_name",
      "parameters": {{"param1": value1, "param2": value2}},
      "rationale": "Why test this configuration"
    }}
  ]
}}

## Stage 2: Random Search (always needed)
Design a search space for 2-4 most impactful parameters:

### Parameter Type Specifications:
- Float: {{"type": "float", "min": value, "max": value, "scaling": "linear"|"log"}}
- Integer: {{"type": "int", "min": value, "max": value}}
- Categorical: {{"type": "categorical", "values": [...]}}

### CRITICAL REQUIREMENTS from Algorithm Analysis:
{_format_algorithm_constraints(algorithm_analysis) if algorithm_analysis else "Use general best practices"}

**IMPORTANT:**
1. RESPECT the parameter constraints from Phase 1 analysis
2. If task is NOT suitable for algorithm, recommend wider search ranges or parameter adjustments
3. For ratio-based parameters (like subspace_dim), compute actual values based on input dimensions
4. Prioritize parameters identified as "critical" in Phase 1

## Complete Output Format (JSON ONLY)

{{
  "ablation_study": {{
    "required": true/false,
    "configs": [
      {{
        "name": "config_name",
        "parameters": {{...}},
        "rationale": "..."
      }}
    ]
  }},
  "random_search": {{
    "search_space": {{
      "param1": {{"type": "...", ...}},
      "param2": {{"type": "...", ...}}
    }},
    "num_trials": 8-15,
    "rationale": "Why these parameters and ranges"
  }},
  "task_suitability_warning": "Any warnings about task-algorithm mismatch",
  "expected_improvement": "What we expect to achieve"
}}

**CRITICAL:**
- Output PURE JSON only (no markdown, no code blocks, no extra text)
- NO diff markers (<<<<<<< SEARCH, =======, >>>>>>> REPLACE)
- Compute actual numerical ranges (don't use placeholders)
- If Phase 1 identified parameter constraints, ENFORCE them

Now design the tuning strategy:"""
    
    print("ğŸ¤– æ­£åœ¨è®© LLM è®¾è®¡è°ƒä¼˜ç­–ç•¥...")
    
    try:
        ai_response = coder.run(prompt)
        
        # ä¿å­˜åŸå§‹å“åº”
        debug_file = osp.join(scenario_dir, "tuning_strategy_raw.txt")
        with open(debug_file, 'w', encoding='utf-8') as f:
            f.write("=== AI åŸå§‹å“åº” ===\n")
            f.write(ai_response)
            f.write("\n\n=== æç¤ºè¯ ===\n")
            f.write(prompt)
        print(f"   ğŸ“„ åŸå§‹ç­–ç•¥å·²ä¿å­˜: {debug_file}")
        
        # æ¸…ç†å¹¶è§£æ JSON
        json_str = clean_json_from_response(ai_response)
        
        if not json_str:
            print("âŒ æ— æ³•ä» AI å“åº”ä¸­æå– JSON")
            print(f"   å“åº”å‰ 500 å­—ç¬¦: {ai_response[:500]}")
            return None
        
        print(f"   âœ“ æˆåŠŸæå– JSON ({len(json_str)} å­—ç¬¦)")
        
        strategy = json.loads(json_str)
        
        # ä¿å­˜æ ¼å¼åŒ–çš„ç­–ç•¥
        strategy_file = osp.join(scenario_dir, "tuning_strategy.json")
        with open(strategy_file, 'w', encoding='utf-8') as f:
            json.dump(strategy, f, indent=2, ensure_ascii=False)
        print(f"   âœ… è°ƒä¼˜ç­–ç•¥å·²ä¿å­˜: {strategy_file}")
        
        # æ‰“å°ç­–ç•¥æ‘˜è¦
        print(f"\nğŸ“Š è°ƒä¼˜ç­–ç•¥æ‘˜è¦:")
        
        ablation = strategy.get('ablation_study', {})
        if ablation.get('required', False):
            num_configs = len(ablation.get('configs', []))
            print(f"   æ¶ˆèå®éªŒ: âœ… éœ€è¦ ({num_configs} ä¸ªé…ç½®)")
        else:
            print(f"   æ¶ˆèå®éªŒ: â­ï¸  è·³è¿‡")
        
        random_search = strategy.get('random_search', {})
        if 'search_space' in random_search:
            num_params = len(random_search['search_space'])
            num_trials = random_search.get('num_trials', 10)
            print(f"   éšæœºæœç´¢: {num_params} ä¸ªå‚æ•°, {num_trials} æ¬¡è¯•éªŒ")
            print(f"   è°ƒä¼˜å‚æ•°: {', '.join(random_search['search_space'].keys())}")
        
        warning = strategy.get('task_suitability_warning')
        if warning:
            print(f"   âš ï¸  ä»»åŠ¡é€‚é…æ€§è­¦å‘Š: {warning}")
        
        return strategy
        
    except json.JSONDecodeError as e:
        print(f"âŒ JSON è§£æå¤±è´¥: {e}")
        print(f"   å°è¯•è§£æ: {json_str[:300] if json_str else 'None'}")
        return None
    except Exception as e:
        print(f"âŒ ç­–ç•¥è®¾è®¡å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None


def _format_algorithm_constraints(algorithm_analysis):
    """æ ¼å¼åŒ–ç®—æ³•çº¦æŸä¸ºæç¤ºè¯"""
    if not algorithm_analysis:
        return "No specific constraints"
    
    constraints = []
    
    for param in algorithm_analysis.get('key_parameters', []):
        if param.get('priority') in ['critical', 'important']:
            name = param['name']
            constraint_info = param.get('constraints', {})
            
            constraint_text = f"- **{name}** ({param['priority']}): {param.get('reason', '')}"
            
            if constraint_info.get('constraint_description'):
                constraint_text += f"\n  Constraint: {constraint_info['constraint_description']}"
            
            if constraint_info.get('recommended_values'):
                constraint_text += f"\n  Recommended: {constraint_info['recommended_values']}"
            
            if param.get('ablation_required'):
                constraint_text += "\n  âš ï¸  MUST do ablation study on this parameter!"
            
            constraints.append(constraint_text)
    
    return '\n'.join(constraints) if constraints else "No specific constraints"


def tune_scenario_immediately(folder_name, scenario_info, coder, algo_info=None):
    """
    Scenario-level Immediate Tuning - æ™ºèƒ½ä¸¤é˜¶æ®µè°ƒä¼˜
    
    Strategy:
    é˜¶æ®µ1: LLM è‡ªä¸»åˆ†æç®—æ³•ç‰¹æ€§å’Œè°ƒä¼˜éœ€æ±‚
    é˜¶æ®µ2: åŸºäºåˆ†æè®¾è®¡è°ƒä¼˜ç­–ç•¥ï¼ˆæ¶ˆèå®éªŒ + éšæœºæœç´¢ï¼‰
    é˜¶æ®µ3: æ‰§è¡Œè°ƒä¼˜å®éªŒ
    """
    if not ENABLE_HYPERPARAMETER_TUNING:
        return None
    
    scenario_name = scenario_info["name"]
    run_dir = scenario_info.get("run_dir", f"run_{scenario_name}")
    scenario_dir = osp.join(folder_name, run_dir)
    
    print(f"\n" + "="*80)
    print(f"ğŸ¯ åœºæ™¯è°ƒä¼˜: {scenario_name} (æ™ºèƒ½ä¸¤é˜¶æ®µæ¨¡å¼)")
    print("="*80)
    
    # åˆ›å»ºè°ƒä¼˜å­ç›®å½•
    tuning_dir = osp.join(scenario_dir, "tuning")
    os.makedirs(tuning_dir, exist_ok=True)
    print(f"   ğŸ“ è°ƒä¼˜ç›®å½•: {tuning_dir}")
    
    # 1. Extract tunable parameters
    print(f"\n{'='*80}")
    print("Step 1: æå–å¯è°ƒå‚æ•°")
    print(f"{'='*80}")
    tunable_params = extract_tunable_parameters_from_code(folder_name)
    if not tunable_params:
        print("âš ï¸ æœªæ‰¾åˆ°å¯è°ƒå‚æ•°ï¼Œè·³è¿‡è°ƒä¼˜")
        return None
        
    # 2. Get baseline results
    result_file = osp.join(scenario_dir, "final_info.json")
    baseline_results = {}
    if osp.exists(result_file):
        with open(result_file, 'r') as f:
            baseline_results = json.load(f)
        print(f"   âœ“ å·²åŠ è½½åŸºçº¿ç»“æœ")
    else:
        print(f"   âš ï¸  æœªæ‰¾åˆ°åŸºçº¿ç»“æœæ–‡ä»¶: {result_file}")
    
    # å‡†å¤‡ç®—æ³•ä¿¡æ¯ï¼ˆåªä» algorithm.tex æå–ï¼Œé¿å…é‡å¤ï¼‰
    if algo_info is None:
        algo_info = {}
        # åªä» algorithm.tex æå–ï¼ˆidea.json çš„å†…å®¹å·²ç»ä½“ç°åœ¨ experiment.py ä¸­äº†ï¼‰
        algo_tex_path = osp.join(folder_name, "algorithm.tex")
        if osp.exists(algo_tex_path):
            with open(algo_tex_path, 'r', encoding='utf-8') as f:
                algo_info['description'] = f.read()
            print(f"   âœ“ å·²è¯»å– algorithm.tex")
        else:
            print(f"   âš ï¸ æœªæ‰¾åˆ° algorithm.texï¼ŒLLM å°†ä¸»è¦ä¾èµ– experiment.py ä»£ç åˆ†æ")
            algo_info['description'] = "No algorithm description available. Please analyze from experiment.py code."
    
    # ========================================================================
    # é˜¶æ®µ1: LLM åˆ†æç®—æ³•ç‰¹æ€§
    # ========================================================================
    algorithm_analysis = analyze_algorithm_characteristics(
        scenario_info=scenario_info,
        tunable_params=tunable_params,
        algo_info=algo_info,
        coder=coder,
        scenario_dir=tuning_dir,
        folder_name=folder_name
    )
    
    if not algorithm_analysis:
        print("âš ï¸ ç®—æ³•åˆ†æå¤±è´¥ï¼Œä½¿ç”¨åŸºç¡€è°ƒä¼˜ç­–ç•¥")
        algorithm_analysis = None
    
    # ========================================================================
    # é˜¶æ®µ2: åŸºäºåˆ†æè®¾è®¡è°ƒä¼˜ç­–ç•¥
    # ========================================================================
    strategy = design_tuning_strategy_for_scenario(
        scenario_info=scenario_info,
        current_results=baseline_results,
        tunable_params=tunable_params,
        algorithm_analysis=algorithm_analysis,
        coder=coder,
        scenario_dir=tuning_dir
    )
    
    if not strategy:
        print("âŒ ç­–ç•¥è®¾è®¡å¤±è´¥")
        return None
    
    # æ£€æŸ¥æ˜¯å¦æœ‰éšæœºæœç´¢é…ç½®
    random_search_config = strategy.get("random_search", {})
    if not random_search_config.get("search_space"):
        print("âŒ æœªå®šä¹‰éšæœºæœç´¢ç©ºé—´")
        return None
    
    # ========================================================================
    # é˜¶æ®µ3: Ensure experiment.py is importable
    # ========================================================================
    print(f"\n{'='*80}")
    print("Step 2: é‡æ„ experiment.py ä¸ºå¯å¯¼å…¥æ¨¡å¼")
    print(f"{'='*80}")
    ensure_importability_prompt = """
We need to run hyperparameter tuning by importing functions from experiment.py into a separate script.

**Your Task:**
Refactor `experiment.py` to ensure the main training logic is encapsulated in a function `run_experiment(args)` that:
1. Accepts an `args` object (Namespace or dict) as input.
2. Returns the results dictionary (the same dict you save to json).
3. Can be imported without running the script (put the `main()` call under `if __name__ == "__main__":`).

**CRITICAL: Use MINIMAL, TARGETED EDITS (Diff).**
- If `run_experiment` already exists, just ensure it returns results.
- If logic is in `main()`, extract it to `run_experiment(args)`.
- DO NOT rewrite the whole file.
"""
    coder.run(ensure_importability_prompt)
    
    # ========================================================================
    # é˜¶æ®µ4: Generate independent tuning script with RANDOM SEARCH
    # ========================================================================
    print(f"\n{'='*80}")
    print("Step 3: ç”Ÿæˆè°ƒä¼˜è„šæœ¬ (tune_experiment.py)")
    print(f"{'='*80}")
    
    search_space_json = json.dumps(random_search_config['search_space'], indent=2)
    num_trials = random_search_config.get('num_trials', 10)
    base_params_json = json.dumps(scenario_info.get('parameters', {}))
    
    tuning_script_prompt = f"""
Create a NEW Python script named `tune_experiment.py` to perform Random Search hyperparameter tuning.

**Requirements:**

1. **Imports:**
   Import `run_experiment` from `experiment`, plus `random`, `numpy as np`, `json`, `os`, `math`, and `argparse`.

2. **Define the Search Space:**
{search_space_json}

3. **Implement `sample_parameters(search_space, rng)` function:**
   - For each parameter in search_space, sample a value based on its type:
   
   **Float parameters:**
   ```python
   if param_spec["type"] == "float":
       if param_spec.get("scaling") == "log":
           # Log-uniform sampling
           log_min = math.log(param_spec["min"])
           log_max = math.log(param_spec["max"])
           value = math.exp(rng.uniform(log_min, log_max))
       else:
           # Linear uniform sampling
           value = rng.uniform(param_spec["min"], param_spec["max"])
   ```
   
   **Integer parameters:**
   ```python
   elif param_spec["type"] == "int":
       value = rng.randint(param_spec["min"], param_spec["max"] + 1)
   ```
   
   **Categorical parameters:**
   ```python
   elif param_spec["type"] == "categorical":
       value = rng.choice(param_spec["values"])
   ```
   
   Return a dictionary of sampled parameters.

4. **Main Random Search Loop:**
   - Run **{num_trials} trials**
   - For each trial i:
     * Sample parameters: `params = sample_parameters(search_space, rng)`
     * Merge with base parameters: {base_params_json}
     * Create args object (Namespace or simple class with attributes)
     * Set `args.out_dir = f"tuning/trial_{{i}}"`
     * Create output directory
     * Call `result = run_experiment(args)` inside try-except
     * Save result to `tuning/trial_{{i}}/final_info.json`
     * Track metrics for best result selection

5. **Best Result Selection:**
   - Compare results across all trials
   - Choose best based on final test accuracy (maximize) or final train loss (minimize)
   - Handle missing/failed trials gracefully

6. **Save Summary:**
   Save to `tuning/tuning_summary.json`:
   ```json
   {{
     "best_config": {{...}},  // Parameters of best trial
     "best_result": {{...}},  // Metrics of best trial
     "all_results": [        // Summary of all trials
       {{"trial": 1, "parameters": {{...}}, "metrics": {{...}}, "status": "success"}},
       ...
     ]
   }}
   ```

**Base Parameters (non-tunable):**
{base_params_json}

**Seed:** Use `rng = random.Random(42)` for reproducibility.

**Output:**
Generate the complete `tune_experiment.py` file code implementing Random Search as specified above.
"""
    coder.run(tuning_script_prompt)
    
    # ========================================================================
    # é˜¶æ®µ5: Execute Tuning Script
    # ========================================================================
    print(f"\n{'='*80}")
    print("Step 4: æ‰§è¡Œè°ƒä¼˜è„šæœ¬")
    print(f"{'='*80}")
    cwd = osp.abspath(folder_name)
    
    try:
        # Run the generated script
        result = subprocess.run(
            ["python", "tune_experiment.py"], 
            cwd=cwd, 
            stdout=subprocess.PIPE, 
            stderr=subprocess.PIPE, 
            text=True, 
            timeout=7200 
        )
        
        if result.returncode != 0:
            print(f"âŒ è°ƒä¼˜è„šæœ¬å¤±è´¥:\n{result.stderr[:500]}")
            
            # å³ä½¿å¤±è´¥ä¹Ÿä¿å­˜ç›¸å…³æ–‡ä»¶åˆ°åœºæ™¯ç›®å½•
            tune_script_src = osp.join(cwd, "tune_experiment.py")
            if osp.exists(tune_script_src):
                tune_script_dst = osp.join(scenario_dir, "tune_experiment.py")
                shutil.copy(tune_script_src, tune_script_dst)
                
                # ä¿å­˜é”™è¯¯æ—¥å¿—
                error_log = osp.join(tuning_dir, "tuning_error.log")
                with open(error_log, 'w', encoding='utf-8') as f:
                    f.write(f"=== STDERR ===\n{result.stderr}\n\n=== STDOUT ===\n{result.stdout}")
                print(f"   ğŸ“„ é”™è¯¯æ—¥å¿—å·²ä¿å­˜: {error_log}")
            
            return None
            
        print("âœ… è°ƒä¼˜å®Œæˆ")
        
        # åŠ è½½è°ƒä¼˜ç»“æœï¼ˆtune_experiment.py ä¼šå°†ç»“æœä¿å­˜åˆ° tuning/tuning_summary.jsonï¼‰
        summary_file = osp.join(cwd, "tuning", "tuning_summary.json")
        
        if osp.exists(summary_file):
            with open(summary_file, 'r', encoding='utf-8') as f:
                summary = json.load(f)
            
            best_config = summary.get("best_config")
            print(f"\nğŸ† æœ€ä½³é…ç½®:")
            print(json.dumps(best_config, indent=2, ensure_ascii=False))
            
            # ========================================================
            # ä¿å­˜æ‰€æœ‰ç›¸å…³æ–‡ä»¶åˆ°åœºæ™¯ç›®å½•ï¼ˆé‡è¦ï¼ï¼‰
            # ========================================================
            
            # 1. ä¿å­˜è°ƒä¼˜æŠ¥å‘Šåˆ°åœºæ™¯æ ¹ç›®å½•
            report_file = osp.join(scenario_dir, "tuning_report.json")
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(summary, f, indent=2, ensure_ascii=False)
            print(f"   ğŸ“„ è°ƒä¼˜æŠ¥å‘Š: {report_file}")
            
            # 2. å¤åˆ¶ tune_experiment.py åˆ°åœºæ™¯ç›®å½•
            tune_script_src = osp.join(cwd, "tune_experiment.py")
            if osp.exists(tune_script_src):
                tune_script_dst = osp.join(scenario_dir, "tune_experiment.py")
                shutil.copy(tune_script_src, tune_script_dst)
                print(f"   ğŸ“„ tune_experiment.py å¿«ç…§å·²ä¿å­˜")
            
            # 3. å¤åˆ¶ experiment.py åˆ° tuning å­ç›®å½•ï¼ˆè°ƒä¼˜æ—¶çš„ç‰ˆæœ¬ï¼‰
            exp_file = osp.join(cwd, "experiment.py")
            if osp.exists(exp_file):
                exp_dst = osp.join(tuning_dir, "experiment.py")
                shutil.copy(exp_file, exp_dst)
                print(f"   ğŸ“„ experiment.py å¿«ç…§å·²ä¿å­˜åˆ° tuning/")
            
            # 4. ä¿å­˜ç®—æ³•åˆ†æï¼ˆå¦‚æœæœ‰ï¼‰
            if algorithm_analysis:
                analysis_file = osp.join(scenario_dir, "algorithm_analysis.json")
                with open(analysis_file, 'w', encoding='utf-8') as f:
                    json.dump(algorithm_analysis, f, indent=2, ensure_ascii=False)
                print(f"   ğŸ“„ ç®—æ³•åˆ†æ: {analysis_file}")
            
            # 5. ä¿å­˜è°ƒä¼˜ç­–ç•¥
            strategy_file = osp.join(scenario_dir, "tuning_strategy.json")
            with open(strategy_file, 'w', encoding='utf-8') as f:
                json.dump(strategy, f, indent=2, ensure_ascii=False)
            print(f"   ğŸ“„ è°ƒä¼˜ç­–ç•¥: {strategy_file}")
            
            # 6. ç§»åŠ¨ä¸»è°ƒä¼˜ç›®å½•åˆ°åœºæ™¯ç›®å½•ï¼ˆå¦‚æœè¿˜æ²¡æœ‰ç§»åŠ¨ï¼‰
            global_tuning_dir = osp.join(cwd, "tuning")
            if osp.exists(global_tuning_dir) and global_tuning_dir != tuning_dir:
                try:
                    # å¤åˆ¶è€Œä¸æ˜¯ç§»åŠ¨ï¼Œé¿å…ç ´ååŸæœ‰ç»“æ„
                    import distutils.dir_util
                    distutils.dir_util.copy_tree(global_tuning_dir, tuning_dir)
                    print(f"   ğŸ“ è°ƒä¼˜æ•°æ®å·²å¤åˆ¶åˆ°åœºæ™¯ç›®å½•")
                except Exception as e:
                    print(f"   âš ï¸ å¤åˆ¶è°ƒä¼˜æ•°æ®å¤±è´¥: {e}")
            
            return summary
        else:
            print(f"âš ï¸ æœªæ‰¾åˆ°è°ƒä¼˜æ‘˜è¦æ–‡ä»¶: {summary_file}")
            return None
            
    except TimeoutExpired:
        print("â° è°ƒä¼˜è¶…æ—¶")
        return None
    except Exception as e:
        print(f"ğŸ’¥ è°ƒä¼˜å¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()
        return None


def generate_comprehensive_visualization(folder_name, coder=None, scenario_results=None):
    """
    ç”Ÿæˆç»¼åˆå¯è§†åŒ–ç»“æœ
    
    å‚æ•°:
        folder_name: å®éªŒæ–‡ä»¶å¤¹è·¯å¾„
        coder: Aider Coder å¯¹è±¡ï¼ˆç”¨äºè®© AI æ›´æ–° plot.pyï¼‰
        scenario_results: åœºæ™¯æ‰§è¡Œç»“æœåˆ—è¡¨
    """
    cwd = osp.abspath(folder_name)
    plots_dir = osp.join(cwd, "multi_scenario_plots")
    
    # æ¸…ç†å¹¶åˆ›å»ºç»˜å›¾ç›®å½•
    if osp.exists(plots_dir):
        shutil.rmtree(plots_dir)
    os.makedirs(plots_dir)
    
    # æ£€æŸ¥ plot.py æ˜¯å¦å­˜åœ¨
    plot_file = osp.join(cwd, "plot.py")
    if not osp.exists(plot_file):
        return False, "plot.py æ–‡ä»¶ä¸å­˜åœ¨"
    
    # ========================================================================
    # æ–°å¢ï¼šè®© AI æ›´æ–° plot.py ä»¥è¯†åˆ«æ‰€æœ‰åœºæ™¯ç›®å½•
    # ========================================================================
    if coder and scenario_results:
        # æ”¶é›†æ‰€æœ‰æˆåŠŸçš„åœºæ™¯ç›®å½•
        successful_runs = []
        for result in scenario_results:
            if result.get("status") == "success":
                run_dir = result.get("run_dir", "")
                scenario_name = result.get("name", "")
                description = result.get("description", "")
                successful_runs.append({
                    "directory": run_dir,
                    "label": scenario_name,
                    "description": description
                })
        
        if successful_runs:
            print(f"ğŸ¤– è®© AI æ›´æ–° plot.py ä»¥è¯†åˆ« {len(successful_runs)} ä¸ªåœºæ™¯ç›®å½•...")
            
            update_plot_prompt = f"""
Please update plot.py to visualize results from the following scenario directories:

{json.dumps(successful_runs, indent=2)}

**CRITICAL: Make MINIMAL, TARGETED EDITS - DO NOT rewrite the entire file!**

Key requirements:
1. Update the 'labels' dictionary to map these scenario directories to their labels
2. Read final_info.json from each directory (may need to adjust file paths)
3. Generate comparison plots showing all scenarios
4. Use clear legends and labels to distinguish different scenarios
5. Create professional-looking plots with appropriate styling

Example labels dictionary format:
```python
labels = {{
    "run_baseline": "Baseline Configuration",
    "run_high_learning_rate": "High Learning Rate",
    "run_noisy_data": "Noisy Data Test",
    # ... more scenarios
}}
```

**Instructions:**
- Locate the existing 'labels' dictionary in plot.py
- Replace it with the updated mapping above
- If file reading logic needs adjustment, modify only that section
- DO NOT rewrite plotting code that's already working

Please make TARGETED modifications to plot.py now.
"""
            coder.run(update_plot_prompt)
            print("âœ… AI å·²æ›´æ–° plot.py")
    
    # æ‰§è¡Œç»˜å›¾å‘½ä»¤
    command = ["python", "plot.py", f"--out_dir={plots_dir}"]
    
    try:
        print("ğŸ¨ ç”Ÿæˆå¤šåœºæ™¯å¯è§†åŒ–ç»“æœ...")
        result = subprocess.run(
            command, 
            cwd=cwd, 
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE, 
            text=True, 
            timeout=600  # 10åˆ†é’Ÿè¶…æ—¶
        )
        
        if result.returncode == 0:
            # æ£€æŸ¥ç”Ÿæˆçš„å›¾è¡¨æ–‡ä»¶
            plot_files = [f for f in os.listdir(plots_dir) if f.endswith('.png')]
            if plot_files:
                print(f"âœ… æˆåŠŸç”Ÿæˆ {len(plot_files)} ä¸ªå›¾è¡¨æ–‡ä»¶:")
                for plot_file in plot_files:
                    print(f"   ğŸ“Š {plot_file}")
                return True, f"ç”Ÿæˆ {len(plot_files)} ä¸ªå¯è§†åŒ–å›¾è¡¨"
            else:
                return False, "ç»˜å›¾å®Œæˆä½†æœªç”Ÿæˆå›¾è¡¨æ–‡ä»¶"
        else:
            error_msg = result.stderr[:500] if result.stderr else "Unknown error"
            return False, f"ç»˜å›¾å¤±è´¥: {error_msg}"
            
    except TimeoutExpired:
        return False, "ç»˜å›¾è¶…æ—¶"
    except Exception as e:
        return False, f"ç»˜å›¾å¼‚å¸¸: {str(e)}"


def perform_original_experiment_loop(idea, folder_name, coder, baseline_results):
    """
    åŸæœ‰çš„å®éªŒå¾ªç¯ï¼ˆä½œä¸ºå¤‡é€‰æ–¹æ¡ˆï¼‰
    """
    print("\nğŸ”„ æ‰§è¡ŒåŸæœ‰å®éªŒå¾ªç¯...")
    
    current_iter = 0
    run = 1
    
    # ç”Ÿæˆåˆå§‹æç¤ºè¯
    next_prompt = coder_prompt.format(
        title=idea["Title"],
        idea=idea["Experiment"],
        max_runs=MAX_RUNS,
        baseline_results=baseline_results,
    )
    
    # å®éªŒå¾ªç¯
    while run < MAX_RUNS + 1:
        if current_iter >= MAX_ITERS:
            print("âš ï¸ è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œåœæ­¢å½“å‰è¿è¡Œ")
            break
        
        print(f"\n--- Run {run} (å°è¯• {current_iter + 1}/{MAX_ITERS}) ---")
        print("ğŸ¤– AI æ­£åœ¨åˆ†æå’Œä¿®æ”¹ä»£ç ...")
        coder_out = coder.run(next_prompt)
        print(coder_out)
        
        if "ALL_COMPLETED" in coder_out:
            print("âœ… AI è¡¨ç¤ºæ‰€æœ‰å®éªŒå·²å®Œæˆ")
            break
        
        print(f"âš™ï¸ æ‰§è¡Œå®éªŒ: python experiment.py --out_dir=run_{run}")
        return_code, next_prompt = run_experiment(folder_name, run)
        
        if return_code == 0:
            print(f"âœ… Run {run} æˆåŠŸå®Œæˆ")
            run += 1
            current_iter = 0
        else:
            print(f"âŒ Run {run} å¤±è´¥ï¼Œå‡†å¤‡é‡è¯•")
            current_iter += 1
    
    if current_iter >= MAX_ITERS:
        print("\nâŒ å®éªŒå¾ªç¯æœªèƒ½å®Œæˆæ‰€æœ‰è¿è¡Œï¼ˆè¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼‰")
        return False
    
    print(f"\nâœ… å®éªŒå¾ªç¯å®Œæˆï¼Œå…±å®Œæˆ {run - 1} æ¬¡è¿è¡Œ")
    
    # æ‰§è¡Œå¯è§†åŒ–
    print("\nğŸ“Š æ‰§è¡Œå¯è§†åŒ–ç”Ÿæˆ...")
    current_iter = 0
    next_prompt = """
Please modify `plot.py` to generate the most relevant plots for the final writeup. 

**CRITICAL: Make MINIMAL, TARGETED EDITS - DO NOT rewrite the entire file!**

**Instructions:**
- Locate the existing 'labels' dictionary in plot.py
- Update it with the correct names for each run directory
- If plotting logic needs adjustment, modify only that specific section
- DO NOT output code that's already working

Focus on updating the labels dictionary and any broken plotting logic.
"""
    
    while True:
        print(f"\nğŸ¤– AI æ­£åœ¨ä¿®æ”¹ plot.py (å°è¯• {current_iter + 1}/{MAX_ITERS})...")
        _ = coder.run(next_prompt)
        
        print("âš™ï¸ æ‰§è¡Œç»˜å›¾: python plot.py")
        return_code, next_prompt = run_plotting(folder_name)
        
        current_iter += 1
        
        if return_code == 0:
            print("âœ… å¯è§†åŒ–ç”ŸæˆæˆåŠŸ")
            break
        elif current_iter >= MAX_ITERS:
            print("âš ï¸ å¯è§†åŒ–ç”Ÿæˆå¤±è´¥ï¼ˆè¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼‰")
            break
        else:
            print("âŒ ç»˜å›¾å¤±è´¥ï¼Œå‡†å¤‡é‡è¯•")
    
    # æ–‡æ¡£æ›´æ–°
    print("\nğŸ“ æ›´æ–°å®éªŒæ–‡æ¡£...")
    next_prompt = """
Please modify `notes.txt` with a description of what each plot shows along with the filename of the figure. 
Somebody else will be using `notes.txt` to write a report on this in the future.
"""
    coder.run(next_prompt)
    print("âœ… æ–‡æ¡£æ›´æ–°å®Œæˆ")
    
    return True
