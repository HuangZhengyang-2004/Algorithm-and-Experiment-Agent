=== AI 原始响应 ===
{
  "search_space": {
    "learning_rate": {"type": "float", "min": 0.003, "max": 0.08, "scaling": "log"},
    "subspace_dim": {"type": "int", "min": 5, "max": 30},
    "momentum": {"type": "float", "min": 0.6, "max": 0.95, "scaling": "linear"},
    "client_fraction": {"type": "float", "min": 0.2, "max": 0.6, "scaling": "linear"}
  },
  "rationale": "With client-local label noise, robustness depends on controlling update variance and noise amplification: learning_rate governs step stability under noisy gradients (log-scaled due to sensitivity), subspace_dim sets projection strength to attenuate off-subspace noise (trade-off between robustness and bias), momentum impacts noise accumulation (lower momentum can prevent amplifying corrupted gradients), and client_fraction affects robustness via aggregation (more clients reduces variance). These ranges center on the scenario defaults while allowing exploration toward more conservative and more robust settings.",
  "num_trials": 12,
  "expected_improvement": "Expect smoother convergence and 2–5% absolute gains in final test accuracy along with lower final train loss by reducing noise amplification (via tuned momentum and learning rate), improving aggregation robustness (via higher client_fraction), and choosing an effective subspace_dim."
}

=== 提示词 ===
You are an expert in hyperparameter optimization. Design a Search Space for Random Search hyperparameter tuning.

# Scenario Information
Name: robustness_to_label_noise_(client-local_corruption)
Description: Test sensitivity to noisy gradients by introducing label corruption on a subset of clients.

# Current Performance (Baseline)
{
  "train_loss": {
    "means": [
      1.0721834446591114,
      0.9126017456142765,
      0.6606965053399307,
      0.637800778435055,
      0.5396145461300815,
      0.4981164763575698,
      0.4811639277526385,
      0.42828455629441126,
      0.41582439568416113,
      0.4057201647811049,
      0.3829781138688807,
      0.37694622351807777,
      0.34516807389987375,
      0.324031762415584,
      0.32350254180933624,
      0.3134534695784539,
      0.2994057751877976,
      0.2967718407639188,
      0.29377550055013324,
      0.2904130773662621,
      0.2822666986043084,
      0.28274035265097536,
      0.277446463818682,
      0.2657769920455817,
      0.2617936792324177,
      0.259448180310217,
      0.25435826898292163,
      0.2484327111221529,
      0.24526755940084455,
      0.24191167756211773,
      0.23611874055554666,
      0.23928344294379694,
      0.23421997248821755,
      0.23214874578282288,
      0.2320471401385402,
      0.23126139277120758,
      0.22921354489622178,
      0.225820704626463,
      0.22436025698955434,
      0.22764960323259112,
      0.22374029730736264,
      0.22132026582838443,
      0.22157996678213265,
      0.22068487602972472,
      0.21396140571352476,
      0.21369808243963168,
      0.2117358635868711,
      0.20830271622986155,
      0.2070345089184634,
      0.20972685397726323,
      0.20576696957397578,
      0.20518149251956885,
      0.2085916785978388,
      0.20861194263630597,
      0.2045210556850101,
      0.2042578162528817,
      0.20509513925820266,
      0.2021652923453858,
      0.2015226333485254,
      0.200378237928888,
      0.19977820602250262,
      0.20108438583447152,
      0.19721990037152437,
      0.19677623305734152,
      0.19732861932623694,
      0.19533270386749368,
      0.1927362696552782,
      0.19162755296968176,
      0.19235055665114828,
      0.1881677747027284,
      0.18791469502447375,
      0.18749808851714103,
      0.18731369177025484,
      0.18770747859744316,
      0.18549307195751683,
      0.1856596158973403,
      0.18604061367124475,
      0.18440600992114184,
      0.1851910128127784,
      0.18534394736341225,
      0.18283582700739334,
      0.18347619382840744,
      0.18369981838269347,
      0.1834306516973962,
      0.18239127604551955,
      0.18345846779761607,
      0.18549931356941946,
      0.1830507017014888,
      0.18124891467244153,
      0.18259282930638623,
      0.18158053712134073,
      0.17816675408993732,
      0.18003662357617448,
      0.1802802436534896,
      0.17661708751289093,
      0.17602439532387618,
      0.17568674075610607,
      0.1740270542775181,
      0.17251009522472274,
      0.17343318163092383
    ],
    "stds": [
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0
    ]
  },
  "test_accuracy": {
    "means": [
      0.6616666666666666,
      0.4816666666666667,
      0.35333333333333333,
      0.6108333333333333,
      0.49916666666666665,
      0.36583333333333334,
      0.49916666666666665,
      0.4141666666666667,
      0.43583333333333335,
      0.5033333333333333,
      0.44333333333333336,
      0.49083333333333334,
      0.49666666666666665,
      0.4975,
      0.4791666666666667,
      0.48083333333333333,
      0.5158333333333334,
      0.4891666666666667,
      0.48583333333333334,
      0.5008333333333334,
      0.51,
      0.5141666666666667,
      0.5016666666666667,
      0.5508333333333333,
      0.5508333333333333,
      0.5341666666666667,
      0.5516666666666666,
      0.5583333333333333,
      0.5658333333333333,
      0.5858333333333333,
      0.5925,
      0.6125,
      0.5933333333333334,
      0.6033333333333334,
      0.6133333333333333,
      0.6033333333333334,
      0.6083333333333333,
      0.6166666666666667,
      0.6241666666666666,
      0.6475,
      0.6441666666666667,
      0.645,
      0.6441666666666667,
      0.6508333333333334,
      0.6641666666666667,
      0.6583333333333333,
      0.6633333333333333,
      0.6666666666666666,
      0.67,
      0.6708333333333333,
      0.6741666666666667,
      0.6775,
      0.6741666666666667,
      0.6758333333333333,
      0.6841666666666667,
      0.6825,
      0.6875,
      0.6908333333333333,
      0.69,
      0.6983333333333334,
      0.6983333333333334,
      0.7033333333333334,
      0.6983333333333334,
      0.6975,
      0.705,
      0.6983333333333334,
      0.6975,
      0.6966666666666667,
      0.6966666666666667,
      0.6975,
      0.6941666666666667,
      0.695,
      0.6933333333333334,
      0.69,
      0.6866666666666666,
      0.6875,
      0.6933333333333334,
      0.6858333333333333,
      0.6858333333333333,
      0.6908333333333333,
      0.6891666666666667,
      0.6916666666666667,
      0.6966666666666667,
      0.6941666666666667,
      0.6925,
      0.6958333333333333,
      0.6991666666666667,
      0.7008333333333333,
      0.7025,
      0.7058333333333333,
      0.71,
      0.7133333333333334,
      0.7116666666666667,
      0.7191666666666666,
      0.725,
      0.725,
      0.7241666666666666,
      0.715,
      0.7166666666666667,
      0.7141666666666666
    ],
    "stds": [
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0
    ]
  },
  "cumulative_comm_floats": {
    "means": [
      5049.0,
      10098.0,
      15147.0,
      20196.0,
      25245.0,
      30294.0,
      35343.0,
      40392.0,
      45441.0,
      50490.0,
      55539.0,
      60588.0,
      65637.0,
      70686.0,
      75735.0,
      80784.0,
      85833.0,
      90882.0,
      95931.0,
      100980.0,
      106029.0,
      111078.0,
      116127.0,
      121176.0,
      126225.0,
      131274.0,
      136323.0,
      141372.0,
      146421.0,
      151470.0,
      156519.0,
      161568.0,
      166617.0,
      171666.0,
      176715.0,
      181764.0,
      186813.0,
      191862.0,
      196911.0,
      201960.0,
      207009.0,
      212058.0,
      217107.0,
      222156.0,
      227205.0,
      232254.0,
      237303.0,
      242352.0,
      247401.0,
      252450.0,
      257499.0,
      262548.0,
      267597.0,
      272646.0,
      277695.0,
      282744.0,
      287793.0,
      292842.0,
      297891.0,
      302940.0,
      307989.0,
      313038.0,
      318087.0,
      323136.0,
      328185.0,
      333234.0,
      338283.0,
      343332.0,
      348381.0,
      353430.0,
      358479.0,
      363528.0,
      368577.0,
      373626.0,
      378675.0,
      383724.0,
      388773.0,
      393822.0,
      398871.0,
      403920.0,
      408969.0,
      414018.0,
      419067.0,
      424116.0,
      429165.0,
      434214.0,
      439263.0,
      444312.0,
      449361.0,
      454410.0,
      459459.0,
      464508.0,
      469557.0,
      474606.0,
      479655.0,
      484704.0,
      489753.0,
      494802.0,
      499851.0,
      504900.0
    ],
    "stds": [
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0
    ]
  }
}

# Available Parameters
{
  "learning_rate": {
    "type": "float",
    "current_value": 0.05,
    "description": "Parameter learning_rate"
  },
  "num_iterations": {
    "type": "int",
    "current_value": 50,
    "description": "Number of federated rounds T"
  },
  "batch_size": {
    "type": "int",
    "current_value": 64,
    "description": "Local minibatch size B"
  },
  "momentum": {
    "type": "float",
    "current_value": 0.9,
    "description": "Momentum coefficient mu"
  },
  "subspace_dim": {
    "type": "int",
    "current_value": 20,
    "description": "Subspace dimension r"
  },
  "local_steps": {
    "type": "int",
    "current_value": 5,
    "description": "Local SGD steps per round tau"
  },
  "client_fraction": {
    "type": "float",
    "current_value": 0.5,
    "description": "Client sampling fraction C"
  },
  "num_clients": {
    "type": "int",
    "current_value": 10,
    "description": "Total number of clients N"
  },
  "dataset_size": {
    "type": "int",
    "current_value": 2000,
    "description": "Parameter dataset_size"
  },
  "input_dim": {
    "type": "int",
    "current_value": 10,
    "description": "Input feature dimension d"
  },
  "n_classes": {
    "type": "int",
    "current_value": 3,
    "description": "Number of classes"
  },
  "l2_reg": {
    "type": "float",
    "current_value": 0.0,
    "description": "L2 regularization strength"
  },
  "classes_per_client": {
    "type": "int",
    "current_value": 2,
    "description": "For noniid_classes strategy: number of dominant classes per client"
  },
  "label_noise_rate": {
    "type": "float",
    "current_value": 0.0,
    "description": "Parameter label_noise_rate"
  },
  "label_noise_clients_fraction": {
    "type": "float",
    "current_value": 0.0,
    "description": "Parameter label_noise_clients_fraction"
  }
}

# Your Task
Select 2-4 key parameters to tune and define their search ranges for Random Search.

## Parameter Type Specifications

### For Float Parameters:
- Specify: `{"type": "float", "min": <value>, "max": <value>, "scaling": "linear" or "log"}`
- Use "log" scaling for parameters that span orders of magnitude (e.g., learning_rate: 0.0001 to 0.1)
- Use "linear" for parameters with narrow ranges

### For Integer Parameters:
- Specify: `{"type": "int", "min": <value>, "max": <value>}`
- Examples: num_layers, num_iterations, batch_size

### For Categorical Parameters:
- Specify: `{"type": "categorical", "values": [val1, val2, ...]}`
- Examples: activation_function, optimizer

## Requirements
1. Select 2-4 parameters that are most likely to affect performance in THIS scenario
2. For each parameter, define appropriate search range based on its current value and expected impact
3. Choose appropriate scaling (linear vs log) for continuous parameters
4. Recommend number of random trials (typically 8-15)

**CRITICAL: Prioritize algorithm-specific parameters!**
- For subspace-based algorithms: ALWAYS include `subspace_dim` or similar if available
- For momentum-based methods: Include `momentum` coefficient if available
- For federated learning: Consider `local_steps`, `client_fraction`, `batch_size`
- Learning rate is important but not the only parameter to tune!

## Output Format (MUST be valid JSON)
```json
{
  "search_space": {
    "learning_rate": {"type": "float", "min": 0.001, "max": 0.2, "scaling": "log"},
    "batch_size": {"type": "categorical", "values": [32, 64, 128]},
    "num_layers": {"type": "int", "min": 1, "max": 5}
  },
  "rationale": "Explanation of why these parameters and ranges were chosen for THIS scenario",
  "num_trials": 10,
  "expected_improvement": "What improvement we expect to see with optimized parameters"
}
```

IMPORTANT:
- Return ONLY valid JSON, no extra text
- DO NOT include file editing instructions or diff markers (<<<<<<< SEARCH, =======, >>>>>>> REPLACE)
- DO NOT wrap JSON in code blocks or create new files
- Just output the raw JSON object directly
- Search ranges should be informed by baseline performance and scenario characteristics
- For subspace-based algorithms, consider including subspace_dim, momentum as tunable parameters if available
- num_trials should be 8-15 (balancing exploration vs computational cost)

**Example Response Format (output this EXACTLY, no other text):**
{
  "search_space": {
    "learning_rate": {"type": "float", "min": 0.001, "max": 0.1, "scaling": "log"}
  },
  "rationale": "...",
  "num_trials": 10,
  "expected_improvement": "..."
}
